{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93a4c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, csv, sys, subprocess, time, random\n",
    "import requests\n",
    "import argparse\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "UA = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Reddit-Downloader/OptionB\"\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\"User-Agent\": UA})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d86cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_name(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"[^\\w\\-. ]+\", \"_\", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s)[:200] or \"reddit_download\"\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# ---- Backoff-aware HTTP helpers ----\n",
    "\n",
    "def request_with_backoff(method: str, url: str, *, max_retries=5, timeout=30, stream=False, headers=None):\n",
    "    \"\"\"\n",
    "    Generic HTTP request with exponential backoff.\n",
    "    Respects Retry-After if present. Jitter added to avoid thundering herds.\n",
    "    Retries on 429 and 5xx. Raises on other 4xx.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        try:\n",
    "            resp = SESSION.request(method, url, timeout=timeout, stream=stream, headers=headers)\n",
    "        except requests.RequestException as e:\n",
    "            if attempt >= max_retries:\n",
    "                raise\n",
    "            sleep = min(60, (2 ** attempt)) + random.uniform(0, 0.5)\n",
    "            print(f\"Network error {e}; retrying in {sleep:.1f}s …\")\n",
    "            time.sleep(sleep)\n",
    "            attempt += 1\n",
    "            continue\n",
    "\n",
    "        if resp.status_code == 429 or 500 <= resp.status_code < 600:\n",
    "            if attempt >= max_retries:\n",
    "                resp.raise_for_status()\n",
    "            retry_after = resp.headers.get(\"Retry-After\")\n",
    "            if retry_after is not None:\n",
    "                try:\n",
    "                    sleep = float(retry_after)\n",
    "                except ValueError:\n",
    "                    sleep = 10.0\n",
    "            else:\n",
    "                sleep = min(60, (2 ** attempt)) + random.uniform(0, 0.5)\n",
    "            print(f\"{resp.status_code} on {url}\\nRetrying in {sleep:.1f}s …\")\n",
    "            time.sleep(sleep)\n",
    "            attempt += 1\n",
    "            continue\n",
    "\n",
    "        # Other 4xx -> raise immediately\n",
    "        if 400 <= resp.status_code < 500:\n",
    "            resp.raise_for_status()\n",
    "\n",
    "        return resp\n",
    "\n",
    "def download_file(url: str, outpath: str, *, max_retries=5):\n",
    "    with request_with_backoff(\"GET\", url, max_retries=max_retries, timeout=60, stream=True) as r:\n",
    "        total = int(r.headers.get(\"Content-Length\", 0))\n",
    "        done = 0\n",
    "        chunk = 1 << 15\n",
    "        with open(outpath, \"wb\") as f:\n",
    "            for part in r.iter_content(chunk_size=chunk):\n",
    "                if not part:\n",
    "                    continue\n",
    "                f.write(part)\n",
    "                done += len(part)\n",
    "                if total:\n",
    "                    pct = done * 100 // total\n",
    "                    print(f\"\\r  {os.path.basename(outpath)}  {pct}% ({done}/{total} bytes)\", end=\"\")\n",
    "        if total:\n",
    "            print()\n",
    "\n",
    "# ---- Reddit parsing + media handling ----\n",
    "\n",
    "def get_post_json(url: str, *, max_retries=5) -> dict:\n",
    "    if not url.startswith((\"http://\", \"https://\")):\n",
    "        raise ValueError(f\"Not a URL: {url}\")\n",
    "    u = url\n",
    "    if not u.endswith(\"/\"):\n",
    "        u += \"/\"\n",
    "    if not u.endswith(\".json\"):\n",
    "        u += \".json\"\n",
    "    r = request_with_backoff(\"GET\", u, max_retries=max_retries, timeout=30)\n",
    "    data = r.json()\n",
    "    if isinstance(data, list) and data and data[0][\"data\"][\"children\"]:\n",
    "        return data[0][\"data\"][\"children\"][0][\"data\"]\n",
    "    return data\n",
    "\n",
    "def pick_best_from_mpd(mpd_xml: str):\n",
    "    root = ET.fromstring(mpd_xml)\n",
    "    ns = {\"mpd\": root.tag.split('}')[0].strip('{')} if '}' in root.tag else {}\n",
    "    def fa(elem, path):\n",
    "        return elem.findall(path, ns) if ns else elem.findall(path)\n",
    "    base_urls = fa(root, \".//mpd:BaseURL\") if ns else root.findall(\".//BaseURL\")\n",
    "    base_url = base_urls[0].text.strip() if base_urls else \"\"\n",
    "    best_video = (0, None)\n",
    "    best_audio = (0, None)\n",
    "    for aset in fa(root, \".//mpd:AdaptationSet\") if ns else root.findall(\".//AdaptationSet\"):\n",
    "        mime = aset.get(\"mimeType\", \"\")\n",
    "        for rep in fa(aset, \"mpd:Representation\") if ns else aset.findall(\"Representation\"):\n",
    "            bw = int(rep.get(\"bandwidth\", \"0\"))\n",
    "            rep_base = fa(rep, \"mpd:BaseURL\") if ns else rep.findall(\"BaseURL\")\n",
    "            if not rep_base:\n",
    "                continue\n",
    "            url = rep_base[0].text.strip()\n",
    "            if base_url and not url.lower().startswith((\"http://\", \"https://\")):\n",
    "                url = urljoin(base_url, url)\n",
    "            if mime.startswith(\"video/\") and bw > best_video[0]:\n",
    "                best_video = (bw, url)\n",
    "            elif mime.startswith(\"audio/\") and bw > best_audio[0]:\n",
    "                best_audio = (bw, url)\n",
    "    return best_video[1], best_audio[1]\n",
    "\n",
    "def has_ffmpeg() -> bool:\n",
    "    try:\n",
    "        subprocess.run([\"ffmpeg\", \"-version\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=False)\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        return False\n",
    "\n",
    "def merge_av(video_path: str, audio_path: str, out_path: str):\n",
    "    cmd = [\"ffmpeg\", \"-y\", \"-i\", video_path, \"-i\", audio_path, \"-c\", \"copy\", out_path]\n",
    "    proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    if proc.returncode != 0:\n",
    "        raise RuntimeError(f\"FFmpeg failed:\\n{proc.stderr}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bde5a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_image(post: dict, outdir: str, *, max_retries=5) -> str:\n",
    "    url = post.get(\"url_overridden_by_dest\") or post.get(\"url\")\n",
    "    if not url:\n",
    "        raise RuntimeError(\"No image URL found.\")\n",
    "    title = safe_name(post.get(\"title\", \"reddit_image\"))\n",
    "    ext = os.path.splitext(urlparse(url).path)[1] or \".jpg\"\n",
    "    out = os.path.join(outdir, f\"{title}{ext}\")\n",
    "    print(f\"Downloading image → {out}\")\n",
    "    download_file(url, out, max_retries=max_retries)\n",
    "    return out\n",
    "\n",
    "def handle_gallery(post: dict, outdir: str, *, max_retries=5) -> list:\n",
    "    media_meta = post.get(\"media_metadata\", {})\n",
    "    gallery_data = post.get(\"gallery_data\", {}).get(\"items\", [])\n",
    "    if not media_meta or not gallery_data:\n",
    "        raise RuntimeError(\"No gallery metadata found.\")\n",
    "    title = safe_name(post.get(\"title\", \"reddit_gallery\"))\n",
    "    gallery_dir = os.path.join(outdir, title)\n",
    "    ensure_dir(gallery_dir)\n",
    "    outputs = []\n",
    "    for i, item in enumerate(gallery_data, 1):\n",
    "        media_id = item[\"media_id\"]\n",
    "        meta = media_meta[media_id]\n",
    "        if \"p\" in meta and meta[\"p\"]:\n",
    "            candidate = meta[\"p\"][-1][\"u\"]\n",
    "        else:\n",
    "            candidate = meta[\"s\"][\"u\"]\n",
    "        candidate = candidate.replace(\"&amp;\", \"&\")\n",
    "        ext = \".jpg\"\n",
    "        if \"m\" in meta.get(\"s\", {}):\n",
    "            mt = meta[\"s\"][\"m\"]\n",
    "            if \"png\" in mt: ext = \".png\"\n",
    "            elif \"gif\" in mt: ext = \".gif\"\n",
    "        out = os.path.join(gallery_dir, f\"{i:02d}{ext}\")\n",
    "        print(f\"Downloading gallery item {i} → {out}\")\n",
    "        download_file(candidate, out, max_retries=max_retries)\n",
    "        outputs.append(out)\n",
    "    return outputs\n",
    "\n",
    "def handle_video(post: dict, outdir: str, *, max_retries=5) -> str:\n",
    "    title = safe_name(post.get(\"title\", \"reddit_video\"))\n",
    "    ensure_dir(outdir)\n",
    "\n",
    "    reddit_video = None\n",
    "    if post.get(\"secure_media\") and post[\"secure_media\"].get(\"reddit_video\"):\n",
    "        reddit_video = post[\"secure_media\"][\"reddit_video\"]\n",
    "    elif post.get(\"media\") and post[\"media\"].get(\"reddit_video\"):\n",
    "        reddit_video = post[\"media\"][\"reddit_video\"]\n",
    "    elif post.get(\"crosspost_parent_list\"):\n",
    "        for parent in post[\"crosspost_parent_list\"]:\n",
    "            if parent.get(\"secure_media\") and parent[\"secure_media\"].get(\"reddit_video\"):\n",
    "                reddit_video = parent[\"secure_media\"][\"reddit_video\"]; break\n",
    "            if parent.get(\"media\") and parent[\"media\"].get(\"reddit_video\"):\n",
    "                reddit_video = parent[\"media\"][\"reddit_video\"]; break\n",
    "\n",
    "    if not reddit_video:\n",
    "        url = post.get(\"url_overridden_by_dest\") or post.get(\"url\", \"\")\n",
    "        if \"v.redd.it\" in url:\n",
    "            dash_url = url.rstrip(\"/\") + \"/DASHPlaylist.mpd\"\n",
    "            reddit_video = {\"dash_url\": dash_url}\n",
    "        else:\n",
    "            raise RuntimeError(\"No reddit_video found on this post.\")\n",
    "\n",
    "    dash_url = reddit_video.get(\"dash_url\")\n",
    "    fallback = reddit_video.get(\"fallback_url\")\n",
    "\n",
    "    if dash_url:\n",
    "        print(f\"Fetching DASH manifest:\\n  {dash_url}\")\n",
    "        r = request_with_backoff(\"GET\", dash_url, max_retries=max_retries, timeout=30)\n",
    "        if r.status_code == 403:\n",
    "            alt = dash_url.replace(\"https://\", \"http://\")\n",
    "            r = request_with_backoff(\"GET\", alt, max_retries=max_retries, timeout=30)\n",
    "        v_url, a_url = pick_best_from_mpd(r.text)\n",
    "        if not v_url and not a_url and fallback:\n",
    "            print(\"No representations in MPD; falling back to single MP4.\")\n",
    "            out = os.path.join(outdir, f\"{title}.mp4\")\n",
    "            download_file(fallback, out, max_retries=max_retries)\n",
    "            return out\n",
    "\n",
    "        v_path = os.path.join(outdir, f\"{title}.video.mp4\")\n",
    "        a_path = os.path.join(outdir, f\"{title}.audio.mp4\")\n",
    "        if v_url:\n",
    "            print(f\"Downloading best video:\\n  {v_url}\\n→ {v_path}\")\n",
    "            download_file(v_url, v_path, max_retries=max_retries)\n",
    "        if a_url:\n",
    "            print(f\"Downloading best audio:\\n  {a_url}\\n→ {a_path}\")\n",
    "            download_file(a_url, a_path, max_retries=max_retries)\n",
    "\n",
    "        if a_url:\n",
    "            if not has_ffmpeg():\n",
    "                raise RuntimeError(\"FFmpeg not found to merge audio+video. Install FFmpeg or add it to PATH.\")\n",
    "            out_path = os.path.join(outdir, f\"{title}.mp4\")\n",
    "            print(\"Merging A+V with FFmpeg…\")\n",
    "            merge_av(v_path, a_path, out_path)\n",
    "            for p in (v_path, a_path):\n",
    "                try: os.remove(p)\n",
    "                except Exception: pass\n",
    "            return out_path\n",
    "        else:\n",
    "            final = os.path.join(outdir, f\"{title}.mp4\")\n",
    "            os.replace(v_path, final)\n",
    "            return final\n",
    "\n",
    "    out = os.path.join(outdir, f\"{title}.mp4\")\n",
    "    print(f\"Downloading fallback MP4 (may be video-only):\\n  {fallback}\\n→ {out}\")\n",
    "    download_file(fallback, out, max_retries=max_retries)\n",
    "    return out\n",
    "\n",
    "def log_external_link(post: dict, post_url: str, links_csv: str):\n",
    "    external_url = post.get(\"url_overridden_by_dest\") or post.get(\"url\") or \"\"\n",
    "    title = post.get(\"title\", \"\")\n",
    "    domain = (post.get(\"domain\") or \"\").lower()\n",
    "    row = [post_url, external_url, title, domain]\n",
    "    exists = os.path.exists(links_csv)\n",
    "    with open(links_csv, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        if not exists:\n",
    "            w.writerow([\"post_url\", \"external_url\", \"title\", \"domain\"])\n",
    "        w.writerow(row)\n",
    "    print(f\"Logged external link → {links_csv}\\n  {external_url}\")\n",
    "\n",
    "def classify_and_handle(url: str, outdir: str, links_csv: str, *, max_retries=5) -> list:\n",
    "    ensure_dir(outdir)\n",
    "    post = get_post_json(url, max_retries=max_retries)\n",
    "\n",
    "    is_gallery = post.get(\"is_gallery\", False)\n",
    "    post_hint = (post.get(\"post_hint\") or \"\").lower()\n",
    "    domain = (post.get(\"domain\") or \"\").lower()\n",
    "    p_url = (post.get(\"url_overridden_by_dest\") or post.get(\"url\") or \"\").lower()\n",
    "\n",
    "    is_reddit_image = post_hint == \"image\" or domain in (\"i.redd.it\", \"i.reddituploads.com\")\n",
    "    is_reddit_video = \"v.redd.it\" in p_url or \\\n",
    "                      (post.get(\"secure_media\") and post[\"secure_media\"].get(\"reddit_video\")) or \\\n",
    "                      (post.get(\"media\") and post[\"media\"].get(\"reddit_video\")) or \\\n",
    "                      bool(post.get(\"crosspost_parent_list\"))\n",
    "\n",
    "    results = []\n",
    "    if is_gallery:\n",
    "        print(\"Detected gallery.\")\n",
    "        results.extend(handle_gallery(post, outdir, max_retries=max_retries))\n",
    "    elif is_reddit_video:\n",
    "        print(\"Detected hosted video (v.redd.it or reddit_video).\")\n",
    "        results.append(handle_video(post, outdir, max_retries=max_retries))\n",
    "    elif is_reddit_image:\n",
    "        print(\"Detected single image.\")\n",
    "        results.append(handle_image(post, outdir, max_retries=max_retries))\n",
    "    else:\n",
    "        print(\"Detected external link; logging instead of downloading.\")\n",
    "        log_external_link(post, url, links_csv)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a59bbbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 1 (25 items)…\n",
      "\n",
      "[1] >>> https://www.reddit.com/r/u_Environmental_Sail68/comments/11nfo8t/jackandjill_peachiikitten_lilykawaii_dirtydallas1\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/u_Environmental_Sail68/comments/11nfo8t/jackandjill_peachiikitten_lilykawaii_dirtydallas1/\n",
      "\n",
      "[2] >>> https://www.reddit.com/r/SluttyConfessions/comments/tfucs4/how_i_f18_went_from_innocent_college_virgin_to\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/tfucs4/how_i_f18_went_from_innocent_college_virgin_to/\n",
      "\n",
      "[3] >>> https://www.reddit.com/r/SluttyConfessions/comments/tex5xg/how_i_f18_went_from_innocent_college_virgin_to\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/tex5xg/how_i_f18_went_from_innocent_college_virgin_to/\n",
      "\n",
      "[4] >>> https://www.reddit.com/r/SluttyConfessions/comments/tc0kxn/how_i_f18_went_from_innocent_college_virgin_to\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/tc0kxn/how_i_f18_went_from_innocent_college_virgin_to/\n",
      "\n",
      "[5] >>> https://www.reddit.com/r/SluttyConfessions/comments/tbxv2a/how_i_f18_went_from_innocent_college_virgin_to\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/tbxv2a/how_i_f18_went_from_innocent_college_virgin_to/\n",
      "\n",
      "[6] >>> https://www.reddit.com/r/SluttyConfessions/comments/tfmh7h/i_gave_a_much_younger_coworker_head_in_the\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/tfmh7h/i_gave_a_much_younger_coworker_head_in_the/\n",
      "\n",
      "[7] >>> https://www.reddit.com/r/SluttyConfessions/comments/nljzbf/f19_i_just_agreed_to_be_free_use_for_a_house_of_6\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/nljzbf/f19_i_just_agreed_to_be_free_use_for_a_house_of_6/\n",
      "\n",
      "[8] >>> https://www.reddit.com/r/SluttyConfessions/comments/n15k5n/my_exwife_blows_me_more_now_that_we_are_divorced\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/n15k5n/my_exwife_blows_me_more_now_that_we_are_divorced/\n",
      "\n",
      "[9] >>> https://www.reddit.com/r/SluttyConfessions/comments/t4at3m/when_i_was_in_high_school_i_f18_got_gangbanged_by\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/t4at3m/when_i_was_in_high_school_i_f18_got_gangbanged_by/\n",
      "\n",
      "[10] >>> https://www.reddit.com/r/SluttyConfessions/comments/slq7tx/i_24f_had_an_unprotected_gangbang_last_month\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/slq7tx/i_24f_had_an_unprotected_gangbang_last_month/\n",
      "\n",
      "[11] >>> https://www.reddit.com/r/SluttyConfessions/comments/tf5sts/i_f25_woke_up_to_him_m25_using_me_as_his_comfort\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/tf5sts/i_f25_woke_up_to_him_m25_using_me_as_his_comfort/\n",
      "\n",
      "[12] >>> https://www.reddit.com/r/RealGirls/comments/i0kmfd/quarantine_has_forced_me_to_get_creative\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.redgifs.com/watch/gorgeousfrightenedcarpenterant\n",
      "\n",
      "[13] >>> https://www.reddit.com/r/SluttyConfessions/comments/nlaj7k/i_25f_turned_my_friend_24f_into_an_absolute_cock\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/nlaj7k/i_25f_turned_my_friend_24f_into_an_absolute_cock/\n",
      "\n",
      "[14] >>> https://www.reddit.com/r/SluttyConfessions/comments/fu8o1h/a_guy_20m_from_work_found_my_19f_nsfw_reddit\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/fu8o1h/a_guy_20m_from_work_found_my_19f_nsfw_reddit/\n",
      "\n",
      "[15] >>> https://www.reddit.com/r/SluttyConfessions/comments/nj1k7v/33f_even_if_a_guy_im_having_sex_with_is_small_i\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/nj1k7v/33f_even_if_a_guy_im_having_sex_with_is_small_i/\n",
      "\n",
      "[16] >>> https://www.reddit.com/r/SluttyConfessions/comments/mpca6j/as_a_female_kindergarten_teacher_f_23_i_get\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/mpca6j/as_a_female_kindergarten_teacher_f_23_i_get/\n",
      "\n",
      "[17] >>> https://www.reddit.com/r/SluttyConfessions/comments/t45lv0/f_22_deepthroated_at_a_rave\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/t45lv0/f_22_deepthroated_at_a_rave/\n",
      "\n",
      "[18] >>> https://www.reddit.com/r/sexstories/comments/tczvf9/f_35_i_am_a_policewoman_this_is_the_story_about\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/sexstories/comments/tczvf9/f_35_i_am_a_policewoman_this_is_the_story_about/\n",
      "\n",
      "[19] >>> https://www.reddit.com/r/SluttyConfessions/comments/tck7ff/my_life_as_a_free_use_girlfriend_did_we_go_too_far\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/tck7ff/my_life_as_a_free_use_girlfriend_did_we_go_too_far/\n",
      "\n",
      "[20] >>> https://www.reddit.com/r/gonewildstories/comments/oawmq9/i_curtsy_to_the_men_that_ive_fucked_mf\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/gonewildstories/comments/oawmq9/i_curtsy_to_the_men_that_ive_fucked_mf/\n",
      "\n",
      "[21] >>> https://www.reddit.com/r/SluttyConfessions/comments/ldhs58/25f_i_gave_my_boyfriend_permission_to_order_me\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/ldhs58/25f_i_gave_my_boyfriend_permission_to_order_me/\n",
      "\n",
      "[22] >>> https://www.reddit.com/r/SluttyConfessions/comments/krv16d/25f_boyfriend_won_a_bet_so_i_have_to_service_him\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/krv16d/25f_boyfriend_won_a_bet_so_i_have_to_service_him/\n",
      "\n",
      "[23] >>> https://www.reddit.com/r/SluttyConfessions/comments/t4dsm5/f25_my_ex_makes_me_suck_him_whenever_i_apologize\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/t4dsm5/f25_my_ex_makes_me_suck_him_whenever_i_apologize/\n",
      "\n",
      "[24] >>> https://www.reddit.com/r/SluttyConfessions/comments/t7prc8/i_just_fucked_the_thickest_cock_ive_ever_seen\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/t7prc8/i_just_fucked_the_thickest_cock_ive_ever_seen/\n",
      "\n",
      "[25] >>> https://www.reddit.com/r/SluttyConfessions/comments/szlm3g/i_30f_fucked_a_guy_from_tinder_that_my_fwb_picked\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/szlm3g/i_30f_fucked_a_guy_from_tinder_that_my_fwb_picked/\n",
      "\n",
      "Sleeping 90s between batches to avoid rate limits…\n",
      "\n",
      "Processing batch 2 (25 items)…\n",
      "\n",
      "[26] >>> https://www.reddit.com/r/SluttyConfessions/comments/svjhir/28f_in_a_relationship_i_got_finger_fucked_in_a\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/svjhir/28f_in_a_relationship_i_got_finger_fucked_in_a/\n",
      "\n",
      "[27] >>> https://www.reddit.com/r/SluttyConfessions/comments/svbc4y/28_f_in_a_relationship_part_2_of_how_i_became_a\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/svbc4y/28_f_in_a_relationship_part_2_of_how_i_became_a/\n",
      "\n",
      "[28] >>> https://www.reddit.com/r/SluttyConfessions/comments/sv5dn8/28_f_in_a_relationship_i_became_a_collateral_fuck\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/sv5dn8/28_f_in_a_relationship_i_became_a_collateral_fuck/\n",
      "\n",
      "[29] >>> https://www.reddit.com/r/SluttyConfessions/comments/swn777/i_f23_gave_my_brotherfianc%C3%A9_m21_and_girlfriend\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/swn777/i_f23_gave_my_brotherfiancé_m21_and_girlfriend/\n",
      "\n",
      "[30] >>> https://www.reddit.com/r/SluttyConfessions/comments/swsx9p/i_fucked_a_stranger_in_the_bathroom_while_on_a\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/swsx9p/i_fucked_a_stranger_in_the_bathroom_while_on_a/\n",
      "\n",
      "[31] >>> https://www.reddit.com/r/SluttyConfessions/comments/swrxra/i_38f_love_feeling_cum_ooze_out_of_my_pussy\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/swrxra/i_38f_love_feeling_cum_ooze_out_of_my_pussy/\n",
      "\n",
      "[32] >>> https://www.reddit.com/r/SluttyConfessions/comments/swk8fp/a_night_of_firsts_my_valentine_gave_me_multiple\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/swk8fp/a_night_of_firsts_my_valentine_gave_me_multiple/\n",
      "\n",
      "[33] >>> https://www.reddit.com/r/SluttyConfessions/comments/sw8vcg/joining_bathrooms_in_mountain_cabin_caught_me_out\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/sw8vcg/joining_bathrooms_in_mountain_cabin_caught_me_out/\n",
      "\n",
      "[34] >>> https://www.reddit.com/r/SluttyConfessions/comments/sw6az0/masturbating_while_listening_to_hubby_fuck\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/sw6az0/masturbating_while_listening_to_hubby_fuck/\n",
      "\n",
      "[35] >>> https://www.reddit.com/r/SluttyConfessions/comments/lbjagu/my_19f_boyfriend_21m_has_turned_me_into_his_cum\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/lbjagu/my_19f_boyfriend_21m_has_turned_me_into_his_cum/\n",
      "\n",
      "[36] >>> https://www.reddit.com/r/SluttyConfessions/comments/srk9za/24f_admitted_i_was_free_use_at_a_party\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/srk9za/24f_admitted_i_was_free_use_at_a_party/\n",
      "\n",
      "[37] >>> https://www.reddit.com/r/SluttyConfessions/comments/sqtcdw/27f_my_fianc%C3%A9_let_one_of_his_friends_use_and\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/sqtcdw/27f_my_fiancé_let_one_of_his_friends_use_and/\n",
      "\n",
      "[38] >>> https://www.reddit.com/r/SluttyConfessions/comments/sqsove/i_26f_blew_my_personal_trainer_28m_this_morning\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/sqsove/i_26f_blew_my_personal_trainer_28m_this_morning/\n",
      "\n",
      "[39] >>> https://www.reddit.com/r/EroticHypnosis/comments/lguhr2/boyfriend_has_forced_fetish_fetish\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/EroticHypnosis/comments/lguhr2/boyfriend_has_forced_fetish_fetish/\n",
      "\n",
      "[40] >>> https://www.reddit.com/r/stupidslutsclub/comments/sk8q7n/my_first_orgy\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/stupidslutsclub/comments/sk8q7n/my_first_orgy/\n",
      "\n",
      "[41] >>> https://www.reddit.com/r/gonewildstories/comments/3f3vm0/turning_my_gf_into_a_slut\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/gonewildstories/comments/3f3vm0/turning_my_gf_into_a_slut/\n",
      "\n",
      "[42] >>> https://www.reddit.com/r/SluttyConfessions/comments/fo2dbw/how_i_turned_my_girlfriend_into_a_slut\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/fo2dbw/how_i_turned_my_girlfriend_into_a_slut/\n",
      "\n",
      "[43] >>> https://www.reddit.com/r/SluttyConfessions/comments/spycns/i_didnt_give_my_coworker_a_choice\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/spycns/i_didnt_give_my_coworker_a_choice/\n",
      "\n",
      "[44] >>> https://www.reddit.com/r/SluttyConfessions/comments/sj9l2s/heres_the_story_of_the_hottest_encounter_i_36m\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/sj9l2s/heres_the_story_of_the_hottest_encounter_i_36m/\n",
      "\n",
      "[45] >>> https://www.reddit.com/r/SluttyConfessions/comments/sj7zyi/late_30s_group_nj_the_first_time_my_wifes_friend\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/sj7zyi/late_30s_group_nj_the_first_time_my_wifes_friend/\n",
      "\n",
      "[46] >>> https://www.reddit.com/r/SluttyConfessions/comments/p6kar1/i_25f_fucked_a_groom_on_a_bachelor_party_pt_2\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/p6kar1/i_25f_fucked_a_groom_on_a_bachelor_party_pt_2/\n",
      "\n",
      "[47] >>> https://www.reddit.com/r/SluttyConfessions/comments/p6dlm7/i_25f_fucked_a_groom_on_his_bachelor_party\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/p6dlm7/i_25f_fucked_a_groom_on_his_bachelor_party/\n",
      "\n",
      "[48] >>> https://www.reddit.com/r/dirtypenpals/comments/l7p04z/f4a_an_ancient_and_mythical_lewd_world_that_we\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/dirtypenpals/comments/l7p04z/f4a_an_ancient_and_mythical_lewd_world_that_we/\n",
      "\n",
      "[49] >>> https://www.reddit.com/r/stupidslutsclub/comments/slouk0/multiple_guys_came_in_me_f_at_our_office_party\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/stupidslutsclub/comments/slouk0/multiple_guys_came_in_me_f_at_our_office_party/\n",
      "\n",
      "[50] >>> https://www.reddit.com/r/SluttyConfessions/comments/soxjre/after_five_years_of_marriage_i_finally_let_my\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/soxjre/after_five_years_of_marriage_i_finally_let_my/\n",
      "\n",
      "Sleeping 90s between batches to avoid rate limits…\n",
      "\n",
      "Processing batch 3 (24 items)…\n",
      "\n",
      "[51] >>> https://www.reddit.com/r/stupidslutsclub/comments/9h3gd1/for_four_years_i_was_a_stupid_slut_in_the_us_army\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/stupidslutsclub/comments/9h3gd1/for_four_years_i_was_a_stupid_slut_in_the_us_army/\n",
      "\n",
      "[52] >>> https://www.reddit.com/r/stupidslutsclub/comments/9hm0ma/continued_for_four_years_i_was_a_stupid_slut_in\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/stupidslutsclub/comments/9hm0ma/continued_for_four_years_i_was_a_stupid_slut_in/\n",
      "\n",
      "[53] >>> https://www.reddit.com/r/SluttyConfessions/comments/sa81xy/f20_in_vegas_for_a_bachelor_party_to_provide_the\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/sa81xy/f20_in_vegas_for_a_bachelor_party_to_provide_the/\n",
      "\n",
      "[54] >>> https://www.reddit.com/r/gonewildstories/comments/sdg3en/f_had_a_wild_time_at_the_bachelor_party_in_vegas\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/gonewildstories/comments/sdg3en/f_had_a_wild_time_at_the_bachelor_party_in_vegas/\n",
      "\n",
      "[55] >>> https://www.reddit.com/r/gonewildstories/comments/sbul85/f_had_a_wild_time_at_the_bachelor_party_in_vegas\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/gonewildstories/comments/sbul85/f_had_a_wild_time_at_the_bachelor_party_in_vegas/\n",
      "\n",
      "[56] >>> https://www.reddit.com/r/gonewildstories/comments/sb2ta1/f_had_a_wild_time_at_the_bachelor_party_in_vegas\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/gonewildstories/comments/sb2ta1/f_had_a_wild_time_at_the_bachelor_party_in_vegas/\n",
      "\n",
      "[57] >>> https://www.reddit.com/r/SluttyConfessions/comments/se57aa/f20_had_a_wild_time_at_the_bachelor_party_in\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/se57aa/f20_had_a_wild_time_at_the_bachelor_party_in/\n",
      "\n",
      "[58] >>> https://www.reddit.com/r/SluttyConfessions/comments/jzqcya/i_25f_gifted_my_best_friend_22f_a_dildo_in_the\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/jzqcya/i_25f_gifted_my_best_friend_22f_a_dildo_in_the/\n",
      "\n",
      "[59] >>> https://www.reddit.com/r/SluttyConfessions/comments/r8jxwb/i_f22_was_gangbanged_for_my_birthday\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/SluttyConfessions/comments/r8jxwb/i_f22_was_gangbanged_for_my_birthday/\n",
      "\n",
      "[60] >>> https://www.reddit.com/r/gonewildstories/comments/4ogo37/college_party_first_time_19f_on_my_knees_surprise\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/gonewildstories/comments/4ogo37/college_party_first_time_19f_on_my_knees_surprise/\n",
      "\n",
      "[61] >>> https://www.reddit.com/r/gonewildstories/comments/4fdbr4/the_most_common_questions_people_ask_me_28f_about\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/gonewildstories/comments/4fdbr4/the_most_common_questions_people_ask_me_28f_about/\n",
      "\n",
      "[62] >>> https://www.reddit.com/r/gonewildstories/comments/3pdu3i/fwb_came_in_my_butt_f_just_hours_before_first\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/gonewildstories/comments/3pdu3i/fwb_came_in_my_butt_f_just_hours_before_first/\n",
      "\n",
      "[63] >>> https://www.reddit.com/r/NSFWIAMA/comments/hmjq5s/ive_participated_in_over_10_paid_gang_bangs_for\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/NSFWIAMA/comments/hmjq5s/ive_participated_in_over_10_paid_gang_bangs_for/\n",
      "\n",
      "[64] >>> https://www.reddit.com/r/NSFWIAMA/comments/bwi9m2/my_evolution_from_doing_paid_gang_bangs_to_a\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/NSFWIAMA/comments/bwi9m2/my_evolution_from_doing_paid_gang_bangs_to_a/\n",
      "\n",
      "[65] >>> https://www.reddit.com/r/NSFWIAMA/comments/7mo598/i_did_paid_gang_bangs_to_get_out_of_debt_and_pay\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/NSFWIAMA/comments/7mo598/i_did_paid_gang_bangs_to_get_out_of_debt_and_pay/\n",
      "\n",
      "[66] >>> https://www.reddit.com/r/gonewildstories/comments/7mjo4d/i_put_myself_through_med_school_doing_gang_bangs\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/gonewildstories/comments/7mjo4d/i_put_myself_through_med_school_doing_gang_bangs/\n",
      "\n",
      "[67] >>> https://www.reddit.com/r/gonewildstories/comments/44y6v2/i_28f_returned_to_a_vegas_sex_club_to_have_sex\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/gonewildstories/comments/44y6v2/i_28f_returned_to_a_vegas_sex_club_to_have_sex/\n",
      "\n",
      "[68] >>> https://www.reddit.com/r/sex/comments/irhty/my_experiences_and_tips_for_face_fucking_and_the\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/sex/comments/irhty/my_experiences_and_tips_for_face_fucking_and_the/\n",
      "\n",
      "[69] >>> https://www.reddit.com/r/TopsAndBottoms/comments/p0nu3z/preparing_my_hole_for_a_gangbang\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/TopsAndBottoms/comments/p0nu3z/preparing_my_hole_for_a_gangbang/\n",
      "\n",
      "[70] >>> https://www.reddit.com/r/AskReddit/comments/87w59r/whats_the_best_sexual_experience_youve_ever_had\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/AskReddit/comments/87w59r/whats_the_best_sexual_experience_youve_ever_had/\n",
      "\n",
      "[71] >>> https://www.reddit.com/r/stupidslutsclub/comments/6yvo25/whats_a_girl_to_do_a_day_before_her_first_gang\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/stupidslutsclub/comments/6yvo25/whats_a_girl_to_do_a_day_before_her_first_gang/\n",
      "\n",
      "[72] >>> https://www.reddit.com/r/MxRMods/comments/rry977/_\n",
      "Detected single image.\n",
      "Downloading image → downloads\\_.gif\n",
      "  _.gif  100% (1252384/1252384 bytes)\n",
      "Saved: downloads\\_.gif\n",
      "\n",
      "[73] >>> https://www.reddit.com/r/gonewildstories/comments/ahygvt/what_i_learned_from_being_gangbanged_group\n",
      "Detected external link; logging instead of downloading.\n",
      "Logged external link → external_links.csv\n",
      "  https://www.reddit.com/r/gonewildstories/comments/ahygvt/what_i_learned_from_being_gangbanged_group/\n",
      "\n",
      "[74] >>> https://www.reddit.com/r/rule34/comments/ntpgz9/hermione_granger_peter_pettigrew_4kolor_harry\n",
      "Detected single image.\n",
      "Downloading image → downloads\\Hermione Granger _amp_ Peter Pettigrew _4kolor_ _Harry Potter_.jpg\n",
      "  Hermione Granger _amp_ Peter Pettigrew _4kolor_ _Harry Potter_.jpg  100% (113403/113403 bytes)\n",
      "Saved: downloads\\Hermione Granger _amp_ Peter Pettigrew _4kolor_ _Harry Potter_.jpg\n"
     ]
    }
   ],
   "source": [
    "# ---- CLI / batching ----\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Reddit image/gallery/video downloader (no yt-dlp). External link posts are logged to CSV.\"\n",
    "    )\n",
    "    parser.add_argument(\"-u\", \"--url\", help=\"Single Reddit post URL\")\n",
    "    parser.add_argument(\"-c\", \"--csv\", dest=\"in_csv\", default=\"posts.csv\",\n",
    "                        help=\"CSV file with one Reddit URL per line\")\n",
    "    parser.add_argument(\"-o\", \"--outdir\", default=\"downloads\", help=\"Output folder for saved media\")\n",
    "    parser.add_argument(\"--links-csv\", default=\"external_links.csv\",\n",
    "                        help=\"Where to log external (non-Reddit-hosted) links\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=25, help=\"Posts per batch before pausing\")\n",
    "    parser.add_argument(\"--batch-pause\", type=int, default=90, help=\"Seconds to sleep between batches\")\n",
    "    parser.add_argument(\"--delay\", type=float, default=2.0, help=\"Seconds to sleep between posts\")\n",
    "    parser.add_argument(\"--max-retries\", type=int, default=5, help=\"Max HTTP retries on 429/5xx\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    ensure_dir(args.outdir)\n",
    "\n",
    "    def process_one(link: str, idx: int):\n",
    "        print(f\"\\n[{idx}] >>> {link}\")\n",
    "        try:\n",
    "            files = classify_and_handle(link, args.outdir, args.links_csv, max_retries=args.max_retries)\n",
    "            for p in files:\n",
    "                print(\"Saved:\", p)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed {link}: {e}\")\n",
    "\n",
    "    if args.url:\n",
    "        process_one(args.url, 1)\n",
    "        return\n",
    "\n",
    "    if os.path.exists(args.in_csv):\n",
    "        with open(args.in_csv, newline=\"\", encoding=\"utf-8\") as f:\n",
    "            rows = [row[0].strip() for row in csv.reader(f) if row and row[0].strip() and not row[0].strip().startswith(\"#\")]\n",
    "        total = len(rows)\n",
    "        i = 0\n",
    "        while i < total:\n",
    "            batch = rows[i:i + args.batch_size]\n",
    "            print(f\"\\nProcessing batch {i//args.batch_size + 1} ({len(batch)} items)…\")\n",
    "            for j, link in enumerate(batch, start=1):\n",
    "                process_one(link, i + j)\n",
    "                time.sleep(args.delay)  # polite delay between posts\n",
    "            i += args.batch_size\n",
    "            if i < total:\n",
    "                print(f\"\\nSleeping {args.batch_pause}s between batches to avoid rate limits…\")\n",
    "                time.sleep(args.batch_pause)\n",
    "        return\n",
    "\n",
    "    # Fallback interactive\n",
    "    url = input(\"Paste a Reddit post URL: \").strip()\n",
    "    process_one(url, 1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Primary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
