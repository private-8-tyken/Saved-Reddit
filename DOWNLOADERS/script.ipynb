{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4869123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    _ = load_dotenv(override=False)\n",
    "except Exception:\n",
    "    pass  # it's fine if python-dotenv isn't installed; fall back to system env\n",
    "\n",
    "REDDIT_CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\", \"\").strip()\n",
    "REDDIT_CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\", \"\").strip()\n",
    "REDDIT_USERNAME = os.getenv(\"REDDIT_USERNAME\", \"\").strip()\n",
    "REDDIT_PASSWORD = os.getenv(\"REDDIT_PASSWORD\", \"\").strip()\n",
    "REDDIT_USER_AGENT = os.getenv(\"REDDIT_USER_AGENT\", \"SavedRedditJSON/1.0\").strip()\n",
    "\n",
    "for k, v in {\n",
    "    \"REDDIT_CLIENT_ID\": REDDIT_CLIENT_ID,\n",
    "    \"REDDIT_CLIENT_SECRET\": \"[set]\" if REDDIT_CLIENT_SECRET else \"\",\n",
    "    \"REDDIT_USERNAME\": REDDIT_USERNAME,\n",
    "    \"REDDIT_PASSWORD\": \"[set]\" if REDDIT_PASSWORD else \"\",\n",
    "    \"REDDIT_USER_AGENT\": REDDIT_USER_AGENT,\n",
    "}.items():\n",
    "    if not v:\n",
    "        print(f\"⚠️ Missing {k}. Set it via environment or a .env file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deda5f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import base64\n",
    "import requests\n",
    "from collections import Counter\n",
    "\n",
    "OAUTH_TOKEN_URL = \"https://www.reddit.com/api/v1/access_token\"\n",
    "\n",
    "_token_cache = {\"access_token\": None, \"expires_at\": 0}\n",
    "\n",
    "def _basic_auth_header(client_id, client_secret):\n",
    "    pair = f\"{client_id}:{client_secret}\".encode(\"utf-8\")\n",
    "    return \"Basic \" + base64.b64encode(pair).decode(\"ascii\")\n",
    "\n",
    "def get_oauth_token(force=False, scope=\"read history\"):\n",
    "    \"\"\"Fetch and cache an OAuth token using password grant.\"\"\"\n",
    "    if not force and _token_cache[\"access_token\"] and time.time() < _token_cache[\"expires_at\"] - 30:\n",
    "        return _token_cache[\"access_token\"]\n",
    "\n",
    "    if not (REDDIT_CLIENT_ID and REDDIT_CLIENT_SECRET and REDDIT_USERNAME and REDDIT_PASSWORD):\n",
    "        raise RuntimeError(\"Reddit OAuth env vars are not fully set.\")\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": _basic_auth_header(REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET),\n",
    "        \"User-Agent\": REDDIT_USER_AGENT or \"SavedRedditJSON/1.0\",\n",
    "    }\n",
    "    data = {\n",
    "        \"grant_type\": \"password\",\n",
    "        \"username\": REDDIT_USERNAME,\n",
    "        \"password\": REDDIT_PASSWORD,\n",
    "        \"scope\": scope,  # minimal scopes we need for reading\n",
    "    }\n",
    "    r = requests.post(OAUTH_TOKEN_URL, headers=headers, data=data, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "    tok = js.get(\"access_token\")\n",
    "    if not tok:\n",
    "        raise RuntimeError(f\"OAuth failed: {js}\")\n",
    "    _token_cache[\"access_token\"] = tok\n",
    "    _token_cache[\"expires_at\"] = time.time() + int(js.get(\"expires_in\", 3600))\n",
    "    return tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8457222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "from tqdm.auto import tqdm\n",
    "import random, json, csv, os, re, time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\"User-Agent\": REDDIT_USER_AGENT or \"SavedRedditJSON/1.0\"})\n",
    "\n",
    "def ensure_bearer():\n",
    "    tok = get_oauth_token(force=False)\n",
    "    SESSION.headers[\"Authorization\"] = f\"Bearer {tok}\"\n",
    "\n",
    "def maybe_oauth_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize any reddit URL to the OAuth domain with raw_json=1.\n",
    "    Accepts:\n",
    "      - https://www.reddit.com/r/.../comments/ID/... \n",
    "      - https://old.reddit.com/...\n",
    "      - permalink paths like /r/.../comments/ID/...\n",
    "    Returns:\n",
    "      https://oauth.reddit.com/r/.../comments/ID/.../.json?raw_json=1\n",
    "    \"\"\"\n",
    "    u = url.strip()\n",
    "    if not u:\n",
    "        raise ValueError(\"Empty URL\")\n",
    "    if not u.startswith(\"http\"):\n",
    "        # treat it as a permalink path\n",
    "        u = \"https://oauth.reddit.com\" + (u if u.startswith(\"/\") else \"/\" + u)\n",
    "    parsed = urlparse(u)\n",
    "    host = \"oauth.reddit.com\"\n",
    "    path = parsed.path\n",
    "    if not path.endswith(\"/\"):\n",
    "        path += \"/\"\n",
    "    if not path.endswith(\".json\"):\n",
    "        path += \".json\"\n",
    "    return f\"https://{host}{path}?raw_json=1\"\n",
    "\n",
    "def request_with_backoff(method, url, *, max_retries=5, timeout=30):\n",
    "    ensure_bearer()\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        try:\n",
    "            resp = SESSION.request(method, url, timeout=timeout)\n",
    "        except requests.RequestException as e:\n",
    "            if attempt >= max_retries:\n",
    "                raise\n",
    "            time.sleep(min(60, 2 ** attempt) + random.uniform(0, 0.5))\n",
    "            attempt += 1\n",
    "            continue\n",
    "\n",
    "        # Handle unauthorized → refresh token once\n",
    "        if resp.status_code == 401 and attempt < max_retries:\n",
    "            ensure_bearer()  # refresh\n",
    "            attempt += 1\n",
    "            time.sleep(1.0)\n",
    "            continue\n",
    "\n",
    "        # Handle rate limit or server errors\n",
    "        if resp.status_code in (429,) or 500 <= resp.status_code < 600:\n",
    "            if attempt >= max_retries:\n",
    "                resp.raise_for_status()\n",
    "            retry_after = resp.headers.get(\"Retry-After\")\n",
    "            if retry_after:\n",
    "                try:\n",
    "                    sleep = float(retry_after)\n",
    "                except ValueError:\n",
    "                    sleep = 10.0\n",
    "            else:\n",
    "                sleep = min(60, 2 ** attempt) + random.uniform(0, 0.5)\n",
    "            time.sleep(sleep)\n",
    "            attempt += 1\n",
    "            continue\n",
    "\n",
    "        if 400 <= resp.status_code < 500:\n",
    "            resp.raise_for_status()\n",
    "\n",
    "        return resp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "381b7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def now_iso():\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def fetch_post_and_comments_oauth(url: str):\n",
    "    \"\"\"Use OAuth domain + raw_json=1 to fetch the familiar 2-element JSON list.\"\"\"\n",
    "    api_url = maybe_oauth_url(url)\n",
    "    r = request_with_backoff(\"GET\", api_url, max_retries=5, timeout=30)\n",
    "    data = r.json()\n",
    "    if not (isinstance(data, list) and len(data) >= 2):\n",
    "        raise RuntimeError(\"Unexpected Reddit JSON format\")\n",
    "    post_listing = data[0][\"data\"][\"children\"]\n",
    "    if not post_listing:\n",
    "        raise RuntimeError(\"Post listing empty\")\n",
    "    post = post_listing[0][\"data\"]\n",
    "    comments_listing = data[1]\n",
    "    return post, comments_listing\n",
    "\n",
    "def extract_comments(listing_node, *, max_depth=6, max_count=5000):\n",
    "    \"\"\"\n",
    "    Convert Reddit's raw 'Listing' tree into a UI-friendly array:\n",
    "    [{id, author, body, score, created_utc, permalink, is_submitter, parent_id, replies:[...]}, ...]\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    remaining = [max_count]\n",
    "\n",
    "    def from_node(node, depth):\n",
    "        if remaining[0] <= 0 or depth > max_depth or not isinstance(node, dict):\n",
    "            return None\n",
    "        kind = node.get(\"kind\")\n",
    "        data = node.get(\"data\", {})\n",
    "\n",
    "        if kind == \"t1\":\n",
    "            remaining[0] -= 1\n",
    "            item = {\n",
    "                \"id\": data.get(\"id\"),\n",
    "                \"author\": data.get(\"author\") or \"[deleted]\",\n",
    "                \"author_fullname\": data.get(\"author_fullname\"),\n",
    "                \"body\": data.get(\"body\") or \"\",          # CommentThread uses plain text body\n",
    "                \"score\": data.get(\"score\"),\n",
    "                \"created_utc\": data.get(\"created_utc\"),\n",
    "                \"permalink\": \"https://www.reddit.com\" + (data.get(\"permalink\") or \"\"),\n",
    "                \"is_submitter\": data.get(\"is_submitter\"),\n",
    "                \"parent_id\": data.get(\"parent_id\"),\n",
    "                \"replies\": []\n",
    "            }\n",
    "            # replies can be \"\" (empty string), None, or a Listing\n",
    "            replies = data.get(\"replies\")\n",
    "            if isinstance(replies, dict):\n",
    "                for ch in replies.get(\"data\", {}).get(\"children\", []):\n",
    "                    child = from_node(ch, depth + 1)\n",
    "                    if child is not None:\n",
    "                        item[\"replies\"].append(child)\n",
    "            return item\n",
    "\n",
    "        if kind == \"Listing\":\n",
    "            for ch in data.get(\"children\", []):\n",
    "                c = from_node(ch, depth)\n",
    "                if c is not None:\n",
    "                    out.append(c)\n",
    "\n",
    "        return None\n",
    "\n",
    "    from_node(listing_node, 1)\n",
    "    return out\n",
    "\n",
    "def classify_media_kind(post: dict) -> str:\n",
    "    url = (post.get(\"url_overridden_by_dest\") or post.get(\"url\") or \"\").lower()\n",
    "    domain = (post.get(\"domain\") or \"\").lower()\n",
    "    post_hint = (post.get(\"post_hint\") or \"\").lower()\n",
    "    if post.get(\"is_gallery\", False): return \"gallery\"\n",
    "    elif \"v.redd.it\" in url or \\\n",
    "       (post.get(\"secure_media\") and post[\"secure_media\"].get(\"reddit_video\")) or \\\n",
    "       (post.get(\"media\") and post[\"media\"].get(\"reddit_video\")) or \\\n",
    "       bool(post.get(\"crosspost_parent_list\")):\n",
    "        return \"video\"\n",
    "    elif post_hint == \"image\" or domain in (\"i.redd.it\", \"i.reddituploads.com\"): return \"image\"\n",
    "    elif post.get(\"is_self\", False): return \"self\"\n",
    "    return \"external\"\n",
    "\n",
    "def make_archive_object(post, comments_listing, *, include_comments=True, comments_depth=6, comments_limit=5000):\n",
    "    obj = {\n",
    "        \"archived_at\": now_iso(),\n",
    "        \"reddit_fullname\": post.get(\"name\"),\n",
    "        \"reddit_id\": post.get(\"id\"),\n",
    "        \"permalink\": \"https://www.reddit.com\" + post.get(\"permalink\", \"\"),\n",
    "        \"title\": post.get(\"title\", \"\"),\n",
    "        \"selftext\": post.get(\"selftext\", \"\"),\n",
    "        \"author\": post.get(\"author\"),\n",
    "        \"author_fullname\": post.get(\"author_fullname\"),\n",
    "        \"subreddit\": post.get(\"subreddit\"),\n",
    "        \"subreddit_id\": post.get(\"subreddit_id\"),\n",
    "        \"created_utc\": post.get(\"created_utc\"),\n",
    "        \"is_self\": post.get(\"is_self\", False),\n",
    "        \"url\": post.get(\"url_overridden_by_dest\") or post.get(\"url\"),\n",
    "        \"domain\": post.get(\"domain\"),\n",
    "        \"post_hint\": post.get(\"post_hint\"),\n",
    "        \"is_gallery\": post.get(\"is_gallery\", False),\n",
    "        \"over_18\": post.get(\"over_18\", False),\n",
    "        \"spoiler\": post.get(\"spoiler\", False),\n",
    "        \"link_flair_text\": post.get(\"link_flair_text\"),\n",
    "        \"is_original_content\": post.get(\"is_original_content\", False),\n",
    "        \"stickied\": post.get(\"stickied\", False),\n",
    "        \"locked\": post.get(\"locked\", False),\n",
    "        \"edited\": post.get(\"edited\"),\n",
    "        \"num_comments\": post.get(\"num_comments\"),\n",
    "        \"score\": post.get(\"score\"),\n",
    "        \"upvote_ratio\": post.get(\"upvote_ratio\"),\n",
    "        \"media\": None,          # schema parity; no downloads\n",
    "        \"external_link\": None,  # set if media_kind == \"external\"\n",
    "        \"raw_post\": post,\n",
    "        \"raw_comments\": None,   # keep the full Reddit structure for fidelity\n",
    "        # new meta about truncation\n",
    "        \"comments_truncated_by_depth\": False,\n",
    "        \"comments_truncated_by_limit\": False,\n",
    "    }\n",
    "\n",
    "    if include_comments:\n",
    "        # always keep raw\n",
    "        obj[\"raw_comments\"] = comments_listing\n",
    "        # build the UI-friendly list\n",
    "        comments = extract_comments(\n",
    "            comments_listing, max_depth=comments_depth, max_count=comments_limit\n",
    "        )\n",
    "        obj[\"comments\"] = comments\n",
    "        # heuristics to mark truncation (best-effort)\n",
    "        obj[\"comments_truncated_by_depth\"] = bool(comments_depth)  # informative only\n",
    "        obj[\"comments_truncated_by_limit\"] = bool(comments_limit and len(comments) >= comments_limit)\n",
    "\n",
    "    return obj\n",
    "\n",
    "def select_bucket(media_kind: str) -> str:\n",
    "    if media_kind in (\"image\", \"gallery\", \"video\"):\n",
    "        return \"media\"\n",
    "    elif media_kind == \"external\":\n",
    "        return \"external\"\n",
    "    return \"text\"\n",
    "\n",
    "def write_json_file(archive_obj, out_root, jsonl_path=None, skip_existing=True):\n",
    "    import os, json\n",
    "    rid = archive_obj.get(\"reddit_id\") or \"post\"\n",
    "    fname = f\"{rid}.json\"\n",
    "    bucket = select_bucket(archive_obj.get(\"media_kind\"))\n",
    "    outdir_bucket = os.path.join(out_root, bucket)\n",
    "    os.makedirs(outdir_bucket, exist_ok=True)\n",
    "    path = os.path.join(outdir_bucket, fname)\n",
    "\n",
    "    # If skipping existing, only truly skip when the file already has a 'comments' array.\n",
    "    if skip_existing and os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                existing = json.load(f)\n",
    "            if isinstance(existing.get(\"comments\"), list) and len(existing[\"comments\"]) > 0:\n",
    "                return path, bucket, \"skipped\"\n",
    "            # else: upgrade this file by writing comments now\n",
    "        except Exception:\n",
    "            # if corrupt/unreadable, fall through to rewrite\n",
    "            pass\n",
    "\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(archive_obj, f, ensure_ascii=False, indent=2)\n",
    "    if jsonl_path:\n",
    "        with open(jsonl_path, \"a\", encoding=\"utf-8\") as jf:\n",
    "            jf.write(json.dumps(archive_obj, ensure_ascii=False) + \"\\n\")\n",
    "    return path, bucket, (\"updated\" if os.path.exists(path) else \"written\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f89f87a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d912bdb5be470582a2ea3403e5e8b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 1:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] https://www.reddit.com/r/CNC_Connect/comments/189a9ms/18_f4m_losangeles_success_story :: HTTPError: 404 Client Error: Not Found for url: https://oauth.reddit.com/r/CNC_Connect/comments/189a9ms/18_f4m_losangeles_success_story/.json?raw_json=1\n",
      "[ERROR] https://www.reddit.com/r/CNC_Connect/comments/1frx6uh/24f4m_co_success_hotel_bar_flirting_turns_dark :: HTTPError: 404 Client Error: Not Found for url: https://oauth.reddit.com/r/CNC_Connect/comments/1frx6uh/24f4m_co_success_hotel_bar_flirting_turns_dark/.json?raw_json=1\n",
      "[ERROR] https://www.reddit.com/r/u_Nikaniikaa/comments/unn3cn/verification_video :: HTTPError: 403 Client Error: Forbidden for url: https://oauth.reddit.com/r/u_Nikaniikaa/comments/unn3cn/verification_video/.json?raw_json=1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1c365916374909904676106f6ac3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cooldown: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38cdc7b2c4a488b845449a0258336dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 2:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done. Written: 0, Skipped: 10, Failed: 3.\n"
     ]
    }
   ],
   "source": [
    "# ---- User knobs ----\n",
    "INPUT_CSV = 'links.csv'            # e.g., \"urls.csv\" (first column = reddit URLs/permalinks)\n",
    "SINGLE_URL = None           # e.g., \"https://www.reddit.com/r/whatever/comments/abc123/...\"\n",
    "OUTDIR = \"out\"     # root folder for outputs\n",
    "JSONL_PATH = None           # e.g., \"archive.jsonl\" to append JSON lines too\n",
    "COMMENTS_DEPTH = 1000\n",
    "COMMENTS_LIMIT = 100000\n",
    "DELAY_BETWEEN = 0.01         # seconds between posts\n",
    "BATCH_SIZE = 1000\n",
    "BATCH_PAUSE = 0            # seconds between batches\n",
    "\n",
    "def process_one(link: str):\n",
    "    post, comments_listing = fetch_post_and_comments_oauth(link)\n",
    "    archive = make_archive_object(\n",
    "        post, comments_listing,\n",
    "        include_comments=True,\n",
    "        comments_depth=COMMENTS_DEPTH,\n",
    "        comments_limit=COMMENTS_LIMIT\n",
    "    )\n",
    "    media_kind = classify_media_kind(post)\n",
    "    archive[\"media_kind\"] = media_kind\n",
    "    if media_kind == \"external\":\n",
    "        archive[\"external_link\"] = post.get(\"url_overridden_by_dest\") or post.get(\"url\")\n",
    "\n",
    "    out_path, bucket, status = write_json_file(archive, OUTDIR, JSONL_PATH, skip_existing=True)\n",
    "    return {\n",
    "        \"id\": archive.get(\"reddit_id\"),\n",
    "        \"bucket\": bucket,\n",
    "        \"link\": archive.get(\"permalink\"),\n",
    "        \"path\": out_path,\n",
    "        \"status\": status,   # \"written\" | \"updated\" | \"skipped\"\n",
    "    }\n",
    "\n",
    "# Build list of links\n",
    "links = []\n",
    "if SINGLE_URL:\n",
    "    links = [SINGLE_URL.strip()]\n",
    "elif INPUT_CSV and os.path.exists(INPUT_CSV):\n",
    "    with open(INPUT_CSV, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        links = [row[0].strip() for row in csv.reader(f) if row and row[0].strip() and not row[0].strip().startswith(\"#\")]\n",
    "\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "totals = Counter()  # counts written, skipped, failed\n",
    "\n",
    "def _set_postfix(pbar, info):\n",
    "    pbar.set_postfix({\n",
    "        \"id\": info.get(\"id\") or \"n/a\",\n",
    "        \"bucket\": info.get(\"bucket\") or \"-\",\n",
    "        \"status\": info.get(\"status\") or \"-\",\n",
    "    })\n",
    "\n",
    "if links:\n",
    "    i = 0\n",
    "    while i < len(links):\n",
    "        batch = links[i:i+BATCH_SIZE]\n",
    "        pbar = tqdm(batch, desc=f\"Batch {i//BATCH_SIZE + 1}\", leave=False)\n",
    "        for link in pbar:\n",
    "            try:\n",
    "                info = process_one(link)\n",
    "                totals[info[\"status\"]] += 1\n",
    "                _set_postfix(pbar, info)\n",
    "            except Exception as e:\n",
    "                totals[\"failed\"] += 1\n",
    "                _set_postfix(pbar, {\"id\": \"-\", \"bucket\": \"-\", \"status\": \"failed\"})\n",
    "                # also print a one-line error without breaking the bar\n",
    "                tqdm.write(f\"[ERROR] {link} :: {type(e).__name__}: {str(e)[:200]}\")\n",
    "            time.sleep(DELAY_BETWEEN)\n",
    "        i += BATCH_SIZE\n",
    "        if i < len(links):\n",
    "            for _ in tqdm(range(BATCH_PAUSE), desc=\"Cooldown\", leave=False):\n",
    "                time.sleep(1)\n",
    "else:\n",
    "    link = input(\"Paste a Reddit post URL (or permalink path): \").strip()\n",
    "    pbar = tqdm([link], desc=\"Processing\", leave=False)\n",
    "    for l in pbar:\n",
    "        try:\n",
    "            info = process_one(l)\n",
    "            totals[info[\"status\"]] += 1\n",
    "            _set_postfix(pbar, info)\n",
    "        except Exception as e:\n",
    "            totals[\"failed\"] += 1\n",
    "            _set_postfix(pbar, {\"id\": \"-\", \"bucket\": \"-\", \"status\": \"failed\"})\n",
    "            tqdm.write(f\"[ERROR] {l} :: {type(e).__name__}: {str(e)[:200]}\")\n",
    "\n",
    "# Final one-line summary\n",
    "print(f\"\\nDone. Written: {totals['written']}, Skipped: {totals['skipped']}, Failed: {totals['failed']}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a03a470",
   "metadata": {},
   "source": [
    "# EXTERNAL LINK EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08a3df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Extract external links and download Redgifs as <post_id>.mp4 ===\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs): return x\n",
    "\n",
    "BASE_OUT = Path(OUT_DIR)\n",
    "EXTERNAL_DIR = BASE_OUT / \"external\"\n",
    "REDDITS_OK = {\"reddit.com\", \"www.reddit.com\", \"old.reddit.com\", \"np.reddit.com\", \"oauth.reddit.com\", \"redd.it\"}\n",
    "REDDIT_NATIVE_MEDIA = {\"i.redd.it\", \"v.redd.it\"}\n",
    "\n",
    "# ---- 1) Helpers to read archives and extract the outbound link ----\n",
    "def _domain(url: str) -> str:\n",
    "    try:\n",
    "        return urlparse(url).netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_external_url(archive_obj: dict) -> str | None:\n",
    "    \"\"\"\n",
    "    From your saved archive object:\n",
    "      { \"raw_post\": {...}, \"raw_comments\": {...}, \"comments\": [...] }\n",
    "    Pull the outbound link for external posts.\n",
    "    \"\"\"\n",
    "    post = archive_obj.get(\"raw_post\") or {}\n",
    "    # Prefer the 'url_overridden_by_dest' field; fallback to 'url'\n",
    "    url = post.get(\"url_overridden_by_dest\") or post.get(\"url\")\n",
    "    if not url:\n",
    "        return None\n",
    "\n",
    "    d = _domain(url)\n",
    "    # Treat non-Reddit, non-native-media as external\n",
    "    if d and d not in REDDITS_OK and d not in REDDIT_NATIVE_MEDIA:\n",
    "        return url\n",
    "    return None\n",
    "\n",
    "# ---- 2) Redgifs normalization & API download ----\n",
    "# Accept common Redgifs URL shapes:\n",
    "RE_REDGIFS_ID = re.compile(\n",
    "    r\"\"\"(?ix)\n",
    "    (?:^|/)(?:watch|ifr)/([a-z0-9]+)     # redgifs.com/watch/<id> or /ifr/<id>\n",
    "    |                                   # OR\n",
    "    (?:^|/)(?:i)/([a-z0-9]+)            # i.redgifs.com/i/<id>\n",
    "    \"\"\".strip()\n",
    ")\n",
    "\n",
    "def redgifs_id_from_url(url: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extract the media ID from redgifs-style URLs:\n",
    "      - https://redgifs.com/watch/<id>\n",
    "      - https://www.redgifs.com/watch/<id>\n",
    "      - https://v3.redgifs.com/watch/<id>\n",
    "      - https://redgifs.com/ifr/<id>\n",
    "      - https://i.redgifs.com/i/<id>\n",
    "    \"\"\"\n",
    "    m = RE_REDGIFS_ID.search(url)\n",
    "    if not m:\n",
    "        return None\n",
    "    # One of the two groups will be set\n",
    "    gid = m.group(1) or m.group(2)\n",
    "    return gid.lower() if gid else None\n",
    "\n",
    "# Redgifs API: get a temporary token, then resolve mp4 URLs\n",
    "REDGIFS_AUTH_URL = \"https://api.redgifs.com/v2/auth/temporary\"\n",
    "REDGIFS_GIF_URL  = \"https://api.redgifs.com/v2/gifs/{id}\"\n",
    "\n",
    "_SESSION = requests.Session()\n",
    "_RG_TOKEN = None\n",
    "_RG_TOKEN_TS = 0\n",
    "\n",
    "def redgifs_token(force: bool = False) -> str:\n",
    "    global _RG_TOKEN, _RG_TOKEN_TS\n",
    "    now = time.time()\n",
    "    # Reuse token for ~20 minutes unless forced\n",
    "    if not force and _RG_TOKEN and (now - _RG_TOKEN_TS) < 1200:\n",
    "        return _RG_TOKEN\n",
    "    r = _SESSION.get(REDGIFS_AUTH_URL, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    _RG_TOKEN = r.json().get(\"token\")\n",
    "    _RG_TOKEN_TS = now\n",
    "    if not _RG_TOKEN:\n",
    "        raise RuntimeError(\"Failed to obtain Redgifs token.\")\n",
    "    return _RG_TOKEN\n",
    "\n",
    "def redgifs_mp4_url(gid: str) -> str:\n",
    "    tok = redgifs_token()\n",
    "    headers = {\"Authorization\": f\"Bearer {tok}\"}\n",
    "    r = _SESSION.get(REDGIFS_GIF_URL.format(id=gid), headers=headers, timeout=30)\n",
    "    # If token expired, refresh once\n",
    "    if r.status_code in (401, 403):\n",
    "        tok = redgifs_token(force=True)\n",
    "        headers = {\"Authorization\": f\"Bearer {tok}\"}\n",
    "        r = _SESSION.get(REDGIFS_GIF_URL.format(id=gid), headers=headers, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    info = r.json().get(\"gif\") or {}\n",
    "    # Prefer HD if present, else SD, else fallback to urls.origin\n",
    "    urls = info.get(\"urls\") or {}\n",
    "    return urls.get(\"hd\") or urls.get(\"sd\") or urls.get(\"origin\")\n",
    "\n",
    "def download_stream(url: str, dest: Path, *, max_retries: int = 4):\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with _SESSION.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                with open(dest, \"wb\") as f:\n",
    "                    for chunk in r.iter_content(chunk_size=1024 * 256):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            if attempt + 1 >= max_retries:\n",
    "                raise\n",
    "            time.sleep(min(2 ** attempt, 15))\n",
    "\n",
    "# ---- 3) Walk external posts, export external links CSV, download Redgifs ----\n",
    "external_json_files = sorted(EXTERNAL_DIR.glob(\"*.json\"))\n",
    "print(f\"Found {len(external_json_files)} external post JSONs in {EXTERNAL_DIR}\")\n",
    "\n",
    "external_rows = []\n",
    "redgifs_failed = []\n",
    "\n",
    "REDGIFS_OUT = BASE_OUT / \"redgifs\"\n",
    "REDGIFS_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for fp in tqdm(external_json_files, desc=\"Scanning external posts\", unit=\"post\"):\n",
    "    try:\n",
    "        data = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "        post = (data or {}).get(\"raw_post\") or {}\n",
    "        pid  = post.get(\"id\") or fp.stem  # fallback to filename if needed\n",
    "\n",
    "        ext_url = extract_external_url(data)\n",
    "        if not ext_url:\n",
    "            # Still record that this external-typed file has no resolvable URL\n",
    "            external_rows.append({\"id\": pid, \"link\": \"\", \"domain\": \"\"})\n",
    "            continue\n",
    "\n",
    "        dom = _domain(ext_url)\n",
    "        external_rows.append({\"id\": pid, \"link\": ext_url, \"domain\": dom})\n",
    "\n",
    "        # Redgifs download\n",
    "        if \"redgifs.com\" in dom or dom.endswith(\".redgifs.com\"):\n",
    "            gid = redgifs_id_from_url(ext_url)\n",
    "            if not gid:\n",
    "                # Sometimes the external URL is a redirect page; skip but log\n",
    "                redgifs_failed.append({\"id\": pid, \"link\": ext_url, \"reason\": \"no_id_from_url\"})\n",
    "                continue\n",
    "\n",
    "            out_path = REDGIFS_OUT / f\"{pid}.mp4\"\n",
    "            if out_path.exists():\n",
    "                # already downloaded\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                mp4_url = redgifs_mp4_url(gid)\n",
    "                if not mp4_url:\n",
    "                    redgifs_failed.append({\"id\": pid, \"link\": ext_url, \"reason\": \"no_mp4_url\"})\n",
    "                    continue\n",
    "                download_stream(mp4_url, out_path)\n",
    "                # Show success line\n",
    "                print(f\"[REDGIFS] id={pid} -> {out_path.name}\")\n",
    "            except Exception as e:\n",
    "                redgifs_failed.append({\"id\": pid, \"link\": ext_url, \"reason\": str(e)})\n",
    "    except Exception as e:\n",
    "        # If we cannot read this JSON at all, log as a redgifs failure only if it looked like redgifs\n",
    "        redgifs_failed.append({\"id\": fp.stem, \"link\": \"\", \"reason\": f\"read_error: {e}\"})\n",
    "\n",
    "# ---- 4) Write summary CSVs ----\n",
    "ext_csv = BASE_OUT / \"external_links.csv\"\n",
    "with ext_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"id\", \"link\", \"domain\"])\n",
    "    w.writeheader()\n",
    "    w.writerows(external_rows)\n",
    "\n",
    "if redgifs_failed:\n",
    "    fail_csv = BASE_OUT / \"redgifs_failed.csv\"\n",
    "    with fail_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"id\", \"link\", \"reason\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(redgifs_failed)\n",
    "    print(f\"\\nSaved Redgifs download failures to: {fail_csv.resolve()}\")\n",
    "\n",
    "print(f\"\\nSaved external links to: {ext_csv.resolve()}\")\n",
    "print(f\"Redgifs saved (if any) to: {REDGIFS_OUT.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938da43",
   "metadata": {},
   "source": [
    "# MEDIA DOWNLOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb94da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Download embedded Reddit-hosted media for posts in out/media/*.json ===\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import html\n",
    "import mimetypes\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs): return x\n",
    "\n",
    "BASE_OUT = Path(OUT_DIR)\n",
    "MEDIA_JSON_DIR = BASE_OUT / \"media\"\n",
    "MEDIA_OUT_DIR = BASE_OUT / \"media_files\"\n",
    "MEDIA_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\"User-Agent\": \"reddit-media-downloader/1.0\"})\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _clean_url(u: str | None) -> str | None:\n",
    "    if not u:\n",
    "        return None\n",
    "    # Reddit often returns HTML-escaped URLs inside JSON\n",
    "    return html.unescape(u)\n",
    "\n",
    "def _domain(u: str | None) -> str:\n",
    "    if not u:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return urlparse(u).netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _ext_from_url_or_type(url: str | None, content_type: str | None) -> str:\n",
    "    # Prefer extension from URL, else derive from content-type\n",
    "    if url:\n",
    "        path = urlparse(url).path\n",
    "        ext = os.path.splitext(path)[1].lower()\n",
    "        if ext in {\".jpg\", \".jpeg\", \".png\", \".gif\", \".mp4\", \".webm\"}:\n",
    "            return ext\n",
    "    if content_type:\n",
    "        ext = mimetypes.guess_extension(content_type.split(\";\")[0].strip())\n",
    "        if ext:\n",
    "            # normalize jpeg\n",
    "            return \".jpg\" if ext == \".jpe\" else ext\n",
    "    # sensible default fallback\n",
    "    return \".mp4\" if (url and \".mp4\" in url) else \".jpg\"\n",
    "\n",
    "def _stream_download(url: str, dest: Path, *, max_retries: int = 4, chunk=1024 * 256):\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with SESSION.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                ctype = r.headers.get(\"Content-Type\")\n",
    "                # if dest has no extension yet, refine using content-type\n",
    "                if dest.suffix == \"\" and ctype:\n",
    "                    dest = dest.with_suffix(_ext_from_url_or_type(url, ctype))\n",
    "                with open(dest, \"wb\") as f:\n",
    "                    for part in r.iter_content(chunk_size=chunk):\n",
    "                        if part:\n",
    "                            f.write(part)\n",
    "            return dest  # final path (may include refined suffix)\n",
    "        except Exception:\n",
    "            if attempt + 1 >= max_retries:\n",
    "                raise\n",
    "            time.sleep(min(2 ** attempt, 15))\n",
    "\n",
    "def _pick_best_preview(post: dict) -> str | None:\n",
    "    \"\"\"\n",
    "    For image/GIF-like posts where 'preview' exists.\n",
    "    Prefer MP4 variant (smaller, plays everywhere), else best image 'source'.\n",
    "    \"\"\"\n",
    "    prev = post.get(\"preview\") or {}\n",
    "    variants = prev.get(\"variants\") or {}\n",
    "    # mp4 variant for gifs, etc.\n",
    "    mp4v = variants.get(\"mp4\") or variants.get(\"reddit_video_preview\")\n",
    "    if mp4v and mp4v.get(\"source\", {}).get(\"url\"):\n",
    "        return _clean_url(mp4v[\"source\"][\"url\"])\n",
    "    # fallback to the image source\n",
    "    src = (prev.get(\"images\") or [{}])[0].get(\"source\", {})\n",
    "    if src.get(\"url\"):\n",
    "        return _clean_url(src[\"url\"])\n",
    "    return None\n",
    "\n",
    "def _pick_vreddit_urls(post: dict) -> tuple[str | None, str | None]:\n",
    "    \"\"\"\n",
    "    v.redd.it posts: return (preferred_mp4_url, fallback_mp4_url)\n",
    "    Try in order: 'hls_url' (m3u8) -> 'fallback_url' (progressive) -> preview mp4.\n",
    "    We only directly download MP4 (no ffmpeg merge here), so we prefer fallback_url,\n",
    "    and otherwise try preview mp4.\n",
    "    \"\"\"\n",
    "    media = post.get(\"media\") or {}\n",
    "    rv = media.get(\"reddit_video\") or {}\n",
    "    fallback = rv.get(\"fallback_url\")  # often progressive mp4 (may be muted on long vids)\n",
    "    hls = rv.get(\"hls_url\")            # m3u8 playlist (would require ffmpeg)\n",
    "    # If no fallback, sometimes preview.mp4 exists:\n",
    "    prev_mp4 = None\n",
    "    prev = post.get(\"preview\") or {}\n",
    "    pv = prev.get(\"reddit_video_preview\") or {}\n",
    "    if isinstance(pv, dict) and pv.get(\"fallback_url\"):\n",
    "        prev_mp4 = pv[\"fallback_url\"]\n",
    "    return (_clean_url(fallback), _clean_url(prev_mp4 or hls))\n",
    "\n",
    "def _gallery_items(post: dict) -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    For gallery posts: return list of (url, suggested_ext).\n",
    "    Uses media_metadata to select best 's' rendition.\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    meta = post.get(\"media_metadata\") or {}\n",
    "    gdata = post.get(\"gallery_data\") or {}\n",
    "    order = [e.get(\"media_id\") for e in gdata.get(\"items\", []) if e.get(\"media_id\")]\n",
    "    for mid in order:\n",
    "        m = meta.get(mid) or {}\n",
    "        s = m.get(\"s\") or {}\n",
    "        url = _clean_url(s.get(\"mp4\") or s.get(\"gif\") or s.get(\"u\") or s.get(\"url\"))\n",
    "        if not url:\n",
    "            continue\n",
    "        # guess extension: mp4 preferred over gif over image\n",
    "        if \"mp4\" in s:\n",
    "            ext = \".mp4\"\n",
    "        elif \"gif\" in s:\n",
    "            ext = \".mp4\"  # we'll still download the gif URL, but use .mp4 if it's actually mp4\n",
    "        else:\n",
    "            # look at mime if present\n",
    "            m_type = m.get(\"m\")\n",
    "            ext = _ext_from_url_or_type(url, m_type)\n",
    "        items.append((url, ext))\n",
    "    return items\n",
    "\n",
    "# ---------- main walk ----------\n",
    "media_jsons = sorted(MEDIA_JSON_DIR.glob(\"*.json\"))\n",
    "print(f\"Found {len(media_jsons)} media post JSONs in {MEDIA_JSON_DIR}\")\n",
    "\n",
    "fail_rows = []\n",
    "downloaded = 0\n",
    "\n",
    "for fp in tqdm(media_jsons, desc=\"Downloading embedded media\", unit=\"post\"):\n",
    "    try:\n",
    "        data = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "        post = (data or {}).get(\"raw_post\") or {}\n",
    "        pid = post.get(\"id\") or fp.stem\n",
    "\n",
    "        # Prefer Reddit-hosted URL if present\n",
    "        url = _clean_url(post.get(\"url_overridden_by_dest\") or post.get(\"url\"))\n",
    "        dom = _domain(url)\n",
    "\n",
    "        # Case A: gallery\n",
    "        if post.get(\"is_gallery\") or (post.get(\"gallery_data\") and post.get(\"media_metadata\")):\n",
    "            items = _gallery_items(post)\n",
    "            if not items:\n",
    "                fail_rows.append({\"id\": pid, \"reason\": \"gallery_no_items\"})\n",
    "                continue\n",
    "            for idx, (item_url, ext) in enumerate(items, start=1):\n",
    "                outfile = MEDIA_OUT_DIR / f\"{pid}_g{idx:02d}{ext if ext.startswith('.') else ('.' + ext)}\"\n",
    "                if outfile.exists():\n",
    "                    continue\n",
    "                try:\n",
    "                    _stream_download(item_url, outfile)\n",
    "                    downloaded += 1\n",
    "                    print(f\"[GAL] {pid} -> {outfile.name}\")\n",
    "                except Exception as e:\n",
    "                    fail_rows.append({\"id\": pid, \"reason\": f\"gallery_item_fail:{e}\"})\n",
    "\n",
    "            continue  # next post\n",
    "\n",
    "        # Case B: native video (v.redd.it)\n",
    "        if (post.get(\"is_video\") or (post.get(\"media\") or {}).get(\"reddit_video\")) and dom.endswith(\"v.redd.it\"):\n",
    "            main_mp4, alt_mp4 = _pick_vreddit_urls(post)\n",
    "            target = MEDIA_OUT_DIR / f\"{pid}.mp4\"\n",
    "            if target.exists():\n",
    "                continue\n",
    "            src = main_mp4 or alt_mp4\n",
    "            if not src:\n",
    "                # last chance: look into preview variants\n",
    "                src = _pick_best_preview(post)\n",
    "            if not src:\n",
    "                fail_rows.append({\"id\": pid, \"reason\": \"vreddit_no_source\"})\n",
    "                continue\n",
    "            try:\n",
    "                _stream_download(src, target)\n",
    "                downloaded += 1\n",
    "                print(f\"[VID] {pid} -> {target.name}\")\n",
    "            except Exception as e:\n",
    "                fail_rows.append({\"id\": pid, \"reason\": f\"vreddit_dl_fail:{e}\"})\n",
    "            continue\n",
    "\n",
    "        # Case C: image / gif via i.redd.it or preview\n",
    "        if dom.endswith(\"i.redd.it\"):\n",
    "            # Direct i.redd.it link\n",
    "            ext = _ext_from_url_or_type(url, None)\n",
    "            target = MEDIA_OUT_DIR / f\"{pid}{ext}\"\n",
    "            if not target.exists():\n",
    "                try:\n",
    "                    _stream_download(url, target)\n",
    "                    downloaded += 1\n",
    "                    print(f\"[IMG] {pid} -> {target.name}\")\n",
    "                except Exception as e:\n",
    "                    fail_rows.append({\"id\": pid, \"reason\": f\"ireddit_dl_fail:{e}\"})\n",
    "            continue\n",
    "\n",
    "        # Fallback: try preview (covers some GIF-to-MP4 conversions)\n",
    "        prev_url = _pick_best_preview(post)\n",
    "        if prev_url and _domain(prev_url) in {\"i.redd.it\", \"v.redd.it\", \"preview.redd.it\"}:\n",
    "            ext = _ext_from_url_or_type(prev_url, None)\n",
    "            target = MEDIA_OUT_DIR / f\"{pid}{ext}\"\n",
    "            if not target.exists():\n",
    "                try:\n",
    "                    _stream_download(prev_url, target)\n",
    "                    downloaded += 1\n",
    "                    print(f\"[PREV] {pid} -> {target.name}\")\n",
    "                except Exception as e:\n",
    "                    fail_rows.append({\"id\": pid, \"reason\": f\"preview_dl_fail:{e}\"})\n",
    "            continue\n",
    "\n",
    "        # If we reach here, it looks like a Reddit-hosted \"media\" without a reliable direct URL\n",
    "        fail_rows.append({\"id\": pid, \"reason\": \"no_reddit_media_url\"})\n",
    "    except Exception as e:\n",
    "        fail_rows.append({\"id\": fp.stem, \"reason\": f\"read_error:{e}\"})\n",
    "\n",
    "# ---------- write failures ----------\n",
    "if fail_rows:\n",
    "    fail_csv = BASE_OUT / \"media_failed.csv\"\n",
    "    with fail_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"id\", \"reason\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(fail_rows)\n",
    "    print(f\"\\nSaved media failures to: {fail_csv.resolve()}\")\n",
    "\n",
    "print(f\"\\nDone. Downloaded: {downloaded}. Files saved under: {MEDIA_OUT_DIR.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Primary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
