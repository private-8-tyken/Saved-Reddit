{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4869123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reddit script app credentials (temporary hardcode ok for local use) ---\n",
    "REDDIT_CLIENT_ID = \"Ik1IhrLMkUe2Y7_jLqj-Ew\"\n",
    "REDDIT_CLIENT_SECRET = \"1j81ffxuNl-e8EzPV4D3OzCVCH-1lw\"\n",
    "REDDIT_USERNAME = \"Grand_Admiral_Tyken\"\n",
    "REDDIT_PASSWORD = \"X5bugNC9j3Bc^Uf\"\n",
    "\n",
    "\n",
    "# --- IO config ---\n",
    "CSV_PATH = \"links.csv\"   # one Reddit URL per line, no header\n",
    "OUT_DIR  = \"out\"         # base folder for outputs\n",
    "\n",
    "# --- polite request pacing ---\n",
    "REQUEST_DELAY_SEC = 0.5  # delay between requests to Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deda5f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "\n",
    "# Session + UA\n",
    "SESSION = requests.Session()\n",
    "UA = f\"reddit-json-downloader/1.0 (by u/{REDDIT_USERNAME})\"\n",
    "\n",
    "# OAuth endpoints and params\n",
    "OAUTH_TOKEN_URL = \"https://www.reddit.com/api/v1/access_token\"\n",
    "OAUTH_API_BASE  = \"https://oauth.reddit.com\"\n",
    "COMMENTS_QUERY  = \"raw_json=1&limit=500&depth=10&showmore=true\"  # fuller comment payload\n",
    "\n",
    "# URL helpers\n",
    "COMMENTS_ID_RE = re.compile(r\"/comments/([a-z0-9]{5,8})\", re.IGNORECASE)\n",
    "SHORTLINK_RE   = re.compile(r\"redd\\.it/([a-z0-9]{5,8})\", re.IGNORECASE)\n",
    "SUB_RE         = re.compile(r\"/r/([^/]+)/comments/\", re.IGNORECASE)\n",
    "\n",
    "def request_with_backoff(method: str, url: str, *, headers=None, data=None, timeout=60, max_retries=5):\n",
    "    for attempt in range(max_retries):\n",
    "        r = SESSION.request(method, url, headers=headers, data=data, timeout=timeout)\n",
    "        if r.status_code < 400:\n",
    "            return r\n",
    "        if r.status_code in (429,) or 500 <= r.status_code < 600:\n",
    "            time.sleep(min(2 ** attempt, 30))\n",
    "            continue\n",
    "        return r\n",
    "    return r  # last response\n",
    "\n",
    "def get_token() -> str:\n",
    "    auth = requests.auth.HTTPBasicAuth(REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET)\n",
    "    data = {\"grant_type\": \"password\", \"username\": REDDIT_USERNAME, \"password\": REDDIT_PASSWORD}\n",
    "    headers = {\"User-Agent\": UA}\n",
    "    r = requests.post(OAUTH_TOKEN_URL, auth=auth, data=data, headers=headers, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    tok = r.json().get(\"access_token\")\n",
    "    if not tok:\n",
    "        raise RuntimeError(f\"OAuth token missing; resp={r.text}\")\n",
    "    return tok\n",
    "\n",
    "def oauth_headers() -> dict:\n",
    "    tok = getattr(SESSION, \"_oauth_token\", None)\n",
    "    if not tok:\n",
    "        tok = get_token()\n",
    "        SESSION._oauth_token = tok\n",
    "    return {\"Authorization\": f\"bearer {tok}\", \"User-Agent\": UA}\n",
    "\n",
    "def accept_quarantine(subreddit: str) -> bool:\n",
    "    if not subreddit:\n",
    "        return False\n",
    "    url = f\"{OAUTH_API_BASE}/api/accept_quarantine\"\n",
    "    r = request_with_backoff(\"POST\", url, headers=oauth_headers(), max_retries=3)\n",
    "    return r.status_code in (200, 204, 409)  # 409 ~ already accepted\n",
    "\n",
    "def parse_link(link: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    m = COMMENTS_ID_RE.search(link)\n",
    "    post_id = m.group(1) if m else (SHORTLINK_RE.search(link).group(1) if SHORTLINK_RE.search(link) else None)\n",
    "    m_sr = SUB_RE.search(link)\n",
    "    subreddit = m_sr.group(1) if m_sr else None\n",
    "    return post_id, subreddit\n",
    "\n",
    "def normalize_comments_url(link: str, fallback_post_id: Optional[str]) -> str:\n",
    "    p = urlparse(link)\n",
    "    path = p.path or \"\"\n",
    "    host = (p.netloc or \"\").lower()\n",
    "    if \"redd.it\" in host or \"/comments/\" not in path:\n",
    "        if fallback_post_id:\n",
    "            path = f\"/comments/{fallback_post_id}/\"\n",
    "    if not path.endswith(\"/\"):\n",
    "        path += \"/\"\n",
    "    return f\"{OAUTH_API_BASE}{path}.json?{COMMENTS_QUERY}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8457222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_more_ids(listing_node) -> List[str]:\n",
    "    ids = []\n",
    "    def walk(node):\n",
    "        if not isinstance(node, dict): return\n",
    "        kind = node.get(\"kind\"); data = node.get(\"data\", {})\n",
    "        if kind == \"more\":\n",
    "            children = data.get(\"children\") or []\n",
    "            ids.extend([c for c in children if c])\n",
    "        elif kind in (\"Listing\", \"t1\"):\n",
    "            for ch in data.get(\"children\", []): walk(ch)\n",
    "            if kind == \"t1\" and isinstance(data.get(\"replies\"), dict): walk(data[\"replies\"])\n",
    "    walk(listing_node)\n",
    "    return ids\n",
    "\n",
    "def index_comments_by_id(listing_node) -> Dict[str, dict]:\n",
    "    idx = {}\n",
    "    def walk(node):\n",
    "        if not isinstance(node, dict): return\n",
    "        kind = node.get(\"kind\"); data = node.get(\"data\", {})\n",
    "        if kind == \"t1\":\n",
    "            cid = (data.get(\"id\") or \"\").lower()\n",
    "            if cid: idx[cid] = node\n",
    "            if isinstance(data.get(\"replies\"), dict): walk(data[\"replies\"])\n",
    "        elif kind == \"Listing\":\n",
    "            for ch in data.get(\"children\", []): walk(ch)\n",
    "    walk(listing_node)\n",
    "    return idx\n",
    "\n",
    "def replace_more_with_children(root_listing: dict, parent_lookup: Dict[str, dict], chunk_result: dict):\n",
    "    listing = chunk_result.get(\"json\", {}).get(\"data\", {}).get(\"things\", [])\n",
    "    for thing in listing:\n",
    "        if thing.get(\"kind\") != \"t1\": continue\n",
    "        data = thing.get(\"data\", {})\n",
    "        pid = data.get(\"parent_id\", \"\")\n",
    "        if pid.startswith(\"t1_\"):\n",
    "            parent_id = pid[3:].lower()\n",
    "            parent = parent_lookup.get(parent_id)\n",
    "            if parent:\n",
    "                if not isinstance(parent[\"data\"].get(\"replies\"), dict):\n",
    "                    parent[\"data\"][\"replies\"] = {\"kind\": \"Listing\", \"data\": {\"children\": []}}\n",
    "                parent[\"data\"][\"replies\"][\"data\"][\"children\"].append(thing)\n",
    "        elif pid.startswith(\"t3_\"):\n",
    "            root_listing[\"data\"][\"children\"].append(thing)\n",
    "\n",
    "def strip_more_nodes(node):\n",
    "    if not isinstance(node, dict): return\n",
    "    kind = node.get(\"kind\"); data = node.get(\"data\", {})\n",
    "    if kind == \"Listing\":\n",
    "        new_children = [ch for ch in data.get(\"children\", []) if ch.get(\"kind\") != \"more\"]\n",
    "        data[\"children\"] = new_children\n",
    "        for ch in new_children: strip_more_nodes(ch)\n",
    "    if kind == \"t1\" and isinstance(data.get(\"replies\"), dict):\n",
    "        strip_more_nodes(data[\"replies\"])\n",
    "\n",
    "def fetch_full_post_and_comments(link: str) -> Tuple[dict, dict]:\n",
    "    if not link.startswith((\"http://\", \"https://\")):\n",
    "        raise ValueError(f\"Not a URL: {link}\")\n",
    "\n",
    "    post_id, sr_hint = parse_link(link)\n",
    "    comments_url = normalize_comments_url(link, post_id)\n",
    "\n",
    "    r = request_with_backoff(\"GET\", comments_url, headers=oauth_headers(), timeout=60)\n",
    "    if r.status_code == 403:\n",
    "        sr = sr_hint or (re.search(r\"/r/([^/]+)/comments/\", comments_url).group(1) if re.search(r\"/r/([^/]+)/comments/\", comments_url) else \"\")\n",
    "        if sr and accept_quarantine(sr):\n",
    "            r = request_with_backoff(\"GET\", comments_url, headers=oauth_headers(), timeout=60)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    if not (isinstance(data, list) and len(data) >= 2):\n",
    "        raise RuntimeError(\"Unexpected Reddit JSON format\")\n",
    "\n",
    "    post_listing = data[0][\"data\"][\"children\"]\n",
    "    if not post_listing:\n",
    "        raise RuntimeError(\"Post listing empty\")\n",
    "    post = post_listing[0][\"data\"]\n",
    "    subreddit = post.get(\"subreddit\") or sr_hint or \"\"\n",
    "    link_id = post.get(\"id\")\n",
    "    comments_listing = data[1]\n",
    "\n",
    "    if link_id:\n",
    "        while True:\n",
    "            more_ids = collect_more_ids(comments_listing)\n",
    "            if not more_ids: break\n",
    "            for i in range(0, len(more_ids), 100):\n",
    "                chunk = more_ids[i:i+100]\n",
    "                form = {\n",
    "                    \"link_id\": f\"t3_{link_id}\",\n",
    "                    \"api_type\": \"json\",\n",
    "                    \"children\": \",\".join(chunk),\n",
    "                    \"sort\": \"confidence\",\n",
    "                    \"limit_children\": False,\n",
    "                    \"raw_json\": 1,\n",
    "                }\n",
    "                url = f\"{OAUTH_API_BASE}/api/morechildren\"\n",
    "                r2 = request_with_backoff(\"POST\", url, headers=oauth_headers(), data=form, timeout=60)\n",
    "                if r2.status_code == 403 and subreddit and accept_quarantine(subreddit):\n",
    "                    r2 = request_with_backoff(\"POST\", url, headers=oauth_headers(), data=form, timeout=60)\n",
    "                r2.raise_for_status()\n",
    "                payload = r2.json()\n",
    "                parent_idx = index_comments_by_id(comments_listing)\n",
    "                replace_more_with_children(comments_listing, parent_idx, payload)\n",
    "            strip_more_nodes(comments_listing)\n",
    "\n",
    "    return post, comments_listing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "381b7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification + canonical archive builder (matches your 1a1ybm-style top level)\n",
    "\n",
    "INTERNAL_REDDIT_HOSTS = {\n",
    "    \"reddit.com\", \"www.reddit.com\", \"old.reddit.com\", \"np.reddit.com\", \"oauth.reddit.com\",\n",
    "    \"redd.it\",\n",
    "}\n",
    "NATIVE_MEDIA_HOSTS = {\"i.redd.it\", \"v.redd.it\"}\n",
    "\n",
    "def domain_of(url: Optional[str]) -> str:\n",
    "    if not url: return \"\"\n",
    "    try:\n",
    "        return urlparse(url).netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def is_gallery(post: dict) -> bool:\n",
    "    return bool(post.get(\"is_gallery\") or (post.get(\"gallery_data\") and post.get(\"media_metadata\")))\n",
    "\n",
    "def summarize_media(post: dict) -> dict:\n",
    "    is_self = bool(post.get(\"is_self\"))\n",
    "    url = post.get(\"url_overridden_by_dest\") or post.get(\"url\")\n",
    "    dom = domain_of(url)\n",
    "    if is_self:\n",
    "        return {\"kind\": \"self\", \"files\": []}\n",
    "    if is_gallery(post):\n",
    "        return {\"kind\": \"gallery\", \"files\": []}\n",
    "    if dom in NATIVE_MEDIA_HOSTS or post.get(\"is_video\") or (post.get(\"media\") or {}).get(\"reddit_video\"):\n",
    "        return {\"kind\": \"video\", \"files\": []}\n",
    "    if dom and dom not in INTERNAL_REDDIT_HOSTS:\n",
    "        return {\"kind\": \"link\", \"files\": []}\n",
    "    if dom == \"i.redd.it\" or (post.get(\"preview\") and (post.get(\"post_hint\") or \"\").startswith(\"image\")):\n",
    "        return {\"kind\": \"image\", \"files\": []}\n",
    "    return {\"kind\": \"self\" if is_self else \"link\", \"files\": []}\n",
    "\n",
    "def external_link_or_none(post: dict) -> Optional[str]:\n",
    "    url = post.get(\"url_overridden_by_dest\") or post.get(\"url\")\n",
    "    dom = domain_of(url)\n",
    "    if url and dom and (dom not in INTERNAL_REDDIT_HOSTS) and (dom not in NATIVE_MEDIA_HOSTS):\n",
    "        return url\n",
    "    return None\n",
    "\n",
    "def classify_post(post_data: dict) -> str:\n",
    "    \"\"\"\n",
    "    Decide among: 'external', 'media', 'text'.\n",
    "    Priority:\n",
    "      1) external  -> off-Reddit (e.g., redgifs.com)\n",
    "      2) media     -> native Reddit media (i.redd.it, v.redd.it), previews, galleries\n",
    "      3) text      -> self-posts without media\n",
    "    \"\"\"\n",
    "    is_self = bool(post_data.get(\"is_self\"))\n",
    "    url = post_data.get(\"url_overridden_by_dest\") or post_data.get(\"url\")\n",
    "    d = domain_of(url) or (post_data.get(\"domain\") or \"\").lower()\n",
    "\n",
    "    if not is_self and d and (d not in INTERNAL_REDDIT_HOSTS) and (d not in NATIVE_MEDIA_HOSTS):\n",
    "        return \"external\"\n",
    "\n",
    "    post_hint   = (post_data.get(\"post_hint\") or \"\").lower()\n",
    "    has_gallery = bool(post_data.get(\"gallery_data\"))\n",
    "    has_preview = bool(post_data.get(\"preview\"))\n",
    "    has_media   = bool(post_data.get(\"media\")) or bool(post_data.get(\"is_video\"))\n",
    "    is_native_media_host = d in NATIVE_MEDIA_HOSTS\n",
    "    is_media_hint = post_hint in {\"image\", \"hosted:video\", \"rich:video\"}\n",
    "\n",
    "    if is_native_media_host or has_media or has_preview or has_gallery or is_media_hint:\n",
    "        return \"media\"\n",
    "    return \"text\"\n",
    "\n",
    "def canonical_archive(post: dict, comments_listing: dict) -> dict:\n",
    "    permalink = post.get(\"permalink\") or \"\"\n",
    "    if permalink and not permalink.startswith(\"http\"):\n",
    "        permalink = f\"https://www.reddit.com{permalink}\"\n",
    "    return {\n",
    "        \"archived_at\": time.strftime(\"%Y-%m-%dT%H:%M:%S.000000+00:00\", time.gmtime()),\n",
    "        \"reddit_fullname\": f\"t3_{post.get('id') or ''}\",\n",
    "        \"reddit_id\": post.get(\"id\"),\n",
    "        \"permalink\": permalink,\n",
    "        \"title\": post.get(\"title\"),\n",
    "        \"selftext\": post.get(\"selftext\"),\n",
    "        \"author\": post.get(\"author\"),\n",
    "        \"author_fullname\": post.get(\"author_fullname\"),\n",
    "        \"subreddit\": post.get(\"subreddit\"),\n",
    "        \"subreddit_id\": post.get(\"subreddit_id\"),\n",
    "        \"created_utc\": post.get(\"created_utc\"),\n",
    "        \"is_self\": post.get(\"is_self\"),\n",
    "        \"url\": post.get(\"url\"),\n",
    "        \"domain\": post.get(\"domain\"),\n",
    "        \"post_hint\": post.get(\"post_hint\"),\n",
    "        \"is_gallery\": is_gallery(post),\n",
    "        \"over_18\": post.get(\"over_18\"),\n",
    "        \"spoiler\": post.get(\"spoiler\"),\n",
    "        \"link_flair_text\": post.get(\"link_flair_text\"),\n",
    "        \"is_original_content\": post.get(\"is_original_content\"),\n",
    "        \"stickied\": post.get(\"stickied\"),\n",
    "        \"locked\": post.get(\"locked\"),\n",
    "        \"edited\": post.get(\"edited\"),\n",
    "        \"num_comments\": post.get(\"num_comments\"),\n",
    "        \"score\": post.get(\"score\"),\n",
    "        \"upvote_ratio\": post.get(\"upvote_ratio\"),\n",
    "        \"media\": summarize_media(post),\n",
    "        \"external_link\": external_link_or_none(post),\n",
    "        \"raw_post\": post,\n",
    "        \"raw_comments\": comments_listing,\n",
    "    }\n",
    "\n",
    "def save_archive(doc: dict, base_out: Path, post_id: str, category: str):\n",
    "    target = base_out / category\n",
    "    target.mkdir(parents=True, exist_ok=True)\n",
    "    (target / f\"{post_id}.json\").write_text(json.dumps(doc, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def already_archived(base_out: Path, pid: str) -> bool:\n",
    "    return any((base_out / sub / f\"{pid}.json\").exists() for sub in (\"media\", \"external\", \"text\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f89f87a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095a07782c0c427894cdd7db3eb42827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Archiving posts:   0%|          | 0/1118 [00:00<?, ?post/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] id=1m42f89 | folder=media | link=https://www.reddit.com/r/AsianRape/comments/1m42f89/call_me_mommy_and_ill_send_you_a_free_photo_for\n",
      "[OK] id=1ldy020 | folder=text | link=https://www.reddit.com/r/Packsmundiales69/comments/1ldy020/pack_completo_de_aziliahadid_informaci%C3%B3n_al_inbox\n",
      "[OK] id=1m08tx8 | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1m08tx8/oh_no_my_big_brother_downloaded_a_mind_control\n",
      "[OK] id=1lz6ey7 | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1lz6ey7/he_fucks_her_til_shes_filled_up\n",
      "[OK] id=1ly74n1 | folder=media | link=https://www.reddit.com/r/HentaiBullying/comments/1ly74n1/my_bully_caught_me_exchanging_numbers_with_my\n",
      "[OK] id=1lxwhtd | folder=media | link=https://www.reddit.com/r/biosuits/comments/1lxwhtd/guardian_tech_is_easily_corruptible_by_mrbones2210\n",
      "[OK] id=1lxnnav | folder=media | link=https://www.reddit.com/r/AhegaoGirls/comments/1lxnnav/do_u_think_ur_entire_dick_will_fit_inside\n",
      "[OK] id=1lxx8ri | folder=media | link=https://www.reddit.com/r/AhegaoGirls/comments/1lxx8ri/1_or_2\n",
      "[OK] id=1lwsvgp | folder=media | link=https://www.reddit.com/r/AhegaoGirls/comments/1lwsvgp/my_first_ahegao_hope_i_did_it_correct\n",
      "[OK] id=1lwe682 | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1lwe682/urrgh_shes_such_a_bitch_after_just_outplaying_me\n",
      "[OK] id=1lugdml | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1lugdml/what_does_it_take_for_a_femcel_to_be_treated_like\n",
      "[OK] id=1lt25m2 | folder=media | link=https://www.reddit.com/r/cumsluts/comments/1lt25m2/one_of_your_friends_send_u_thiss\n",
      "[OK] id=1lr6j9f | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1lr6j9f/collage_student_kidnapped_and_abused_for_days\n",
      "[OK] id=160r3gi | folder=media | link=https://www.reddit.com/r/ApenasFansGarota/comments/160r3gi/asian_girl\n",
      "[OK] id=10k4n9e | folder=media | link=https://www.reddit.com/r/PornhubAds/comments/10k4n9e/can_anyone_find_a_name_for_this_chick\n",
      "[OK] id=1k6u0ka | folder=media | link=https://www.reddit.com/r/GOONED/comments/1k6u0ka/asian_slut_battle_are_you_team_abg_110_or_team\n",
      "[OK] id=1jcgiwn | folder=media | link=https://www.reddit.com/r/AmateurBritish/comments/1jcgiwn/teachers_love_sundays\n",
      "[OK] id=1lmf8kk | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1lmf8kk/you_shouldnt_be_shy_when_talking_to_your_friends\n",
      "[OK] id=1lmjyte | folder=media | link=https://www.reddit.com/r/MyAiGF/comments/1lmjyte/esdeath\n",
      "[OK] id=1lh32fn | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1lh32fn/hopefully_me_someday\n",
      "[OK] id=1lgignw | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1lgignw/fucked_around_and_found_out\n",
      "[OK] id=1lg6dp6 | folder=media | link=https://www.reddit.com/r/biosuits/comments/1lg6dp6/vorepunk_space_peril_by_win4699\n",
      "[OK] id=1ldaty7 | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1ldaty7/captured_broken_shipped_off_then_forced_to_marry\n",
      "[OK] id=1lcojlr | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1lcojlr/what_better_way_to_break_a_nation_than_to_break\n",
      "[OK] id=1lcfc5h | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1lcfc5h/they_couldnt_find_anything_of_value_in_my\n",
      "[OK] id=1lcef80 | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1lcef80/never_let_a_hole_go_to_waste\n",
      "[OK] id=1l66g4m | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1l66g4m/this_is_what_real_girls_do_to_their_friends_i\n",
      "[OK] id=1kzgn1l | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1kzgn1l/looks_like_hypnosis_beats_telekinesis\n",
      "[OK] id=1kt08co | folder=media | link=https://www.reddit.com/r/traumatizedsluts2/comments/1kt08co/to_all_the_people_in_high_school_who_said_i_was_a\n",
      "[OK] id=1ksnqa5 | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1ksnqa5/when_you_actually_start_enjoying_getting_raped_by\n",
      "[OK] id=1kpo53z | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1kpo53z/wwhat_the_fuck_who_picks_up_a_delivery_like_this\n",
      "[OK] id=1kk0aqj | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1kk0aqj/want_this_so_bad_anyone_welcome\n",
      "[OK] id=1kjc9hw | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1kjc9hw/a_drunken_no_means_yes\n",
      "[OK] id=1kio85o | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1kio85o/just_testing_what_youve_learned_in_your_self\n",
      "[OK] id=1ki6woy | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1ki6woy/cocksleeves_dont_need_to_be_conscious_to_be_used\n",
      "[OK] id=1ki01fv | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1ki01fv/you_thought_you_had_a_real_friend_but_she_made\n",
      "[OK] id=1d56g8y | folder=media | link=https://www.reddit.com/r/short_porn/comments/1d56g8y/moaning_girl\n",
      "[OK] id=1kf3rrg | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1kf3rrg/i_want_to_be_tricked_into_getting_spitroasted\n",
      "[OK] id=1keh4bs | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1keh4bs/fuck_why_did_i_decide_to_wear_this_again_id\n",
      "[OK] id=1kbuihs | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1kbuihs/why_do_our_girls_nights_always_end_up_like_this\n",
      "[OK] id=1kaemne | folder=external | link=https://www.reddit.com/r/u_jokeaboutdaddyissues/comments/1kaemne/before_i_called_a_man_over_to_fuck_me_in_the\n",
      "[OK] id=1fx27p5 | folder=text | link=https://www.reddit.com/r/u_FishermanAncient4937/comments/1fx27p5/katyaaeliza\n",
      "[OK] id=1k00qve | folder=media | link=https://www.reddit.com/r/ffmFantasy/comments/1k00qve/she_was_jealous_he_got_me_pregnant_so_she_asked\n",
      "[OK] id=1js9z13 | folder=media | link=https://www.reddit.com/r/u_ellesaurus23/comments/1js9z13/posting_some_clips_soon_i_just_wanna_show_u_how\n",
      "[OK] id=1jsy8ir | folder=media | link=https://www.reddit.com/r/cumsluts/comments/1jsy8ir/just_made_a_porno_in_a_photo_studio_he_came_in_my\n",
      "[OK] id=1jpdijf | folder=media | link=https://www.reddit.com/r/MyAiGF/comments/1jpdijf/does_anybody_know_a_good_nsfw_app_that_i_can_use\n",
      "[OK] id=1jhu385 | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1jhu385/i_want_to_be_a_goth_cheerleader_that_obeys_anyone\n",
      "[OK] id=18pig6m | folder=media | link=https://www.reddit.com/r/MasterSnips/comments/18pig6m/free_use_girlfriend\n",
      "[OK] id=zsei5v | folder=media | link=https://www.reddit.com/r/nsfw/comments/zsei5v/mile_high_club\n",
      "[OK] id=1jgutui | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1jgutui/wyd_in_the_situation_freeze_fight_or_flee\n",
      "[OK] id=1jgxtl1 | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1jgxtl1/i_want_this_toy_and_then_someone_to_controll_it\n",
      "[OK] id=1jh8gy7 | folder=media | link=https://www.reddit.com/r/cumsluts/comments/1jh8gy7/he_did_hit_my_face_and_all_over_me\n",
      "[OK] id=1jgg9sw | folder=media | link=https://www.reddit.com/r/cumsluts/comments/1jgg9sw/my_college_roommates_turned_me_in_to_a_cumslut\n",
      "[OK] id=1jghu70 | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1jghu70/i_found_a_video_on_my_gfs_phone_i_wonder_what\n",
      "[OK] id=11s36dp | folder=media | link=https://www.reddit.com/r/doubledildos/comments/11s36dp/sharing_is_caring\n",
      "[OK] id=1jadh1g | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1jadh1g/oh_man_you_know_most_little_brothers_dont_allow\n",
      "[OK] id=1ja2k1i | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1ja2k1i/legalize_it\n",
      "[OK] id=1ja7dvz | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1ja7dvz/i_want_to_go_on_a_double_together_with_my_friend\n",
      "[OK] id=1j9jc48 | folder=media | link=https://www.reddit.com/r/ffmFantasy/comments/1j9jc48/woooops\n",
      "[OK] id=1j5161y | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1j5161y/i_always_get_passed_around_during_family\n",
      "[OK] id=1j3lx28 | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1j3lx28/i_want_to_replace_my_big_bros_fleshlight_3\n",
      "[OK] id=1j3e1d8 | folder=media | link=https://www.reddit.com/r/cumsluts/comments/1j3e1d8/beautiful\n",
      "[OK] id=1j33exl | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1j33exl/having_a_knot_in_my_throat_should_just_be_my\n",
      "[OK] id=1g4e5kj | folder=media | link=https://www.reddit.com/r/3Porn_Edits/comments/1g4e5kj/hannahowo_edit\n",
      "[OK] id=1iwewts | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1iwewts/i_found_her_masturbating_at_work_and_naked_to\n",
      "[OK] id=1isrcr7 | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1isrcr7/rape_school_is_so_cool_the_best_rapetoys_are_there\n",
      "[OK] id=1ir2gwr | folder=media | link=https://www.reddit.com/r/HentaiBullying/comments/1ir2gwr/too_bad_the_guy_you_were_sexting_with_is_one_of\n",
      "[OK] id=1isngsf | folder=media | link=https://www.reddit.com/r/cumsluts/comments/1isngsf/bet_you_dont_zoom_in\n",
      "[OK] id=1ii2htx | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1ii2htx/10_minutes_into_the_sleepover_vs_2_hours_into_the\n",
      "[OK] id=1ih35z4 | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1ih35z4/i_wish_i_could_actually_be_hypnotized\n",
      "[OK] id=1i7wn0k | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1i7wn0k/how_sleepovers_with_my_friends_usually_end_up\n",
      "[OK] id=1i80e4i | folder=external | link=https://www.reddit.com/r/bdsm/comments/1i80e4i/katie_kush_brutalized\n",
      "[OK] id=1i3zxl2 | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1i3zxl2/why_is_cum_so_fucking_hot_to_me\n",
      "[OK] id=1hznyem | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1hznyem/its_over_nobody_listens_to_techno\n",
      "[OK] id=1hzq7b2 | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1hzq7b2/your_slutty_little_sister_discovers_your_huge\n",
      "[OK] id=1hkujr2 | folder=media | link=https://www.reddit.com/r/AsiansBigTits/comments/1hkujr2/do_you_like_this_on_me\n",
      "[OK] id=1huybg8 | folder=external | link=https://www.reddit.com/r/rape_hentai/comments/1huybg8/i_dont_know_what_to_do_but_i_need_the_money_from\n",
      "[OK] id=1huefw6 | folder=media | link=https://www.reddit.com/r/bdsm/comments/1huefw6/the_cause_and_the_result_in_one_photo\n",
      "[OK] id=1hu811g | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1hu811g/dont_forget_to_mark_your_property_ill_make_sure\n",
      "[OK] id=1hobf76 | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1hobf76/finding_out_my_boyfriend_fucked_all_my_friends\n",
      "[OK] id=1hnj2xs | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1hnj2xs/i_want_a_thick_white_cock_to_snap_my_choker_every\n",
      "[OK] id=1hoef9x | folder=external | link=https://www.reddit.com/r/bdsm/comments/1hoef9x/working_ana_foxx_with_sound_in_comments\n",
      "[OK] id=1hnyiyv | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1hnyiyv/my_throat_is_just_a_cocksleeve_for_men_to_use\n",
      "[OK] id=1hnqazl | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1hnqazl/i_want_to_get_drugged_raped_and_then_blackmailed\n",
      "[OK] id=1hm6g5p | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1hm6g5p/for_christmas_you_got_a_magic_fleshlight_that\n",
      "[OK] id=1hmjhgq | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1hmjhgq/idol_rapes_new_girl\n",
      "[OK] id=1hl8sdr | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1hl8sdr/raped_by_ghosts\n",
      "[OK] id=1hifhbm | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1hifhbm/i_am_just_a_girl_from_atlantis_i_told_himhe_didnt\n",
      "[OK] id=1hhdkrl | folder=media | link=https://www.reddit.com/r/rapefantasies/comments/1hhdkrl/im_only_411_and_87lbs_but_do_u_think_my_body_is\n",
      "[OK] id=1hd7w4n | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1hd7w4n/now_she_become_best_sextoy\n",
      "[OK] id=1h4jxwh | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1h4jxwh/who_misses_daddy\n",
      "[OK] id=1h42q0i | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1h42q0i/i_want_my_useless_pussy_taped_up\n",
      "[OK] id=1gnbatr | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1gnbatr/i_want_to_make_my_girlfriend_watch_as_he_takes_me\n",
      "[OK] id=1glg76v | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1glg76v/i_hold_all_the_power_of_your_life_in_my_hands\n",
      "[OK] id=1gjdxsb | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1gjdxsb/my_boyfriend_is_an_idiot_he_deserves_this_for\n",
      "[OK] id=1ghhij9 | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1ghhij9/im_sure_i_can_help_jjust_let_me_go_to_the\n",
      "[OK] id=1g6tlmc | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1g6tlmc/that_wasnt_very_were_just_friends_of_us\n",
      "[OK] id=1g6hyvr | folder=media | link=https://www.reddit.com/r/bdsm/comments/1g6hyvr/too_sensitive_for_torture\n",
      "[OK] id=1g5yxxk | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1g5yxxk/i_wish_i_could_go_to_a_limitless_rave_wearing_my\n",
      "[OK] id=1g59euz | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1g59euz/i_need_this_relationship\n",
      "[OK] id=1g5b0d7 | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1g5b0d7/hey_guys_we_brought_some_booze_why_are_there_only\n",
      "[OK] id=1fkiv4x | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1fkiv4x/im_fertile_and_ovulating_so_impregnate_me_already\n",
      "[OK] id=1fanaaw | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/1fanaaw/removed_by_reddit\n",
      "[OK] id=1euudyy | folder=media | link=https://www.reddit.com/r/u_uselessrandom272/comments/1euudyy/two_words_that_make_me_instantly_wet\n",
      "[OK] id=1eshkdy | folder=media | link=https://www.reddit.com/r/slutsofsnapchat/comments/1eshkdy/describe_my_ass_with_two_words_and_i_will_decide\n",
      "[OK] id=mhgtnm | folder=external | link=https://www.reddit.com/r/NewYorkNine/comments/mhgtnm/elena_by_mike_dowson\n",
      "[OK] id=1ercf6r | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1ercf6r/its_daddys_birthday_so_stop_fucking_crying_and\n",
      "[OK] id=1emlxk2 | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1emlxk2/i_failed_my_cheerleading_drill_at_the_football\n",
      "[OK] id=1em3hs9 | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1em3hs9/my_fantasies_revolve_around_being_raped\n",
      "[OK] id=1ejoh7p | folder=media | link=https://www.reddit.com/r/rapefantasies/comments/1ejoh7p/i_love_being_a_rape_meat_whore\n",
      "[OK] id=1e8877o | folder=media | link=https://www.reddit.com/r/u_yourfriendclover/comments/1e8877o/i_know_you_cant_resist_painting_my_face_when_i\n",
      "[OK] id=1e5qcjb | folder=media | link=https://www.reddit.com/r/bdsm/comments/1e5qcjb/home_invasion_package_pt2\n",
      "[OK] id=1e1196o | folder=media | link=https://www.reddit.com/r/HentaiBullying/comments/1e1196o/my_bullies_tricked_me_into_signing_up_for_public\n",
      "[OK] id=1dyacud | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1dyacud/they_raped_me_and_my_bestie_and_told_us_that_it\n",
      "[OK] id=1dyqycv | folder=media | link=https://www.reddit.com/r/cumsluts/comments/1dyqycv/i_think_we_look_cute_together\n",
      "[OK] id=1dyag49 | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1dyag49/getting_raped_in_a_movie_theater_with_people\n",
      "[OK] id=1dynt4p | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1dynt4p/cutie_making_him_cum\n",
      "[OK] id=1dy6def | folder=media | link=https://www.reddit.com/r/HentaiBullying/comments/1dy6def/a_dom_and_a_sub_switch_roles_pt3_oc\n",
      "[OK] id=1ctgxid | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1ctgxid/bought_the_newest_sexbot_it_begs_for_mercy_so\n",
      "[OK] id=1bc7j0y | folder=media | link=https://www.reddit.com/r/bangmybully/comments/1bc7j0y/delete_them_part_2\n",
      "[OK] id=1bwdawk | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1bwdawk/after_i_broke_up_with_my_boyfriend_he_was_furious\n",
      "[OK] id=1b88bto | folder=media | link=https://www.reddit.com/r/rapefantasies/comments/1b88bto/would_i_get_a_rapetarget_if_i_would_be_alone_in_a\n",
      "[OK] id=1b7uaol | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1b7uaol/shouldnt_have_lost_that_bet\n",
      "[OK] id=1b78d1k | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1b78d1k/she_said_she_doesnt_fuck_on_the_first_date_i_told\n",
      "[OK] id=1b6o9xn | folder=media | link=https://www.reddit.com/r/HentaiBullying/comments/1b6o9xn/your_face_would_be_of_no_use_except_maintain_used\n",
      "[OK] id=1atpird | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1atpird/i_wanted_to_be_famous_but_not_like_this\n",
      "[OK] id=1aiphm7 | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1aiphm7/it_might_sound_like_a_contradiction_but_there_is\n",
      "[OK] id=19bdlu0 | folder=external | link=https://www.reddit.com/r/cumsluts/comments/19bdlu0/the_edge_of_tomorrow\n",
      "[OK] id=198fvf8 | folder=external | link=https://www.reddit.com/r/cumsluts/comments/198fvf8/big_surprise\n",
      "[OK] id=1972sh2 | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/1972sh2/dont_lie_this_is_your_biggest_dream_to_be_fucked\n",
      "[OK] id=196elqg | folder=external | link=https://www.reddit.com/r/cumsluts/comments/196elqg/goth_girls_love_cum\n",
      "[OK] id=195ym1t | folder=external | link=https://www.reddit.com/r/cumsluts/comments/195ym1t/cutie_gets_a_facial\n",
      "[OK] id=195ldc9 | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/195ldc9/my_boyfriend_doesnt_know_about_the_side_hustle\n",
      "[OK] id=195miyk | folder=external | link=https://www.reddit.com/r/cumsluts/comments/195miyk/right_in_the_face\n",
      "[OK] id=1900kwe | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1900kwe/lovely\n",
      "[OK] id=18wbr22 | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/18wbr22/daddy_said_he_found_me_a_job_wkrking_for_his\n",
      "[OK] id=18w8edt | folder=media | link=https://www.reddit.com/r/bdsm/comments/18w8edt/orgasm_tracker_2023\n",
      "[OK] id=186fhsz | folder=external | link=https://www.reddit.com/r/u_jokeaboutdaddyissues/comments/186fhsz/audio_post_therapy_im_my_daddys_fuck_puppet\n",
      "[OK] id=181kob2 | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/181kob2/the_amount_of_time_may_vary_but_eventually_all\n",
      "[OK] id=17zhkt4 | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/17zhkt4/my_friends_tied_me_here_and_left_me_without_my\n",
      "[OK] id=17t9b35 | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/17t9b35/im_an_android_you_cant_hack_me_lol_curlypleasure80\n",
      "[OK] id=17mub7t | folder=external | link=https://www.reddit.com/r/IShouldBuyABoat/comments/17mub7t/urgent_i_need_a_boat_reason\n",
      "[OK] id=17sm7t6 | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/17sm7t6/i_guess_i_was_in_the_wrong_place_at_the_wrong_time\n",
      "[OK] id=17pmbv8 | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/17pmbv8/how_did_you_get_me_in_this_situation\n",
      "[OK] id=17dtepi | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/17dtepi/i_wanna_try_any_of_these_concepts_3\n",
      "[OK] id=17dos9t | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/17dos9t/if_i_knew_i_would_be_in_this_position_right_now_i\n",
      "[OK] id=17d02gq | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/17d02gq/i_may_be_infertile_but_that_certainly_wont_stop\n",
      "[OK] id=17ctlyg | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/17ctlyg/my_friends_and_i_got_lost_we_walked_for_hours\n",
      "[OK] id=17b7ref | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/17b7ref/i_love_catching_guys_taking_pictures_of_me_its_so\n",
      "[OK] id=17awd5z | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/17awd5z/i_might_enjoy_being_taken_advantage_off_just_a\n",
      "[OK] id=17awpgq | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/17awpgq/he_threw_my_dress_out_the_window_a_few_blocks\n",
      "[OK] id=17b0m8q | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/17b0m8q/they_said_there_were_special_accommodations_for\n",
      "[OK] id=174ff9w | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/174ff9w/i_wanted_to_hang_with_my_friends_but_seems_my\n",
      "[OK] id=174hbur | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/174hbur/i_hate_night_cleaning_at_my_job\n",
      "[OK] id=16zc0ds | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/16zc0ds/help_tell_me_what_would_have_me_in_this_situation\n",
      "[OK] id=16y4ow0 | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/16y4ow0/hmmmmm_this_cosplay_party_misses_something_sigh_i\n",
      "[OK] id=15lqo4r | folder=media | link=https://www.reddit.com/r/rapefantasies/comments/15lqo4r/did_you_know_sex_store_workers_are_free_use_rape\n",
      "[OK] id=16nnb3k | folder=media | link=https://www.reddit.com/r/cumsluts/comments/16nnb3k/cum_loving_asian\n",
      "[OK] id=16lk8ag | folder=external | link=https://www.reddit.com/r/cumsluts/comments/16lk8ag/12_shots_but_she_survived\n",
      "[OK] id=16g6b1v | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/16g6b1v/all_goth_girls_spend_extra_time_on_their_makeup\n",
      "[OK] id=16ddubi | folder=external | link=https://www.reddit.com/r/cumsluts/comments/16ddubi/a_certified_cumslut\n",
      "[OK] id=16401dk | folder=external | link=https://www.reddit.com/r/BallsDeepThroat/comments/16401dk/relentless\n",
      "[OK] id=165wbfu | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/165wbfu/my_childhood_friend_found_some_meds_that_work_on\n",
      "[OK] id=163zc2p | folder=external | link=https://www.reddit.com/r/collegesluts/comments/163zc2p/my_ex_always_hated_that_men_looked_at_me\n",
      "[OK] id=163l5w9 | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/163l5w9/the_face_i_make_as_my_thoughts_are_fucked_out_of\n",
      "[OK] id=161fg27 | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/161fg27/invited_for_a_night_of_fun_neither_of_them_told\n",
      "[OK] id=15o4tvg | folder=media | link=https://www.reddit.com/r/RealGirls/comments/15o4tvg/summer_is_almost_overtime_to_work_on_the_winter\n",
      "[OK] id=15oaq5n | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/15oaq5n/i_might_have_ngh_spoken_too_soon\n",
      "[OK] id=15h4o40 | folder=media | link=https://www.reddit.com/r/rape_hentai/comments/15h4o40/whats_everyones_favorite_dynamic\n",
      "\n",
      "Saved failed links to: S:\\minds\\Desktop\\Downloader and Reddit System\\Saved-Reddit\\DOWNLOADERS\\out\\failed.csv\n",
      "\n",
      "Done. Success: 169, Skipped: 947, Failed: 2, Total: 1118. Output root: S:\\minds\\Desktop\\Downloader and Reddit System\\Saved-Reddit\\DOWNLOADERS\\out\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs): return x\n",
    "\n",
    "base_out = Path(OUT_DIR)\n",
    "base_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load links first (for progress bar)\n",
    "links: List[str] = []\n",
    "with open(CSV_PATH, newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if row and row[0].strip():\n",
    "            links.append(row[0].strip())\n",
    "\n",
    "ok = failed = skipped = 0\n",
    "failed_rows: List[Dict[str, str]] = []\n",
    "\n",
    "for link in tqdm(links, desc=\"Archiving posts\", unit=\"post\"):\n",
    "    pre_id, _ = parse_link(link)\n",
    "    if pre_id and already_archived(base_out, pre_id):\n",
    "        skipped += 1\n",
    "        continue\n",
    "    try:\n",
    "        post, comments_listing = fetch_full_post_and_comments(link)\n",
    "        pid = post.get(\"id\")\n",
    "        if not pid:\n",
    "            raise RuntimeError(\"Missing post id\")\n",
    "        category = classify_post(post)\n",
    "        archive = canonical_archive(post, comments_listing)\n",
    "        save_archive(archive, base_out, pid, category)\n",
    "        ok += 1\n",
    "        print(f\"[OK] id={pid} | folder={category} | link={link}\")\n",
    "    except Exception as e:\n",
    "        failed += 1\n",
    "        status = getattr(getattr(e, \"response\", None), \"status_code\", \"\")\n",
    "        failed_rows.append({\"link\": link, \"guessed_id\": pre_id or \"\", \"status\": str(status), \"error\": str(e)})\n",
    "    time.sleep(REQUEST_DELAY_SEC)\n",
    "\n",
    "# Export failed links\n",
    "if failed_rows:\n",
    "    fail_path = base_out / \"failed.csv\"\n",
    "    with fail_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"link\", \"guessed_id\", \"status\", \"error\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(failed_rows)\n",
    "    print(f\"\\nSaved failed links to: {fail_path.resolve()}\")\n",
    "\n",
    "print(f\"\\nDone. Success: {ok}, Skipped: {skipped}, Failed: {failed}, Total: {len(links)}. Output root: {base_out.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a03a470",
   "metadata": {},
   "source": [
    "# EXTERNAL LINK EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08a3df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Extract external links and download Redgifs as <post_id>.mp4 ===\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs): return x\n",
    "\n",
    "BASE_OUT = Path(OUT_DIR)\n",
    "EXTERNAL_DIR = BASE_OUT / \"external\"\n",
    "REDDITS_OK = {\"reddit.com\", \"www.reddit.com\", \"old.reddit.com\", \"np.reddit.com\", \"oauth.reddit.com\", \"redd.it\"}\n",
    "REDDIT_NATIVE_MEDIA = {\"i.redd.it\", \"v.redd.it\"}\n",
    "\n",
    "# ---- 1) Helpers to read archives and extract the outbound link ----\n",
    "def _domain(url: str) -> str:\n",
    "    try:\n",
    "        return urlparse(url).netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_external_url(archive_obj: dict) -> str | None:\n",
    "    \"\"\"\n",
    "    From your saved archive object:\n",
    "      { \"raw_post\": {...}, \"raw_comments\": {...}, \"comments\": [...] }\n",
    "    Pull the outbound link for external posts.\n",
    "    \"\"\"\n",
    "    post = archive_obj.get(\"raw_post\") or {}\n",
    "    # Prefer the 'url_overridden_by_dest' field; fallback to 'url'\n",
    "    url = post.get(\"url_overridden_by_dest\") or post.get(\"url\")\n",
    "    if not url:\n",
    "        return None\n",
    "\n",
    "    d = _domain(url)\n",
    "    # Treat non-Reddit, non-native-media as external\n",
    "    if d and d not in REDDITS_OK and d not in REDDIT_NATIVE_MEDIA:\n",
    "        return url\n",
    "    return None\n",
    "\n",
    "# ---- 2) Redgifs normalization & API download ----\n",
    "# Accept common Redgifs URL shapes:\n",
    "RE_REDGIFS_ID = re.compile(\n",
    "    r\"\"\"(?ix)\n",
    "    (?:^|/)(?:watch|ifr)/([a-z0-9]+)     # redgifs.com/watch/<id> or /ifr/<id>\n",
    "    |                                   # OR\n",
    "    (?:^|/)(?:i)/([a-z0-9]+)            # i.redgifs.com/i/<id>\n",
    "    \"\"\".strip()\n",
    ")\n",
    "\n",
    "def redgifs_id_from_url(url: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extract the media ID from redgifs-style URLs:\n",
    "      - https://redgifs.com/watch/<id>\n",
    "      - https://www.redgifs.com/watch/<id>\n",
    "      - https://v3.redgifs.com/watch/<id>\n",
    "      - https://redgifs.com/ifr/<id>\n",
    "      - https://i.redgifs.com/i/<id>\n",
    "    \"\"\"\n",
    "    m = RE_REDGIFS_ID.search(url)\n",
    "    if not m:\n",
    "        return None\n",
    "    # One of the two groups will be set\n",
    "    gid = m.group(1) or m.group(2)\n",
    "    return gid.lower() if gid else None\n",
    "\n",
    "# Redgifs API: get a temporary token, then resolve mp4 URLs\n",
    "REDGIFS_AUTH_URL = \"https://api.redgifs.com/v2/auth/temporary\"\n",
    "REDGIFS_GIF_URL  = \"https://api.redgifs.com/v2/gifs/{id}\"\n",
    "\n",
    "_SESSION = requests.Session()\n",
    "_RG_TOKEN = None\n",
    "_RG_TOKEN_TS = 0\n",
    "\n",
    "def redgifs_token(force: bool = False) -> str:\n",
    "    global _RG_TOKEN, _RG_TOKEN_TS\n",
    "    now = time.time()\n",
    "    # Reuse token for ~20 minutes unless forced\n",
    "    if not force and _RG_TOKEN and (now - _RG_TOKEN_TS) < 1200:\n",
    "        return _RG_TOKEN\n",
    "    r = _SESSION.get(REDGIFS_AUTH_URL, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    _RG_TOKEN = r.json().get(\"token\")\n",
    "    _RG_TOKEN_TS = now\n",
    "    if not _RG_TOKEN:\n",
    "        raise RuntimeError(\"Failed to obtain Redgifs token.\")\n",
    "    return _RG_TOKEN\n",
    "\n",
    "def redgifs_mp4_url(gid: str) -> str:\n",
    "    tok = redgifs_token()\n",
    "    headers = {\"Authorization\": f\"Bearer {tok}\"}\n",
    "    r = _SESSION.get(REDGIFS_GIF_URL.format(id=gid), headers=headers, timeout=30)\n",
    "    # If token expired, refresh once\n",
    "    if r.status_code in (401, 403):\n",
    "        tok = redgifs_token(force=True)\n",
    "        headers = {\"Authorization\": f\"Bearer {tok}\"}\n",
    "        r = _SESSION.get(REDGIFS_GIF_URL.format(id=gid), headers=headers, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    info = r.json().get(\"gif\") or {}\n",
    "    # Prefer HD if present, else SD, else fallback to urls.origin\n",
    "    urls = info.get(\"urls\") or {}\n",
    "    return urls.get(\"hd\") or urls.get(\"sd\") or urls.get(\"origin\")\n",
    "\n",
    "def download_stream(url: str, dest: Path, *, max_retries: int = 4):\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with _SESSION.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                with open(dest, \"wb\") as f:\n",
    "                    for chunk in r.iter_content(chunk_size=1024 * 256):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            if attempt + 1 >= max_retries:\n",
    "                raise\n",
    "            time.sleep(min(2 ** attempt, 15))\n",
    "\n",
    "# ---- 3) Walk external posts, export external links CSV, download Redgifs ----\n",
    "external_json_files = sorted(EXTERNAL_DIR.glob(\"*.json\"))\n",
    "print(f\"Found {len(external_json_files)} external post JSONs in {EXTERNAL_DIR}\")\n",
    "\n",
    "external_rows = []\n",
    "redgifs_failed = []\n",
    "\n",
    "REDGIFS_OUT = BASE_OUT / \"redgifs\"\n",
    "REDGIFS_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for fp in tqdm(external_json_files, desc=\"Scanning external posts\", unit=\"post\"):\n",
    "    try:\n",
    "        data = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "        post = (data or {}).get(\"raw_post\") or {}\n",
    "        pid  = post.get(\"id\") or fp.stem  # fallback to filename if needed\n",
    "\n",
    "        ext_url = extract_external_url(data)\n",
    "        if not ext_url:\n",
    "            # Still record that this external-typed file has no resolvable URL\n",
    "            external_rows.append({\"id\": pid, \"link\": \"\", \"domain\": \"\"})\n",
    "            continue\n",
    "\n",
    "        dom = _domain(ext_url)\n",
    "        external_rows.append({\"id\": pid, \"link\": ext_url, \"domain\": dom})\n",
    "\n",
    "        # Redgifs download\n",
    "        if \"redgifs.com\" in dom or dom.endswith(\".redgifs.com\"):\n",
    "            gid = redgifs_id_from_url(ext_url)\n",
    "            if not gid:\n",
    "                # Sometimes the external URL is a redirect page; skip but log\n",
    "                redgifs_failed.append({\"id\": pid, \"link\": ext_url, \"reason\": \"no_id_from_url\"})\n",
    "                continue\n",
    "\n",
    "            out_path = REDGIFS_OUT / f\"{pid}.mp4\"\n",
    "            if out_path.exists():\n",
    "                # already downloaded\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                mp4_url = redgifs_mp4_url(gid)\n",
    "                if not mp4_url:\n",
    "                    redgifs_failed.append({\"id\": pid, \"link\": ext_url, \"reason\": \"no_mp4_url\"})\n",
    "                    continue\n",
    "                download_stream(mp4_url, out_path)\n",
    "                # Show success line\n",
    "                print(f\"[REDGIFS] id={pid} -> {out_path.name}\")\n",
    "            except Exception as e:\n",
    "                redgifs_failed.append({\"id\": pid, \"link\": ext_url, \"reason\": str(e)})\n",
    "    except Exception as e:\n",
    "        # If we cannot read this JSON at all, log as a redgifs failure only if it looked like redgifs\n",
    "        redgifs_failed.append({\"id\": fp.stem, \"link\": \"\", \"reason\": f\"read_error: {e}\"})\n",
    "\n",
    "# ---- 4) Write summary CSVs ----\n",
    "ext_csv = BASE_OUT / \"external_links.csv\"\n",
    "with ext_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"id\", \"link\", \"domain\"])\n",
    "    w.writeheader()\n",
    "    w.writerows(external_rows)\n",
    "\n",
    "if redgifs_failed:\n",
    "    fail_csv = BASE_OUT / \"redgifs_failed.csv\"\n",
    "    with fail_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"id\", \"link\", \"reason\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(redgifs_failed)\n",
    "    print(f\"\\nSaved Redgifs download failures to: {fail_csv.resolve()}\")\n",
    "\n",
    "print(f\"\\nSaved external links to: {ext_csv.resolve()}\")\n",
    "print(f\"Redgifs saved (if any) to: {REDGIFS_OUT.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938da43",
   "metadata": {},
   "source": [
    "# MEDIA DOWNLOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb94da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Download embedded Reddit-hosted media for posts in out/media/*.json ===\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import html\n",
    "import mimetypes\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs): return x\n",
    "\n",
    "BASE_OUT = Path(OUT_DIR)\n",
    "MEDIA_JSON_DIR = BASE_OUT / \"media\"\n",
    "MEDIA_OUT_DIR = BASE_OUT / \"media_files\"\n",
    "MEDIA_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\"User-Agent\": \"reddit-media-downloader/1.0\"})\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _clean_url(u: str | None) -> str | None:\n",
    "    if not u:\n",
    "        return None\n",
    "    # Reddit often returns HTML-escaped URLs inside JSON\n",
    "    return html.unescape(u)\n",
    "\n",
    "def _domain(u: str | None) -> str:\n",
    "    if not u:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return urlparse(u).netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _ext_from_url_or_type(url: str | None, content_type: str | None) -> str:\n",
    "    # Prefer extension from URL, else derive from content-type\n",
    "    if url:\n",
    "        path = urlparse(url).path\n",
    "        ext = os.path.splitext(path)[1].lower()\n",
    "        if ext in {\".jpg\", \".jpeg\", \".png\", \".gif\", \".mp4\", \".webm\"}:\n",
    "            return ext\n",
    "    if content_type:\n",
    "        ext = mimetypes.guess_extension(content_type.split(\";\")[0].strip())\n",
    "        if ext:\n",
    "            # normalize jpeg\n",
    "            return \".jpg\" if ext == \".jpe\" else ext\n",
    "    # sensible default fallback\n",
    "    return \".mp4\" if (url and \".mp4\" in url) else \".jpg\"\n",
    "\n",
    "def _stream_download(url: str, dest: Path, *, max_retries: int = 4, chunk=1024 * 256):\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with SESSION.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                ctype = r.headers.get(\"Content-Type\")\n",
    "                # if dest has no extension yet, refine using content-type\n",
    "                if dest.suffix == \"\" and ctype:\n",
    "                    dest = dest.with_suffix(_ext_from_url_or_type(url, ctype))\n",
    "                with open(dest, \"wb\") as f:\n",
    "                    for part in r.iter_content(chunk_size=chunk):\n",
    "                        if part:\n",
    "                            f.write(part)\n",
    "            return dest  # final path (may include refined suffix)\n",
    "        except Exception:\n",
    "            if attempt + 1 >= max_retries:\n",
    "                raise\n",
    "            time.sleep(min(2 ** attempt, 15))\n",
    "\n",
    "def _pick_best_preview(post: dict) -> str | None:\n",
    "    \"\"\"\n",
    "    For image/GIF-like posts where 'preview' exists.\n",
    "    Prefer MP4 variant (smaller, plays everywhere), else best image 'source'.\n",
    "    \"\"\"\n",
    "    prev = post.get(\"preview\") or {}\n",
    "    variants = prev.get(\"variants\") or {}\n",
    "    # mp4 variant for gifs, etc.\n",
    "    mp4v = variants.get(\"mp4\") or variants.get(\"reddit_video_preview\")\n",
    "    if mp4v and mp4v.get(\"source\", {}).get(\"url\"):\n",
    "        return _clean_url(mp4v[\"source\"][\"url\"])\n",
    "    # fallback to the image source\n",
    "    src = (prev.get(\"images\") or [{}])[0].get(\"source\", {})\n",
    "    if src.get(\"url\"):\n",
    "        return _clean_url(src[\"url\"])\n",
    "    return None\n",
    "\n",
    "def _pick_vreddit_urls(post: dict) -> tuple[str | None, str | None]:\n",
    "    \"\"\"\n",
    "    v.redd.it posts: return (preferred_mp4_url, fallback_mp4_url)\n",
    "    Try in order: 'hls_url' (m3u8) -> 'fallback_url' (progressive) -> preview mp4.\n",
    "    We only directly download MP4 (no ffmpeg merge here), so we prefer fallback_url,\n",
    "    and otherwise try preview mp4.\n",
    "    \"\"\"\n",
    "    media = post.get(\"media\") or {}\n",
    "    rv = media.get(\"reddit_video\") or {}\n",
    "    fallback = rv.get(\"fallback_url\")  # often progressive mp4 (may be muted on long vids)\n",
    "    hls = rv.get(\"hls_url\")            # m3u8 playlist (would require ffmpeg)\n",
    "    # If no fallback, sometimes preview.mp4 exists:\n",
    "    prev_mp4 = None\n",
    "    prev = post.get(\"preview\") or {}\n",
    "    pv = prev.get(\"reddit_video_preview\") or {}\n",
    "    if isinstance(pv, dict) and pv.get(\"fallback_url\"):\n",
    "        prev_mp4 = pv[\"fallback_url\"]\n",
    "    return (_clean_url(fallback), _clean_url(prev_mp4 or hls))\n",
    "\n",
    "def _gallery_items(post: dict) -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    For gallery posts: return list of (url, suggested_ext).\n",
    "    Uses media_metadata to select best 's' rendition.\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    meta = post.get(\"media_metadata\") or {}\n",
    "    gdata = post.get(\"gallery_data\") or {}\n",
    "    order = [e.get(\"media_id\") for e in gdata.get(\"items\", []) if e.get(\"media_id\")]\n",
    "    for mid in order:\n",
    "        m = meta.get(mid) or {}\n",
    "        s = m.get(\"s\") or {}\n",
    "        url = _clean_url(s.get(\"mp4\") or s.get(\"gif\") or s.get(\"u\") or s.get(\"url\"))\n",
    "        if not url:\n",
    "            continue\n",
    "        # guess extension: mp4 preferred over gif over image\n",
    "        if \"mp4\" in s:\n",
    "            ext = \".mp4\"\n",
    "        elif \"gif\" in s:\n",
    "            ext = \".mp4\"  # we'll still download the gif URL, but use .mp4 if it's actually mp4\n",
    "        else:\n",
    "            # look at mime if present\n",
    "            m_type = m.get(\"m\")\n",
    "            ext = _ext_from_url_or_type(url, m_type)\n",
    "        items.append((url, ext))\n",
    "    return items\n",
    "\n",
    "# ---------- main walk ----------\n",
    "media_jsons = sorted(MEDIA_JSON_DIR.glob(\"*.json\"))\n",
    "print(f\"Found {len(media_jsons)} media post JSONs in {MEDIA_JSON_DIR}\")\n",
    "\n",
    "fail_rows = []\n",
    "downloaded = 0\n",
    "\n",
    "for fp in tqdm(media_jsons, desc=\"Downloading embedded media\", unit=\"post\"):\n",
    "    try:\n",
    "        data = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "        post = (data or {}).get(\"raw_post\") or {}\n",
    "        pid = post.get(\"id\") or fp.stem\n",
    "\n",
    "        # Prefer Reddit-hosted URL if present\n",
    "        url = _clean_url(post.get(\"url_overridden_by_dest\") or post.get(\"url\"))\n",
    "        dom = _domain(url)\n",
    "\n",
    "        # Case A: gallery\n",
    "        if post.get(\"is_gallery\") or (post.get(\"gallery_data\") and post.get(\"media_metadata\")):\n",
    "            items = _gallery_items(post)\n",
    "            if not items:\n",
    "                fail_rows.append({\"id\": pid, \"reason\": \"gallery_no_items\"})\n",
    "                continue\n",
    "            for idx, (item_url, ext) in enumerate(items, start=1):\n",
    "                outfile = MEDIA_OUT_DIR / f\"{pid}_g{idx:02d}{ext if ext.startswith('.') else ('.' + ext)}\"\n",
    "                if outfile.exists():\n",
    "                    continue\n",
    "                try:\n",
    "                    _stream_download(item_url, outfile)\n",
    "                    downloaded += 1\n",
    "                    print(f\"[GAL] {pid} -> {outfile.name}\")\n",
    "                except Exception as e:\n",
    "                    fail_rows.append({\"id\": pid, \"reason\": f\"gallery_item_fail:{e}\"})\n",
    "\n",
    "            continue  # next post\n",
    "\n",
    "        # Case B: native video (v.redd.it)\n",
    "        if (post.get(\"is_video\") or (post.get(\"media\") or {}).get(\"reddit_video\")) and dom.endswith(\"v.redd.it\"):\n",
    "            main_mp4, alt_mp4 = _pick_vreddit_urls(post)\n",
    "            target = MEDIA_OUT_DIR / f\"{pid}.mp4\"\n",
    "            if target.exists():\n",
    "                continue\n",
    "            src = main_mp4 or alt_mp4\n",
    "            if not src:\n",
    "                # last chance: look into preview variants\n",
    "                src = _pick_best_preview(post)\n",
    "            if not src:\n",
    "                fail_rows.append({\"id\": pid, \"reason\": \"vreddit_no_source\"})\n",
    "                continue\n",
    "            try:\n",
    "                _stream_download(src, target)\n",
    "                downloaded += 1\n",
    "                print(f\"[VID] {pid} -> {target.name}\")\n",
    "            except Exception as e:\n",
    "                fail_rows.append({\"id\": pid, \"reason\": f\"vreddit_dl_fail:{e}\"})\n",
    "            continue\n",
    "\n",
    "        # Case C: image / gif via i.redd.it or preview\n",
    "        if dom.endswith(\"i.redd.it\"):\n",
    "            # Direct i.redd.it link\n",
    "            ext = _ext_from_url_or_type(url, None)\n",
    "            target = MEDIA_OUT_DIR / f\"{pid}{ext}\"\n",
    "            if not target.exists():\n",
    "                try:\n",
    "                    _stream_download(url, target)\n",
    "                    downloaded += 1\n",
    "                    print(f\"[IMG] {pid} -> {target.name}\")\n",
    "                except Exception as e:\n",
    "                    fail_rows.append({\"id\": pid, \"reason\": f\"ireddit_dl_fail:{e}\"})\n",
    "            continue\n",
    "\n",
    "        # Fallback: try preview (covers some GIF-to-MP4 conversions)\n",
    "        prev_url = _pick_best_preview(post)\n",
    "        if prev_url and _domain(prev_url) in {\"i.redd.it\", \"v.redd.it\", \"preview.redd.it\"}:\n",
    "            ext = _ext_from_url_or_type(prev_url, None)\n",
    "            target = MEDIA_OUT_DIR / f\"{pid}{ext}\"\n",
    "            if not target.exists():\n",
    "                try:\n",
    "                    _stream_download(prev_url, target)\n",
    "                    downloaded += 1\n",
    "                    print(f\"[PREV] {pid} -> {target.name}\")\n",
    "                except Exception as e:\n",
    "                    fail_rows.append({\"id\": pid, \"reason\": f\"preview_dl_fail:{e}\"})\n",
    "            continue\n",
    "\n",
    "        # If we reach here, it looks like a Reddit-hosted \"media\" without a reliable direct URL\n",
    "        fail_rows.append({\"id\": pid, \"reason\": \"no_reddit_media_url\"})\n",
    "    except Exception as e:\n",
    "        fail_rows.append({\"id\": fp.stem, \"reason\": f\"read_error:{e}\"})\n",
    "\n",
    "# ---------- write failures ----------\n",
    "if fail_rows:\n",
    "    fail_csv = BASE_OUT / \"media_failed.csv\"\n",
    "    with fail_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"id\", \"reason\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(fail_rows)\n",
    "    print(f\"\\nSaved media failures to: {fail_csv.resolve()}\")\n",
    "\n",
    "print(f\"\\nDone. Downloaded: {downloaded}. Files saved under: {MEDIA_OUT_DIR.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Primary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
