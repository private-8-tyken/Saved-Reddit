{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4869123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reddit script app credentials (temporary hardcode ok for local use) ---\n",
    "REDDIT_CLIENT_ID = \"Ik1IhrLMkUe2Y7_jLqj-Ew\"\n",
    "REDDIT_CLIENT_SECRET = \"1j81ffxuNl-e8EzPV4D3OzCVCH-1lw\"\n",
    "REDDIT_USERNAME = \"Grand_Admiral_Tyken\"\n",
    "REDDIT_PASSWORD = \"X5bugNC9j3Bc^Uf\"\n",
    "\n",
    "\n",
    "# --- IO config ---\n",
    "CSV_PATH = \"links.csv\"   # one Reddit URL per line, no header\n",
    "OUT_DIR  = \"out\"         # base folder for outputs\n",
    "\n",
    "# --- polite request pacing ---\n",
    "REQUEST_DELAY_SEC = 0.5  # delay between requests to Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deda5f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "\n",
    "# Session + UA\n",
    "SESSION = requests.Session()\n",
    "UA = f\"reddit-json-downloader/1.0 (by u/{REDDIT_USERNAME})\"\n",
    "\n",
    "# OAuth endpoints and params\n",
    "OAUTH_TOKEN_URL = \"https://www.reddit.com/api/v1/access_token\"\n",
    "OAUTH_API_BASE  = \"https://oauth.reddit.com\"\n",
    "COMMENTS_QUERY  = \"raw_json=1&limit=500&depth=10&showmore=true\"  # fuller comment payload\n",
    "\n",
    "# URL helpers\n",
    "COMMENTS_ID_RE = re.compile(r\"/comments/([a-z0-9]{5,8})\", re.IGNORECASE)\n",
    "SHORTLINK_RE   = re.compile(r\"redd\\.it/([a-z0-9]{5,8})\", re.IGNORECASE)\n",
    "SUB_RE         = re.compile(r\"/r/([^/]+)/comments/\", re.IGNORECASE)\n",
    "\n",
    "def request_with_backoff(method: str, url: str, *, headers=None, data=None, timeout=60, max_retries=5):\n",
    "    for attempt in range(max_retries):\n",
    "        r = SESSION.request(method, url, headers=headers, data=data, timeout=timeout)\n",
    "        if r.status_code < 400:\n",
    "            return r\n",
    "        if r.status_code in (429,) or 500 <= r.status_code < 600:\n",
    "            time.sleep(min(2 ** attempt, 30))\n",
    "            continue\n",
    "        return r\n",
    "    return r  # last response\n",
    "\n",
    "def get_token() -> str:\n",
    "    auth = requests.auth.HTTPBasicAuth(REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET)\n",
    "    data = {\"grant_type\": \"password\", \"username\": REDDIT_USERNAME, \"password\": REDDIT_PASSWORD}\n",
    "    headers = {\"User-Agent\": UA}\n",
    "    r = requests.post(OAUTH_TOKEN_URL, auth=auth, data=data, headers=headers, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    tok = r.json().get(\"access_token\")\n",
    "    if not tok:\n",
    "        raise RuntimeError(f\"OAuth token missing; resp={r.text}\")\n",
    "    return tok\n",
    "\n",
    "def oauth_headers() -> dict:\n",
    "    tok = getattr(SESSION, \"_oauth_token\", None)\n",
    "    if not tok:\n",
    "        tok = get_token()\n",
    "        SESSION._oauth_token = tok\n",
    "    return {\"Authorization\": f\"bearer {tok}\", \"User-Agent\": UA}\n",
    "\n",
    "def accept_quarantine(subreddit: str) -> bool:\n",
    "    if not subreddit:\n",
    "        return False\n",
    "    url = f\"{OAUTH_API_BASE}/api/accept_quarantine\"\n",
    "    r = request_with_backoff(\"POST\", url, headers=oauth_headers(), max_retries=3)\n",
    "    return r.status_code in (200, 204, 409)  # 409 ~ already accepted\n",
    "\n",
    "def parse_link(link: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    m = COMMENTS_ID_RE.search(link)\n",
    "    post_id = m.group(1) if m else (SHORTLINK_RE.search(link).group(1) if SHORTLINK_RE.search(link) else None)\n",
    "    m_sr = SUB_RE.search(link)\n",
    "    subreddit = m_sr.group(1) if m_sr else None\n",
    "    return post_id, subreddit\n",
    "\n",
    "def normalize_comments_url(link: str, fallback_post_id: Optional[str]) -> str:\n",
    "    p = urlparse(link)\n",
    "    path = p.path or \"\"\n",
    "    host = (p.netloc or \"\").lower()\n",
    "    if \"redd.it\" in host or \"/comments/\" not in path:\n",
    "        if fallback_post_id:\n",
    "            path = f\"/comments/{fallback_post_id}/\"\n",
    "    if not path.endswith(\"/\"):\n",
    "        path += \"/\"\n",
    "    return f\"{OAUTH_API_BASE}{path}.json?{COMMENTS_QUERY}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8457222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_more_ids(listing_node) -> List[str]:\n",
    "    ids = []\n",
    "    def walk(node):\n",
    "        if not isinstance(node, dict): return\n",
    "        kind = node.get(\"kind\"); data = node.get(\"data\", {})\n",
    "        if kind == \"more\":\n",
    "            children = data.get(\"children\") or []\n",
    "            ids.extend([c for c in children if c])\n",
    "        elif kind in (\"Listing\", \"t1\"):\n",
    "            for ch in data.get(\"children\", []): walk(ch)\n",
    "            if kind == \"t1\" and isinstance(data.get(\"replies\"), dict): walk(data[\"replies\"])\n",
    "    walk(listing_node)\n",
    "    return ids\n",
    "\n",
    "def index_comments_by_id(listing_node) -> Dict[str, dict]:\n",
    "    idx = {}\n",
    "    def walk(node):\n",
    "        if not isinstance(node, dict): return\n",
    "        kind = node.get(\"kind\"); data = node.get(\"data\", {})\n",
    "        if kind == \"t1\":\n",
    "            cid = (data.get(\"id\") or \"\").lower()\n",
    "            if cid: idx[cid] = node\n",
    "            if isinstance(data.get(\"replies\"), dict): walk(data[\"replies\"])\n",
    "        elif kind == \"Listing\":\n",
    "            for ch in data.get(\"children\", []): walk(ch)\n",
    "    walk(listing_node)\n",
    "    return idx\n",
    "\n",
    "def replace_more_with_children(root_listing: dict, parent_lookup: Dict[str, dict], chunk_result: dict):\n",
    "    listing = chunk_result.get(\"json\", {}).get(\"data\", {}).get(\"things\", [])\n",
    "    for thing in listing:\n",
    "        if thing.get(\"kind\") != \"t1\": continue\n",
    "        data = thing.get(\"data\", {})\n",
    "        pid = data.get(\"parent_id\", \"\")\n",
    "        if pid.startswith(\"t1_\"):\n",
    "            parent_id = pid[3:].lower()\n",
    "            parent = parent_lookup.get(parent_id)\n",
    "            if parent:\n",
    "                if not isinstance(parent[\"data\"].get(\"replies\"), dict):\n",
    "                    parent[\"data\"][\"replies\"] = {\"kind\": \"Listing\", \"data\": {\"children\": []}}\n",
    "                parent[\"data\"][\"replies\"][\"data\"][\"children\"].append(thing)\n",
    "        elif pid.startswith(\"t3_\"):\n",
    "            root_listing[\"data\"][\"children\"].append(thing)\n",
    "\n",
    "def strip_more_nodes(node):\n",
    "    if not isinstance(node, dict): return\n",
    "    kind = node.get(\"kind\"); data = node.get(\"data\", {})\n",
    "    if kind == \"Listing\":\n",
    "        new_children = [ch for ch in data.get(\"children\", []) if ch.get(\"kind\") != \"more\"]\n",
    "        data[\"children\"] = new_children\n",
    "        for ch in new_children: strip_more_nodes(ch)\n",
    "    if kind == \"t1\" and isinstance(data.get(\"replies\"), dict):\n",
    "        strip_more_nodes(data[\"replies\"])\n",
    "\n",
    "def fetch_full_post_and_comments(link: str) -> Tuple[dict, dict]:\n",
    "    if not link.startswith((\"http://\", \"https://\")):\n",
    "        raise ValueError(f\"Not a URL: {link}\")\n",
    "\n",
    "    post_id, sr_hint = parse_link(link)\n",
    "    comments_url = normalize_comments_url(link, post_id)\n",
    "\n",
    "    r = request_with_backoff(\"GET\", comments_url, headers=oauth_headers(), timeout=60)\n",
    "    if r.status_code == 403:\n",
    "        sr = sr_hint or (re.search(r\"/r/([^/]+)/comments/\", comments_url).group(1) if re.search(r\"/r/([^/]+)/comments/\", comments_url) else \"\")\n",
    "        if sr and accept_quarantine(sr):\n",
    "            r = request_with_backoff(\"GET\", comments_url, headers=oauth_headers(), timeout=60)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    if not (isinstance(data, list) and len(data) >= 2):\n",
    "        raise RuntimeError(\"Unexpected Reddit JSON format\")\n",
    "\n",
    "    post_listing = data[0][\"data\"][\"children\"]\n",
    "    if not post_listing:\n",
    "        raise RuntimeError(\"Post listing empty\")\n",
    "    post = post_listing[0][\"data\"]\n",
    "    subreddit = post.get(\"subreddit\") or sr_hint or \"\"\n",
    "    link_id = post.get(\"id\")\n",
    "    comments_listing = data[1]\n",
    "\n",
    "    if link_id:\n",
    "        while True:\n",
    "            more_ids = collect_more_ids(comments_listing)\n",
    "            if not more_ids: break\n",
    "            for i in range(0, len(more_ids), 100):\n",
    "                chunk = more_ids[i:i+100]\n",
    "                form = {\n",
    "                    \"link_id\": f\"t3_{link_id}\",\n",
    "                    \"api_type\": \"json\",\n",
    "                    \"children\": \",\".join(chunk),\n",
    "                    \"sort\": \"confidence\",\n",
    "                    \"limit_children\": False,\n",
    "                    \"raw_json\": 1,\n",
    "                }\n",
    "                url = f\"{OAUTH_API_BASE}/api/morechildren\"\n",
    "                r2 = request_with_backoff(\"POST\", url, headers=oauth_headers(), data=form, timeout=60)\n",
    "                if r2.status_code == 403 and subreddit and accept_quarantine(subreddit):\n",
    "                    r2 = request_with_backoff(\"POST\", url, headers=oauth_headers(), data=form, timeout=60)\n",
    "                r2.raise_for_status()\n",
    "                payload = r2.json()\n",
    "                parent_idx = index_comments_by_id(comments_listing)\n",
    "                replace_more_with_children(comments_listing, parent_idx, payload)\n",
    "            strip_more_nodes(comments_listing)\n",
    "\n",
    "    return post, comments_listing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "381b7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification + canonical archive builder (matches your 1a1ybm-style top level)\n",
    "\n",
    "INTERNAL_REDDIT_HOSTS = {\n",
    "    \"reddit.com\", \"www.reddit.com\", \"old.reddit.com\", \"np.reddit.com\", \"oauth.reddit.com\",\n",
    "    \"redd.it\",\n",
    "}\n",
    "NATIVE_MEDIA_HOSTS = {\"i.redd.it\", \"v.redd.it\"}\n",
    "\n",
    "def domain_of(url: Optional[str]) -> str:\n",
    "    if not url: return \"\"\n",
    "    try:\n",
    "        return urlparse(url).netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def is_gallery(post: dict) -> bool:\n",
    "    return bool(post.get(\"is_gallery\") or (post.get(\"gallery_data\") and post.get(\"media_metadata\")))\n",
    "\n",
    "def summarize_media(post: dict) -> dict:\n",
    "    is_self = bool(post.get(\"is_self\"))\n",
    "    url = post.get(\"url_overridden_by_dest\") or post.get(\"url\")\n",
    "    dom = domain_of(url)\n",
    "    if is_self:\n",
    "        return {\"kind\": \"self\", \"files\": []}\n",
    "    if is_gallery(post):\n",
    "        return {\"kind\": \"gallery\", \"files\": []}\n",
    "    if dom in NATIVE_MEDIA_HOSTS or post.get(\"is_video\") or (post.get(\"media\") or {}).get(\"reddit_video\"):\n",
    "        return {\"kind\": \"video\", \"files\": []}\n",
    "    if dom and dom not in INTERNAL_REDDIT_HOSTS:\n",
    "        return {\"kind\": \"link\", \"files\": []}\n",
    "    if dom == \"i.redd.it\" or (post.get(\"preview\") and (post.get(\"post_hint\") or \"\").startswith(\"image\")):\n",
    "        return {\"kind\": \"image\", \"files\": []}\n",
    "    return {\"kind\": \"self\" if is_self else \"link\", \"files\": []}\n",
    "\n",
    "def external_link_or_none(post: dict) -> Optional[str]:\n",
    "    url = post.get(\"url_overridden_by_dest\") or post.get(\"url\")\n",
    "    dom = domain_of(url)\n",
    "    if url and dom and (dom not in INTERNAL_REDDIT_HOSTS) and (dom not in NATIVE_MEDIA_HOSTS):\n",
    "        return url\n",
    "    return None\n",
    "\n",
    "def classify_post(post_data: dict) -> str:\n",
    "    \"\"\"\n",
    "    Decide among: 'external', 'media', 'text'.\n",
    "    Priority:\n",
    "      1) external  -> off-Reddit (e.g., redgifs.com)\n",
    "      2) media     -> native Reddit media (i.redd.it, v.redd.it), previews, galleries\n",
    "      3) text      -> self-posts without media\n",
    "    \"\"\"\n",
    "    is_self = bool(post_data.get(\"is_self\"))\n",
    "    url = post_data.get(\"url_overridden_by_dest\") or post_data.get(\"url\")\n",
    "    d = domain_of(url) or (post_data.get(\"domain\") or \"\").lower()\n",
    "\n",
    "    if not is_self and d and (d not in INTERNAL_REDDIT_HOSTS) and (d not in NATIVE_MEDIA_HOSTS):\n",
    "        return \"external\"\n",
    "\n",
    "    post_hint   = (post_data.get(\"post_hint\") or \"\").lower()\n",
    "    has_gallery = bool(post_data.get(\"gallery_data\"))\n",
    "    has_preview = bool(post_data.get(\"preview\"))\n",
    "    has_media   = bool(post_data.get(\"media\")) or bool(post_data.get(\"is_video\"))\n",
    "    is_native_media_host = d in NATIVE_MEDIA_HOSTS\n",
    "    is_media_hint = post_hint in {\"image\", \"hosted:video\", \"rich:video\"}\n",
    "\n",
    "    if is_native_media_host or has_media or has_preview or has_gallery or is_media_hint:\n",
    "        return \"media\"\n",
    "    return \"text\"\n",
    "\n",
    "def canonical_archive(post: dict, comments_listing: dict) -> dict:\n",
    "    permalink = post.get(\"permalink\") or \"\"\n",
    "    if permalink and not permalink.startswith(\"http\"):\n",
    "        permalink = f\"https://www.reddit.com{permalink}\"\n",
    "    return {\n",
    "        \"archived_at\": time.strftime(\"%Y-%m-%dT%H:%M:%S.000000+00:00\", time.gmtime()),\n",
    "        \"reddit_fullname\": f\"t3_{post.get('id') or ''}\",\n",
    "        \"reddit_id\": post.get(\"id\"),\n",
    "        \"permalink\": permalink,\n",
    "        \"title\": post.get(\"title\"),\n",
    "        \"selftext\": post.get(\"selftext\"),\n",
    "        \"author\": post.get(\"author\"),\n",
    "        \"author_fullname\": post.get(\"author_fullname\"),\n",
    "        \"subreddit\": post.get(\"subreddit\"),\n",
    "        \"subreddit_id\": post.get(\"subreddit_id\"),\n",
    "        \"created_utc\": post.get(\"created_utc\"),\n",
    "        \"is_self\": post.get(\"is_self\"),\n",
    "        \"url\": post.get(\"url\"),\n",
    "        \"domain\": post.get(\"domain\"),\n",
    "        \"post_hint\": post.get(\"post_hint\"),\n",
    "        \"is_gallery\": is_gallery(post),\n",
    "        \"over_18\": post.get(\"over_18\"),\n",
    "        \"spoiler\": post.get(\"spoiler\"),\n",
    "        \"link_flair_text\": post.get(\"link_flair_text\"),\n",
    "        \"is_original_content\": post.get(\"is_original_content\"),\n",
    "        \"stickied\": post.get(\"stickied\"),\n",
    "        \"locked\": post.get(\"locked\"),\n",
    "        \"edited\": post.get(\"edited\"),\n",
    "        \"num_comments\": post.get(\"num_comments\"),\n",
    "        \"score\": post.get(\"score\"),\n",
    "        \"upvote_ratio\": post.get(\"upvote_ratio\"),\n",
    "        \"media\": summarize_media(post),\n",
    "        \"external_link\": external_link_or_none(post),\n",
    "        \"raw_post\": post,\n",
    "        \"raw_comments\": comments_listing,\n",
    "    }\n",
    "\n",
    "def save_archive(doc: dict, base_out: Path, post_id: str, category: str):\n",
    "    target = base_out / category\n",
    "    target.mkdir(parents=True, exist_ok=True)\n",
    "    (target / f\"{post_id}.json\").write_text(json.dumps(doc, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def already_archived(base_out: Path, pid: str) -> bool:\n",
    "    return any((base_out / sub / f\"{pid}.json\").exists() for sub in (\"media\", \"external\", \"text\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f89f87a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b84eeb3a9c8246f2a6ef838db34bd3fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Archiving posts:   0%|          | 0/791 [00:00<?, ?post/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] id=1kg1uki | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1kg1uki/whats_the_deal_with_anal\n",
      "[OK] id=1j4w59d | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1j4w59d/city_rape_bait_hijinks\n",
      "[OK] id=1ir6m9d | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1ir6m9d/raped_by_every_single_male_friend_of_mine\n",
      "[OK] id=1ib4y8d | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1ib4y8d/my_online_dom_hypnod_me_into_servitude_and_i\n",
      "[OK] id=1gf7h8j | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1gf7h8j/my_boyfriend_anally_raped_me_while_i_was_sick\n",
      "[OK] id=1dzbrl6 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1dzbrl6/i_cheated_on_my_boyfriend_by_rape_baiting_because\n",
      "[OK] id=1n48w0q | folder=external | link=https://www.reddit.com/r/rapefantasies/comments/1n48w0q/asian_hubby_let_this_white_stranger_rape_me_in\n",
      "[OK] id=1mxhnof | folder=external | link=https://www.reddit.com/r/rapefantasies/comments/1mxhnof/used_as_a_party_hole_at_friends_party_where_20\n",
      "[OK] id=1mt2rbv | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1mt2rbv/my_housemate_raped_me_while_i_was_passed_out\n",
      "[OK] id=1mpy7ta | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1mpy7ta/i_used_to_help_counsel_other_victims\n",
      "[OK] id=1mlh4fo | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1mlh4fo/completely_overpowered_by_a_football_player\n",
      "[OK] id=1m6qbff | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1m6qbff/i_was_raped_by_my_rich_friends_dad\n",
      "[OK] id=1m63iq0 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1m63iq0/how_should_i_approach_a_guy_who_raped_me\n",
      "[OK] id=1ly9j0w | folder=text | link=https://www.reddit.com/r/AhegaoGirls/comments/1ly9j0w/my_dadzie_saw_me_doing_ahegao_and_he_was_like_wtf\n",
      "[OK] id=1ltpc6n | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1ltpc6n/a_guy_tricked_me_into_having_sex_at_a_house_party\n",
      "[OK] id=1lqkd94 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1lqkd94/gang_raped_by_4_black_guys\n",
      "[OK] id=1lc3hzk | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1lc3hzk/arranged_gang_bang_rape_but_mistakes_happen\n",
      "[OK] id=1kztbyd | folder=text | link=https://www.reddit.com/r/rape_hentai/comments/1kztbyd/someone_please_do_this_to_me\n",
      "[OK] id=1kr2hez | folder=text | link=https://www.reddit.com/r/rape_hentai/comments/1kr2hez/get_this_filthy_thing_away_from_me\n",
      "[OK] id=1kkzal4 | folder=text | link=https://www.reddit.com/r/rape_hentai/comments/1kkzal4/shouldve_had_a_flying_type_bitch\n",
      "[OK] id=1kal0p9 | folder=text | link=https://www.reddit.com/r/sandiegosinglez/comments/1kal0p9/f21_would_you_smash_or_pass\n",
      "[OK] id=1k4ac78 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1k4ac78/my_most_humiliating_gang_rape\n",
      "[OK] id=1k0puoa | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1k0puoa/i_fucked_up_my_life_by_flirting_with_a_coworker\n",
      "[OK] id=1jygm41 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1jygm41/drunk_insecure_college_girl\n",
      "[OK] id=1jupbna | folder=text | link=https://www.reddit.com/r/rape_hentai/comments/1jupbna/nowhere_would_be_safe_with_an_app_like_this\n",
      "[OK] id=1jsrxb9 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1jsrxb9/first_time_on_the_train_in_japan\n",
      "[OK] id=1jfbar3 | folder=text | link=https://www.reddit.com/r/rape_hentai/comments/1jfbar3/im_totally_against_rape_thats_why_i_always_ask\n",
      "[OK] id=1jb9b2u | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1jb9b2u/meeting_up_with_my_rapist\n",
      "[OK] id=1jb9vbo | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1jb9vbo/they_made_me_a_slut\n",
      "[OK] id=1j75w4p | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1j75w4p/getting_raped_after_using_my_friends_shower_at_a\n",
      "[OK] id=1j1yvcn | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1j1yvcn/two_men_at_once\n",
      "[OK] id=1ihl2ax | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1ihl2ax/one_of_my_patients_lured_me_to_his_apartment_for\n",
      "[OK] id=1ibf6z1 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1ibf6z1/raped_after_a_night_out_cheating\n",
      "[OK] id=1i8yt0y | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1i8yt0y/my_college_friend_claimed_she_found_us_a_ride\n",
      "[OK] id=1i7hp5f | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1i7hp5f/getting_raped_by_my_sisters_black_friends_made_me\n",
      "[OK] id=1i6cvrm | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1i6cvrm/my_f20_bf_m21_proved_me_wrong\n",
      "[OK] id=1i1r8js | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1i1r8js/they_raped_me_on_the_way_back_to_my_dorm_and\n",
      "[OK] id=d57lf5 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/d57lf5/how_i_accidentally_brainwashed_myself_to_cheer\n",
      "[OK] id=1i0jkv5 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1i0jkv5/my_best_friends_story_about_her_rape_turned_me_on\n",
      "[OK] id=1hurhp1 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1hurhp1/i_got_passed_around_on_new_years_eve\n",
      "[OK] id=1hmwtyu | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1hmwtyu/not_my_first_my_most_recent_defeated\n",
      "[OK] id=1hmax0a | folder=text | link=https://www.reddit.com/r/cumsluts/comments/1hmax0a/sharing_is_caring\n",
      "[OK] id=1hmb6rq | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1hmb6rq/my_best_friend_made_me_choke_myself\n",
      "[OK] id=1hmaet6 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1hmaet6/i_got_drunk_and_gangraped_after_a_christmas_eve\n",
      "[OK] id=1hjmf9s | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1hjmf9s/raped_by_my_best_friend_and_3_of_his_friends\n",
      "[OK] id=1hjdvd6 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1hjdvd6/f18_got_drunk_at_a_party_last_night_and_got_raped\n",
      "[OK] id=1gvqqi7 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1gvqqi7/the_first_time_i_was_raped_in_my_sleep_by_my\n",
      "[OK] id=1gfk0sy | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1gfk0sy/movies_for_my_bf_to_rape_me_to\n",
      "[OK] id=1g6a1h9 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1g6a1h9/i_wouldnt_indulge_my_boyfriends_cuckold_kink_so\n",
      "[OK] id=1g5o38j | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1g5o38j/i_keep_going_back_to_my_rapist_ex\n",
      "[OK] id=1g57d7h | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1g57d7h/raped_on_my_bachelorette_party\n",
      "[OK] id=1ff8i1k | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1ff8i1k/raped_while_pretending_to_be_asleep\n",
      "[OK] id=1f9j4td | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1f9j4td/got_raped_on_my_birthday\n",
      "[OK] id=1f7fdbd | folder=text | link=https://www.reddit.com/r/rape_hentai/comments/1f7fdbd/are_you_a_little_rapecunt_desperate_to_be_used\n",
      "[OK] id=1f2caov | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1f2caov/raped_in_my_car_for_5_hours\n",
      "[OK] id=1eui4x8 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1eui4x8/a_new_low\n",
      "[OK] id=1etx3r5 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1etx3r5/my_angry_bf_raped_me\n",
      "[OK] id=1es656x | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1es656x/i_became_the_neighborhood_slut_in_the_projects\n",
      "[OK] id=1eqhtgg | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1eqhtgg/carrying_my_rapists_baby\n",
      "[OK] id=1emnx2p | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1emnx2p/i_came_out_as_trans_to_my_brother_and_he_raped_me\n",
      "[OK] id=1eknwz5 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1eknwz5/hypersexual_at_the_doctors\n",
      "[OK] id=1ej5wqe | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1ej5wqe/i_25f_got_raped_by_an_ex_at_a_party_and_my_bf_34m\n",
      "[OK] id=1eg1j8e | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1eg1j8e/wheres_the_line_between_trauma_and_turn_on\n",
      "[OK] id=1e6swng | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1e6swng/one_drunk_halloween_night\n",
      "[OK] id=akocp3 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/akocp3/my_experience_of_rape_abuse_and_stockholm\n",
      "[OK] id=aru6ss | folder=text | link=https://www.reddit.com/r/Rapekink/comments/aru6ss/just_another_origin_story\n",
      "[OK] id=1dwzkph | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1dwzkph/somnophilia_sleep_rape_ghb\n",
      "[OK] id=1dv2nx5 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1dv2nx5/every_year_i_get_raped_on_my_birthday\n",
      "[OK] id=1dsri0d | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1dsri0d/my_drink_was_spiked_i_drank_it_anyway\n",
      "[OK] id=t1kxhg | folder=text | link=https://www.reddit.com/r/Rapekink/comments/t1kxhg/i_recently_found_a_video_of_my_own_rape\n",
      "[OK] id=1dlxajv | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1dlxajv/4_months_of_madness\n",
      "[OK] id=1diizhr | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1diizhr/a_girlboss_an_unexpected_journey\n",
      "[OK] id=1904phs | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1904phs/raped_into_the_new_year\n",
      "[OK] id=1dc5oqz | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1dc5oqz/i_will_never_be_the_same_full_story\n",
      "[OK] id=1daqk6l | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1daqk6l/trying_to_make_sense_of_why_i_am_like_this\n",
      "[OK] id=1d9r6j0 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1d9r6j0/stumbled_upon_a_bad_part_of_town_while_urban\n",
      "[OK] id=1cvte65 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1cvte65/i_think_im_addicted_to_ithim_and_i_need_help\n",
      "[OK] id=1cqez1w | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1cqez1w/my_first_night_at_college_ended_in_an_intoxicated\n",
      "[OK] id=16pt8he | folder=text | link=https://www.reddit.com/r/Rapekink/comments/16pt8he/raped_for_my_share_of_rent\n",
      "[OK] id=1chwry2 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1chwry2/he_broke_me\n",
      "[OK] id=1ce1ujb | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1ce1ujb/when_it_happened_in_a_park\n",
      "[OK] id=1b0v6ng | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1b0v6ng/f20_my_friend_raped_me_anally\n",
      "[OK] id=1ah2x6k | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1ah2x6k/anal_rape_victim_anal_princess\n",
      "[OK] id=1cdkoxm | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1cdkoxm/right_back_at_it_tinder_baiting\n",
      "[OK] id=1c6cp1i | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1c6cp1i/24f_gratification_from_sharing_my_body\n",
      "[OK] id=1c495gc | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1c495gc/hooking_up_with_the_guy_who_raped_me\n",
      "[OK] id=1c42teq | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1c42teq/confession_3\n",
      "[OK] id=1c38084 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1c38084/i_41f_got_drugged_and_raped_at_a_frat_party\n",
      "[OK] id=1c2fsov | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1c2fsov/confession_2\n",
      "[OK] id=1c1ok26 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1c1ok26/i_was_gang_raped_in_college\n",
      "[OK] id=1byq4ll | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1byq4ll/i_had_the_epiphany_the_best_sex_of_my_life_was\n",
      "[OK] id=1bxtxs4 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1bxtxs4/i_used_to_go_to_house_parties_alone_all_the_time\n",
      "[OK] id=1bsoz7o | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1bsoz7o/to_the_wlw_with_this_kink\n",
      "[OK] id=15h799u | folder=text | link=https://www.reddit.com/r/Rapekink/comments/15h799u/sex_trafficked\n",
      "[OK] id=ppron2 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/ppron2/i_basically_became_a_sex_slave_in_a_small_village\n",
      "[OK] id=khforx | folder=text | link=https://www.reddit.com/r/Rapekink/comments/khforx/i_19f_and_my_friend_18f_were_raped_onboard_a\n",
      "[OK] id=1ayrigk | folder=text | link=https://www.reddit.com/r/rape_hentai/comments/1ayrigk/what_are_you_doing_if_you_found_them_like_this\n",
      "[OK] id=1an7743 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1an7743/my_brother_hates_that_im_a_lesbian\n",
      "[OK] id=1adjxok | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1adjxok/my_virgin_pussy_and_ass_were_raped_by_two_police\n",
      "[OK] id=1ae0dyt | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1ae0dyt/cocky_date_assumed_he_knew_my_own_body_better\n",
      "[OK] id=1abq2lv | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1abq2lv/i_24f_paid_our_protection_fee_for_my_familys\n",
      "[OK] id=1abw355 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1abw355/raped_by_my_porn_addicted_brother\n",
      "[OK] id=19f9bx9 | folder=text | link=https://www.reddit.com/r/rape_hentai/comments/19f9bx9/what_of_this_mark_will_you_choose\n",
      "[OK] id=19dys6d | folder=text | link=https://www.reddit.com/r/Rapekink/comments/19dys6d/i_was_actually_begging_for_it\n",
      "[OK] id=19d0vqo | folder=text | link=https://www.reddit.com/r/Rapekink/comments/19d0vqo/the_second_time_at_the_gangs_house\n",
      "[OK] id=193hglq | folder=text | link=https://www.reddit.com/r/Rapekink/comments/193hglq/i_24f_was_coerced_into_sex_with_gang_members_for\n",
      "[OK] id=19bf2rv | folder=text | link=https://www.reddit.com/r/Rapekink/comments/19bf2rv/coercion_works_one_two_many_times\n",
      "[OK] id=1982c2l | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1982c2l/exposing_myself_as_a_rape_toy_to_my_friends\n",
      "[OK] id=196osdo | folder=text | link=https://www.reddit.com/r/Rapekink/comments/196osdo/bf_likes_to_embarrass_me_in_front_of_his_friends\n",
      "[OK] id=195v26f | folder=text | link=https://www.reddit.com/r/Rapekink/comments/195v26f/when_i_was_really_young_i_got_gangraped_and_now\n",
      "[OK] id=11i1j85 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/11i1j85/my_two_cnc_rapists\n",
      "[OK] id=avzyon | folder=text | link=https://www.reddit.com/r/Rapekink/comments/avzyon/my_stepbrother_used_me_as_his_fucktoy_for_3_years\n",
      "[OK] id=190d55y | folder=text | link=https://www.reddit.com/r/Rapekink/comments/190d55y/19f_raped_at_a_local_metal_show\n",
      "[OK] id=18bo0wo | folder=text | link=https://www.reddit.com/r/Rapekink/comments/18bo0wo/husband37_anally_raped_me24_before_church_and\n",
      "[OK] id=18rbo8b | folder=text | link=https://www.reddit.com/r/Rapekink/comments/18rbo8b/accidently_sent_a_racist_text_to_my_friend_she\n",
      "[OK] id=18yimy0 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/18yimy0/forced_in_the_ghetto_because_i_24f_didnt_know_the\n",
      "[OK] id=18xsita | folder=text | link=https://www.reddit.com/r/Rapekink/comments/18xsita/new_years\n",
      "[OK] id=18wk79r | folder=text | link=https://www.reddit.com/r/Rapekink/comments/18wk79r/new_year_new_rapes\n",
      "[OK] id=18pezj7 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/18pezj7/on_the_train\n",
      "[OK] id=18b3ass | folder=text | link=https://www.reddit.com/r/Rapekink/comments/18b3ass/cnc_with_an_unexpected_ending\n",
      "[OK] id=184t8ef | folder=text | link=https://www.reddit.com/r/Rapekink/comments/184t8ef/i_let_my_bfs_best_friend_rape_me_in_my_sleep\n",
      "[OK] id=17nxd85 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/17nxd85/my_pussy_makes_normal_men_into_rapists\n",
      "[OK] id=1824b7c | folder=text | link=https://www.reddit.com/r/Rapekink/comments/1824b7c/got_raped_in_someones_backyard_at_a_house_show\n",
      "[OK] id=180ably | folder=text | link=https://www.reddit.com/r/Rapekink/comments/180ably/19f_the_guy_im_seeing_is_really_pushy_and_it\n",
      "[OK] id=knb2mg | folder=text | link=https://www.reddit.com/r/Rapekink/comments/knb2mg/rapebaiting_while_hitchhiking_last_year\n",
      "[OK] id=17rfhi9 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/17rfhi9/my_halloween_night\n",
      "[OK] id=17pkxu7 | folder=text | link=https://www.reddit.com/r/rape_hentai/comments/17pkxu7/shouldnt_have_opened_that_door\n",
      "[OK] id=17pacy5 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/17pacy5/one_of_the_ways_my_dom_abused_me_that_ruined_me\n",
      "[OK] id=17j5y5b | folder=text | link=https://www.reddit.com/r/Rapekink/comments/17j5y5b/i_was_held_against_my_will_by_two_men_and_i_get\n",
      "[OK] id=17j32va | folder=text | link=https://www.reddit.com/r/Rapekink/comments/17j32va/18f_halloween_baiting\n",
      "[OK] id=kzxgd5 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/kzxgd5/kidnapped_and_raped_for_7_years\n",
      "[OK] id=l1v492 | folder=text | link=https://www.reddit.com/r/Rapekink/comments/l1v492/hostage_for_4_years\n",
      "[OK] id=16utfek | folder=text | link=https://www.reddit.com/r/Rapekink/comments/16utfek/i_19f_was_abducted_gangraped_and_then_gentleraped\n",
      "[OK] id=150j3xy | folder=text | link=https://www.reddit.com/r/Rapekink/comments/150j3xy/i_19f_was_abducted_and_gangraped\n",
      "\n",
      "Done. Success: 134, Skipped: 657, Failed: 0, Total: 791. Output root: S:\\minds\\Desktop\\Downloader and Reddit System\\Saved-Reddit\\DOWNLOADERS\\out\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs): return x\n",
    "\n",
    "base_out = Path(OUT_DIR)\n",
    "base_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load links first (for progress bar)\n",
    "links: List[str] = []\n",
    "with open(CSV_PATH, newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if row and row[0].strip():\n",
    "            links.append(row[0].strip())\n",
    "\n",
    "ok = failed = skipped = 0\n",
    "failed_rows: List[Dict[str, str]] = []\n",
    "\n",
    "for link in tqdm(links, desc=\"Archiving posts\", unit=\"post\"):\n",
    "    pre_id, _ = parse_link(link)\n",
    "    if pre_id and already_archived(base_out, pre_id):\n",
    "        skipped += 1\n",
    "        continue\n",
    "    try:\n",
    "        post, comments_listing = fetch_full_post_and_comments(link)\n",
    "        pid = post.get(\"id\")\n",
    "        if not pid:\n",
    "            raise RuntimeError(\"Missing post id\")\n",
    "        category = classify_post(post)\n",
    "        archive = canonical_archive(post, comments_listing)\n",
    "        save_archive(archive, base_out, pid, category)\n",
    "        ok += 1\n",
    "        print(f\"[OK] id={pid} | folder={category} | link={link}\")\n",
    "    except Exception as e:\n",
    "        failed += 1\n",
    "        status = getattr(getattr(e, \"response\", None), \"status_code\", \"\")\n",
    "        failed_rows.append({\"link\": link, \"guessed_id\": pre_id or \"\", \"status\": str(status), \"error\": str(e)})\n",
    "    time.sleep(REQUEST_DELAY_SEC)\n",
    "\n",
    "# Export failed links\n",
    "if failed_rows:\n",
    "    fail_path = base_out / \"failed.csv\"\n",
    "    with fail_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"link\", \"guessed_id\", \"status\", \"error\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(failed_rows)\n",
    "    print(f\"\\nSaved failed links to: {fail_path.resolve()}\")\n",
    "\n",
    "print(f\"\\nDone. Success: {ok}, Skipped: {skipped}, Failed: {failed}, Total: {len(links)}. Output root: {base_out.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a03a470",
   "metadata": {},
   "source": [
    "# EXTERNAL LINK EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08a3df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Extract external links and download Redgifs as <post_id>.mp4 ===\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs): return x\n",
    "\n",
    "BASE_OUT = Path(OUT_DIR)\n",
    "EXTERNAL_DIR = BASE_OUT / \"external\"\n",
    "REDDITS_OK = {\"reddit.com\", \"www.reddit.com\", \"old.reddit.com\", \"np.reddit.com\", \"oauth.reddit.com\", \"redd.it\"}\n",
    "REDDIT_NATIVE_MEDIA = {\"i.redd.it\", \"v.redd.it\"}\n",
    "\n",
    "# ---- 1) Helpers to read archives and extract the outbound link ----\n",
    "def _domain(url: str) -> str:\n",
    "    try:\n",
    "        return urlparse(url).netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_external_url(archive_obj: dict) -> str | None:\n",
    "    \"\"\"\n",
    "    From your saved archive object:\n",
    "      { \"raw_post\": {...}, \"raw_comments\": {...}, \"comments\": [...] }\n",
    "    Pull the outbound link for external posts.\n",
    "    \"\"\"\n",
    "    post = archive_obj.get(\"raw_post\") or {}\n",
    "    # Prefer the 'url_overridden_by_dest' field; fallback to 'url'\n",
    "    url = post.get(\"url_overridden_by_dest\") or post.get(\"url\")\n",
    "    if not url:\n",
    "        return None\n",
    "\n",
    "    d = _domain(url)\n",
    "    # Treat non-Reddit, non-native-media as external\n",
    "    if d and d not in REDDITS_OK and d not in REDDIT_NATIVE_MEDIA:\n",
    "        return url\n",
    "    return None\n",
    "\n",
    "# ---- 2) Redgifs normalization & API download ----\n",
    "# Accept common Redgifs URL shapes:\n",
    "RE_REDGIFS_ID = re.compile(\n",
    "    r\"\"\"(?ix)\n",
    "    (?:^|/)(?:watch|ifr)/([a-z0-9]+)     # redgifs.com/watch/<id> or /ifr/<id>\n",
    "    |                                   # OR\n",
    "    (?:^|/)(?:i)/([a-z0-9]+)            # i.redgifs.com/i/<id>\n",
    "    \"\"\".strip()\n",
    ")\n",
    "\n",
    "def redgifs_id_from_url(url: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extract the media ID from redgifs-style URLs:\n",
    "      - https://redgifs.com/watch/<id>\n",
    "      - https://www.redgifs.com/watch/<id>\n",
    "      - https://v3.redgifs.com/watch/<id>\n",
    "      - https://redgifs.com/ifr/<id>\n",
    "      - https://i.redgifs.com/i/<id>\n",
    "    \"\"\"\n",
    "    m = RE_REDGIFS_ID.search(url)\n",
    "    if not m:\n",
    "        return None\n",
    "    # One of the two groups will be set\n",
    "    gid = m.group(1) or m.group(2)\n",
    "    return gid.lower() if gid else None\n",
    "\n",
    "# Redgifs API: get a temporary token, then resolve mp4 URLs\n",
    "REDGIFS_AUTH_URL = \"https://api.redgifs.com/v2/auth/temporary\"\n",
    "REDGIFS_GIF_URL  = \"https://api.redgifs.com/v2/gifs/{id}\"\n",
    "\n",
    "_SESSION = requests.Session()\n",
    "_RG_TOKEN = None\n",
    "_RG_TOKEN_TS = 0\n",
    "\n",
    "def redgifs_token(force: bool = False) -> str:\n",
    "    global _RG_TOKEN, _RG_TOKEN_TS\n",
    "    now = time.time()\n",
    "    # Reuse token for ~20 minutes unless forced\n",
    "    if not force and _RG_TOKEN and (now - _RG_TOKEN_TS) < 1200:\n",
    "        return _RG_TOKEN\n",
    "    r = _SESSION.get(REDGIFS_AUTH_URL, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    _RG_TOKEN = r.json().get(\"token\")\n",
    "    _RG_TOKEN_TS = now\n",
    "    if not _RG_TOKEN:\n",
    "        raise RuntimeError(\"Failed to obtain Redgifs token.\")\n",
    "    return _RG_TOKEN\n",
    "\n",
    "def redgifs_mp4_url(gid: str) -> str:\n",
    "    tok = redgifs_token()\n",
    "    headers = {\"Authorization\": f\"Bearer {tok}\"}\n",
    "    r = _SESSION.get(REDGIFS_GIF_URL.format(id=gid), headers=headers, timeout=30)\n",
    "    # If token expired, refresh once\n",
    "    if r.status_code in (401, 403):\n",
    "        tok = redgifs_token(force=True)\n",
    "        headers = {\"Authorization\": f\"Bearer {tok}\"}\n",
    "        r = _SESSION.get(REDGIFS_GIF_URL.format(id=gid), headers=headers, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    info = r.json().get(\"gif\") or {}\n",
    "    # Prefer HD if present, else SD, else fallback to urls.origin\n",
    "    urls = info.get(\"urls\") or {}\n",
    "    return urls.get(\"hd\") or urls.get(\"sd\") or urls.get(\"origin\")\n",
    "\n",
    "def download_stream(url: str, dest: Path, *, max_retries: int = 4):\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with _SESSION.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                with open(dest, \"wb\") as f:\n",
    "                    for chunk in r.iter_content(chunk_size=1024 * 256):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            if attempt + 1 >= max_retries:\n",
    "                raise\n",
    "            time.sleep(min(2 ** attempt, 15))\n",
    "\n",
    "# ---- 3) Walk external posts, export external links CSV, download Redgifs ----\n",
    "external_json_files = sorted(EXTERNAL_DIR.glob(\"*.json\"))\n",
    "print(f\"Found {len(external_json_files)} external post JSONs in {EXTERNAL_DIR}\")\n",
    "\n",
    "external_rows = []\n",
    "redgifs_failed = []\n",
    "\n",
    "REDGIFS_OUT = BASE_OUT / \"redgifs\"\n",
    "REDGIFS_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for fp in tqdm(external_json_files, desc=\"Scanning external posts\", unit=\"post\"):\n",
    "    try:\n",
    "        data = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "        post = (data or {}).get(\"raw_post\") or {}\n",
    "        pid  = post.get(\"id\") or fp.stem  # fallback to filename if needed\n",
    "\n",
    "        ext_url = extract_external_url(data)\n",
    "        if not ext_url:\n",
    "            # Still record that this external-typed file has no resolvable URL\n",
    "            external_rows.append({\"id\": pid, \"link\": \"\", \"domain\": \"\"})\n",
    "            continue\n",
    "\n",
    "        dom = _domain(ext_url)\n",
    "        external_rows.append({\"id\": pid, \"link\": ext_url, \"domain\": dom})\n",
    "\n",
    "        # Redgifs download\n",
    "        if \"redgifs.com\" in dom or dom.endswith(\".redgifs.com\"):\n",
    "            gid = redgifs_id_from_url(ext_url)\n",
    "            if not gid:\n",
    "                # Sometimes the external URL is a redirect page; skip but log\n",
    "                redgifs_failed.append({\"id\": pid, \"link\": ext_url, \"reason\": \"no_id_from_url\"})\n",
    "                continue\n",
    "\n",
    "            out_path = REDGIFS_OUT / f\"{pid}.mp4\"\n",
    "            if out_path.exists():\n",
    "                # already downloaded\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                mp4_url = redgifs_mp4_url(gid)\n",
    "                if not mp4_url:\n",
    "                    redgifs_failed.append({\"id\": pid, \"link\": ext_url, \"reason\": \"no_mp4_url\"})\n",
    "                    continue\n",
    "                download_stream(mp4_url, out_path)\n",
    "                # Show success line\n",
    "                print(f\"[REDGIFS] id={pid} -> {out_path.name}\")\n",
    "            except Exception as e:\n",
    "                redgifs_failed.append({\"id\": pid, \"link\": ext_url, \"reason\": str(e)})\n",
    "    except Exception as e:\n",
    "        # If we cannot read this JSON at all, log as a redgifs failure only if it looked like redgifs\n",
    "        redgifs_failed.append({\"id\": fp.stem, \"link\": \"\", \"reason\": f\"read_error: {e}\"})\n",
    "\n",
    "# ---- 4) Write summary CSVs ----\n",
    "ext_csv = BASE_OUT / \"external_links.csv\"\n",
    "with ext_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"id\", \"link\", \"domain\"])\n",
    "    w.writeheader()\n",
    "    w.writerows(external_rows)\n",
    "\n",
    "if redgifs_failed:\n",
    "    fail_csv = BASE_OUT / \"redgifs_failed.csv\"\n",
    "    with fail_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"id\", \"link\", \"reason\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(redgifs_failed)\n",
    "    print(f\"\\nSaved Redgifs download failures to: {fail_csv.resolve()}\")\n",
    "\n",
    "print(f\"\\nSaved external links to: {ext_csv.resolve()}\")\n",
    "print(f\"Redgifs saved (if any) to: {REDGIFS_OUT.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938da43",
   "metadata": {},
   "source": [
    "# MEDIA DOWNLOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb94da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Download embedded Reddit-hosted media for posts in out/media/*.json ===\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import html\n",
    "import mimetypes\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs): return x\n",
    "\n",
    "BASE_OUT = Path(OUT_DIR)\n",
    "MEDIA_JSON_DIR = BASE_OUT / \"media\"\n",
    "MEDIA_OUT_DIR = BASE_OUT / \"media_files\"\n",
    "MEDIA_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\"User-Agent\": \"reddit-media-downloader/1.0\"})\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _clean_url(u: str | None) -> str | None:\n",
    "    if not u:\n",
    "        return None\n",
    "    # Reddit often returns HTML-escaped URLs inside JSON\n",
    "    return html.unescape(u)\n",
    "\n",
    "def _domain(u: str | None) -> str:\n",
    "    if not u:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return urlparse(u).netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _ext_from_url_or_type(url: str | None, content_type: str | None) -> str:\n",
    "    # Prefer extension from URL, else derive from content-type\n",
    "    if url:\n",
    "        path = urlparse(url).path\n",
    "        ext = os.path.splitext(path)[1].lower()\n",
    "        if ext in {\".jpg\", \".jpeg\", \".png\", \".gif\", \".mp4\", \".webm\"}:\n",
    "            return ext\n",
    "    if content_type:\n",
    "        ext = mimetypes.guess_extension(content_type.split(\";\")[0].strip())\n",
    "        if ext:\n",
    "            # normalize jpeg\n",
    "            return \".jpg\" if ext == \".jpe\" else ext\n",
    "    # sensible default fallback\n",
    "    return \".mp4\" if (url and \".mp4\" in url) else \".jpg\"\n",
    "\n",
    "def _stream_download(url: str, dest: Path, *, max_retries: int = 4, chunk=1024 * 256):\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with SESSION.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                ctype = r.headers.get(\"Content-Type\")\n",
    "                # if dest has no extension yet, refine using content-type\n",
    "                if dest.suffix == \"\" and ctype:\n",
    "                    dest = dest.with_suffix(_ext_from_url_or_type(url, ctype))\n",
    "                with open(dest, \"wb\") as f:\n",
    "                    for part in r.iter_content(chunk_size=chunk):\n",
    "                        if part:\n",
    "                            f.write(part)\n",
    "            return dest  # final path (may include refined suffix)\n",
    "        except Exception:\n",
    "            if attempt + 1 >= max_retries:\n",
    "                raise\n",
    "            time.sleep(min(2 ** attempt, 15))\n",
    "\n",
    "def _pick_best_preview(post: dict) -> str | None:\n",
    "    \"\"\"\n",
    "    For image/GIF-like posts where 'preview' exists.\n",
    "    Prefer MP4 variant (smaller, plays everywhere), else best image 'source'.\n",
    "    \"\"\"\n",
    "    prev = post.get(\"preview\") or {}\n",
    "    variants = prev.get(\"variants\") or {}\n",
    "    # mp4 variant for gifs, etc.\n",
    "    mp4v = variants.get(\"mp4\") or variants.get(\"reddit_video_preview\")\n",
    "    if mp4v and mp4v.get(\"source\", {}).get(\"url\"):\n",
    "        return _clean_url(mp4v[\"source\"][\"url\"])\n",
    "    # fallback to the image source\n",
    "    src = (prev.get(\"images\") or [{}])[0].get(\"source\", {})\n",
    "    if src.get(\"url\"):\n",
    "        return _clean_url(src[\"url\"])\n",
    "    return None\n",
    "\n",
    "def _pick_vreddit_urls(post: dict) -> tuple[str | None, str | None]:\n",
    "    \"\"\"\n",
    "    v.redd.it posts: return (preferred_mp4_url, fallback_mp4_url)\n",
    "    Try in order: 'hls_url' (m3u8) -> 'fallback_url' (progressive) -> preview mp4.\n",
    "    We only directly download MP4 (no ffmpeg merge here), so we prefer fallback_url,\n",
    "    and otherwise try preview mp4.\n",
    "    \"\"\"\n",
    "    media = post.get(\"media\") or {}\n",
    "    rv = media.get(\"reddit_video\") or {}\n",
    "    fallback = rv.get(\"fallback_url\")  # often progressive mp4 (may be muted on long vids)\n",
    "    hls = rv.get(\"hls_url\")            # m3u8 playlist (would require ffmpeg)\n",
    "    # If no fallback, sometimes preview.mp4 exists:\n",
    "    prev_mp4 = None\n",
    "    prev = post.get(\"preview\") or {}\n",
    "    pv = prev.get(\"reddit_video_preview\") or {}\n",
    "    if isinstance(pv, dict) and pv.get(\"fallback_url\"):\n",
    "        prev_mp4 = pv[\"fallback_url\"]\n",
    "    return (_clean_url(fallback), _clean_url(prev_mp4 or hls))\n",
    "\n",
    "def _gallery_items(post: dict) -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    For gallery posts: return list of (url, suggested_ext).\n",
    "    Uses media_metadata to select best 's' rendition.\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    meta = post.get(\"media_metadata\") or {}\n",
    "    gdata = post.get(\"gallery_data\") or {}\n",
    "    order = [e.get(\"media_id\") for e in gdata.get(\"items\", []) if e.get(\"media_id\")]\n",
    "    for mid in order:\n",
    "        m = meta.get(mid) or {}\n",
    "        s = m.get(\"s\") or {}\n",
    "        url = _clean_url(s.get(\"mp4\") or s.get(\"gif\") or s.get(\"u\") or s.get(\"url\"))\n",
    "        if not url:\n",
    "            continue\n",
    "        # guess extension: mp4 preferred over gif over image\n",
    "        if \"mp4\" in s:\n",
    "            ext = \".mp4\"\n",
    "        elif \"gif\" in s:\n",
    "            ext = \".mp4\"  # we'll still download the gif URL, but use .mp4 if it's actually mp4\n",
    "        else:\n",
    "            # look at mime if present\n",
    "            m_type = m.get(\"m\")\n",
    "            ext = _ext_from_url_or_type(url, m_type)\n",
    "        items.append((url, ext))\n",
    "    return items\n",
    "\n",
    "# ---------- main walk ----------\n",
    "media_jsons = sorted(MEDIA_JSON_DIR.glob(\"*.json\"))\n",
    "print(f\"Found {len(media_jsons)} media post JSONs in {MEDIA_JSON_DIR}\")\n",
    "\n",
    "fail_rows = []\n",
    "downloaded = 0\n",
    "\n",
    "for fp in tqdm(media_jsons, desc=\"Downloading embedded media\", unit=\"post\"):\n",
    "    try:\n",
    "        data = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "        post = (data or {}).get(\"raw_post\") or {}\n",
    "        pid = post.get(\"id\") or fp.stem\n",
    "\n",
    "        # Prefer Reddit-hosted URL if present\n",
    "        url = _clean_url(post.get(\"url_overridden_by_dest\") or post.get(\"url\"))\n",
    "        dom = _domain(url)\n",
    "\n",
    "        # Case A: gallery\n",
    "        if post.get(\"is_gallery\") or (post.get(\"gallery_data\") and post.get(\"media_metadata\")):\n",
    "            items = _gallery_items(post)\n",
    "            if not items:\n",
    "                fail_rows.append({\"id\": pid, \"reason\": \"gallery_no_items\"})\n",
    "                continue\n",
    "            for idx, (item_url, ext) in enumerate(items, start=1):\n",
    "                outfile = MEDIA_OUT_DIR / f\"{pid}_g{idx:02d}{ext if ext.startswith('.') else ('.' + ext)}\"\n",
    "                if outfile.exists():\n",
    "                    continue\n",
    "                try:\n",
    "                    _stream_download(item_url, outfile)\n",
    "                    downloaded += 1\n",
    "                    print(f\"[GAL] {pid} -> {outfile.name}\")\n",
    "                except Exception as e:\n",
    "                    fail_rows.append({\"id\": pid, \"reason\": f\"gallery_item_fail:{e}\"})\n",
    "\n",
    "            continue  # next post\n",
    "\n",
    "        # Case B: native video (v.redd.it)\n",
    "        if (post.get(\"is_video\") or (post.get(\"media\") or {}).get(\"reddit_video\")) and dom.endswith(\"v.redd.it\"):\n",
    "            main_mp4, alt_mp4 = _pick_vreddit_urls(post)\n",
    "            target = MEDIA_OUT_DIR / f\"{pid}.mp4\"\n",
    "            if target.exists():\n",
    "                continue\n",
    "            src = main_mp4 or alt_mp4\n",
    "            if not src:\n",
    "                # last chance: look into preview variants\n",
    "                src = _pick_best_preview(post)\n",
    "            if not src:\n",
    "                fail_rows.append({\"id\": pid, \"reason\": \"vreddit_no_source\"})\n",
    "                continue\n",
    "            try:\n",
    "                _stream_download(src, target)\n",
    "                downloaded += 1\n",
    "                print(f\"[VID] {pid} -> {target.name}\")\n",
    "            except Exception as e:\n",
    "                fail_rows.append({\"id\": pid, \"reason\": f\"vreddit_dl_fail:{e}\"})\n",
    "            continue\n",
    "\n",
    "        # Case C: image / gif via i.redd.it or preview\n",
    "        if dom.endswith(\"i.redd.it\"):\n",
    "            # Direct i.redd.it link\n",
    "            ext = _ext_from_url_or_type(url, None)\n",
    "            target = MEDIA_OUT_DIR / f\"{pid}{ext}\"\n",
    "            if not target.exists():\n",
    "                try:\n",
    "                    _stream_download(url, target)\n",
    "                    downloaded += 1\n",
    "                    print(f\"[IMG] {pid} -> {target.name}\")\n",
    "                except Exception as e:\n",
    "                    fail_rows.append({\"id\": pid, \"reason\": f\"ireddit_dl_fail:{e}\"})\n",
    "            continue\n",
    "\n",
    "        # Fallback: try preview (covers some GIF-to-MP4 conversions)\n",
    "        prev_url = _pick_best_preview(post)\n",
    "        if prev_url and _domain(prev_url) in {\"i.redd.it\", \"v.redd.it\", \"preview.redd.it\"}:\n",
    "            ext = _ext_from_url_or_type(prev_url, None)\n",
    "            target = MEDIA_OUT_DIR / f\"{pid}{ext}\"\n",
    "            if not target.exists():\n",
    "                try:\n",
    "                    _stream_download(prev_url, target)\n",
    "                    downloaded += 1\n",
    "                    print(f\"[PREV] {pid} -> {target.name}\")\n",
    "                except Exception as e:\n",
    "                    fail_rows.append({\"id\": pid, \"reason\": f\"preview_dl_fail:{e}\"})\n",
    "            continue\n",
    "\n",
    "        # If we reach here, it looks like a Reddit-hosted \"media\" without a reliable direct URL\n",
    "        fail_rows.append({\"id\": pid, \"reason\": \"no_reddit_media_url\"})\n",
    "    except Exception as e:\n",
    "        fail_rows.append({\"id\": fp.stem, \"reason\": f\"read_error:{e}\"})\n",
    "\n",
    "# ---------- write failures ----------\n",
    "if fail_rows:\n",
    "    fail_csv = BASE_OUT / \"media_failed.csv\"\n",
    "    with fail_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"id\", \"reason\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(fail_rows)\n",
    "    print(f\"\\nSaved media failures to: {fail_csv.resolve()}\")\n",
    "\n",
    "print(f\"\\nDone. Downloaded: {downloaded}. Files saved under: {MEDIA_OUT_DIR.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Primary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
