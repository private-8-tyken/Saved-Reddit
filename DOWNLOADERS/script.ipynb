{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4869123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reddit script app credentials (temporary hardcode ok for local use) ---\n",
    "REDDIT_CLIENT_ID = \"Ik1IhrLMkUe2Y7_jLqj-Ew\"\n",
    "REDDIT_CLIENT_SECRET = \"1j81ffxuNl-e8EzPV4D3OzCVCH-1lw\"\n",
    "REDDIT_USERNAME = \"Grand_Admiral_Tyken\"\n",
    "REDDIT_PASSWORD = \"X5bugNC9j3Bc^Uf\"\n",
    "\n",
    "\n",
    "# --- IO config ---\n",
    "CSV_PATH = \"links.csv\"   # one Reddit URL per line, no header\n",
    "OUT_DIR  = \"out\"         # base folder for outputs\n",
    "\n",
    "# --- polite request pacing ---\n",
    "REQUEST_DELAY_SEC = 0.5  # delay between requests to Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "deda5f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "\n",
    "# Session + headers\n",
    "SESSION = requests.Session()\n",
    "UA = f\"reddit-json-downloader-jupyter/1.0 (by u/{REDDIT_USERNAME})\"\n",
    "\n",
    "# OAuth endpoints and params\n",
    "OAUTH_TOKEN_URL = \"https://www.reddit.com/api/v1/access_token\"\n",
    "OAUTH_API_BASE  = \"https://oauth.reddit.com\"\n",
    "COMMENTS_QUERY  = \"raw_json=1&limit=500&depth=10&showmore=true\"  # fuller comment payload\n",
    "\n",
    "# URL parsing helpers\n",
    "COMMENTS_ID_RE = re.compile(r\"/comments/([a-z0-9]{5,8})\", re.IGNORECASE)\n",
    "SHORTLINK_RE   = re.compile(r\"redd\\.it/([a-z0-9]{5,8})\", re.IGNORECASE)\n",
    "SUB_RE         = re.compile(r\"/r/([^/]+)/comments/\", re.IGNORECASE)\n",
    "\n",
    "def request_with_backoff(method: str, url: str, *, headers=None, data=None, timeout=60, max_retries=5):\n",
    "    for attempt in range(max_retries):\n",
    "        r = SESSION.request(method, url, headers=headers, data=data, timeout=timeout)\n",
    "        # Success\n",
    "        if r.status_code < 400:\n",
    "            return r\n",
    "        # Quarantine, rate limit, or transient failures\n",
    "        if r.status_code in (429,) or 500 <= r.status_code < 600:\n",
    "            time.sleep(min(2 ** attempt, 30))\n",
    "            continue\n",
    "        return r  # hard error\n",
    "    return r\n",
    "\n",
    "def get_token() -> str:\n",
    "    auth = requests.auth.HTTPBasicAuth(REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET)\n",
    "    data = {\"grant_type\": \"password\", \"username\": REDDIT_USERNAME, \"password\": REDDIT_PASSWORD}\n",
    "    headers = {\"User-Agent\": UA}\n",
    "    r = requests.post(OAUTH_TOKEN_URL, auth=auth, data=data, headers=headers, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    tok = r.json().get(\"access_token\")\n",
    "    if not tok:\n",
    "        raise RuntimeError(f\"OAuth token missing; resp={r.text}\")\n",
    "    return tok\n",
    "\n",
    "def oauth_headers() -> dict:\n",
    "    tok = getattr(SESSION, \"_oauth_token\", None)\n",
    "    if not tok:\n",
    "        tok = get_token()\n",
    "        SESSION._oauth_token = tok\n",
    "    return {\"Authorization\": f\"bearer {tok}\", \"User-Agent\": UA}\n",
    "\n",
    "def accept_quarantine(subreddit: str) -> bool:\n",
    "    if not subreddit:\n",
    "        return False\n",
    "    url = f\"{OAUTH_API_BASE}/api/accept_quarantine\"\n",
    "    r = request_with_backoff(\"POST\", url, headers=oauth_headers(), max_retries=3)\n",
    "    return r.status_code in (200, 204, 409)  # 409 ~ already accepted\n",
    "\n",
    "def parse_link(link: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    m = COMMENTS_ID_RE.search(link)\n",
    "    if m:\n",
    "        post_id = m.group(1)\n",
    "    else:\n",
    "        m2 = SHORTLINK_RE.search(link)\n",
    "        post_id = m2.group(1) if m2 else None\n",
    "    m_sr = SUB_RE.search(link)\n",
    "    subreddit = m_sr.group(1) if m_sr else None\n",
    "    return post_id, subreddit\n",
    "\n",
    "def normalize_comments_url(link: str, fallback_post_id: Optional[str]) -> str:\n",
    "    p = urlparse(link)\n",
    "    path = p.path or \"\"\n",
    "    host = (p.netloc or \"\").lower()\n",
    "    if \"redd.it\" in host or \"/comments/\" not in path:\n",
    "        if fallback_post_id:\n",
    "            path = f\"/comments/{fallback_post_id}/\"\n",
    "    if not path.endswith(\"/\"):\n",
    "        path += \"/\"\n",
    "    return f\"{OAUTH_API_BASE}{path}.json?{COMMENTS_QUERY}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8457222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_more_ids(listing_node) -> List[str]:\n",
    "    ids = []\n",
    "    def walk(node):\n",
    "        if not isinstance(node, dict): return\n",
    "        kind = node.get(\"kind\"); data = node.get(\"data\", {})\n",
    "        if kind == \"more\":\n",
    "            children = data.get(\"children\") or []\n",
    "            ids.extend([c for c in children if c])\n",
    "        elif kind in (\"Listing\", \"t1\"):\n",
    "            for ch in data.get(\"children\", []):\n",
    "                walk(ch)\n",
    "            if kind == \"t1\" and isinstance(data.get(\"replies\"), dict):\n",
    "                walk(data[\"replies\"])\n",
    "    walk(listing_node)\n",
    "    return ids\n",
    "\n",
    "def index_comments_by_id(listing_node) -> Dict[str, dict]:\n",
    "    idx = {}\n",
    "    def walk(node):\n",
    "        if not isinstance(node, dict): return\n",
    "        kind = node.get(\"kind\"); data = node.get(\"data\", {})\n",
    "        if kind == \"t1\":\n",
    "            cid = (data.get(\"id\") or \"\").lower()\n",
    "            if cid: idx[cid] = node\n",
    "            if isinstance(data.get(\"replies\"), dict):\n",
    "                walk(data[\"replies\"])\n",
    "        elif kind == \"Listing\":\n",
    "            for ch in data.get(\"children\", []):\n",
    "                walk(ch)\n",
    "    walk(listing_node)\n",
    "    return idx\n",
    "\n",
    "def replace_more_with_children(root_listing: dict, parent_lookup: Dict[str, dict], chunk_result: dict):\n",
    "    listing = chunk_result.get(\"json\", {}).get(\"data\", {}).get(\"things\", [])\n",
    "    for thing in listing:\n",
    "        if thing.get(\"kind\") != \"t1\":\n",
    "            continue\n",
    "        data = thing.get(\"data\", {})\n",
    "        pid = data.get(\"parent_id\", \"\")\n",
    "        if pid.startswith(\"t1_\"):\n",
    "            parent_id = pid[3:].lower()\n",
    "            parent = parent_lookup.get(parent_id)\n",
    "            if parent:\n",
    "                if not isinstance(parent[\"data\"].get(\"replies\"), dict):\n",
    "                    parent[\"data\"][\"replies\"] = {\"kind\": \"Listing\", \"data\": {\"children\": []}}\n",
    "                parent[\"data\"][\"replies\"][\"data\"][\"children\"].append(thing)\n",
    "        elif pid.startswith(\"t3_\"):\n",
    "            root_listing[\"data\"][\"children\"].append(thing)\n",
    "\n",
    "def strip_more_nodes(node):\n",
    "    if not isinstance(node, dict): return\n",
    "    kind = node.get(\"kind\"); data = node.get(\"data\", {})\n",
    "    if kind == \"Listing\":\n",
    "        new_children = []\n",
    "        for ch in data.get(\"children\", []):\n",
    "            if ch.get(\"kind\") == \"more\":\n",
    "                continue\n",
    "            new_children.append(ch)\n",
    "        data[\"children\"] = new_children\n",
    "        for ch in new_children:\n",
    "            strip_more_nodes(ch)\n",
    "    if kind == \"t1\" and isinstance(data.get(\"replies\"), dict):\n",
    "        strip_more_nodes(data[\"replies\"])\n",
    "\n",
    "def fetch_full_post_and_comments(link: str):\n",
    "    \"\"\"Return (post_obj, full_comments_listing, raw_array) with all 'more' expanded.\"\"\"\n",
    "    if not link.startswith((\"http://\", \"https://\")):\n",
    "        raise ValueError(f\"Not a URL: {link}\")\n",
    "\n",
    "    post_id, sr_hint = parse_link(link)\n",
    "    comments_url = normalize_comments_url(link, post_id)\n",
    "\n",
    "    r = request_with_backoff(\"GET\", comments_url, headers=oauth_headers(), timeout=60)\n",
    "    if r.status_code == 403:\n",
    "        # try quarantine once\n",
    "        sr = sr_hint\n",
    "        if not sr:\n",
    "            m = re.search(r\"/r/([^/]+)/comments/\", comments_url)\n",
    "            sr = m.group(1) if m else \"\"\n",
    "        if sr and accept_quarantine(sr):\n",
    "            r = request_with_backoff(\"GET\", comments_url, headers=oauth_headers(), timeout=60)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    if not (isinstance(data, list) and len(data) >= 2):\n",
    "        raise RuntimeError(\"Unexpected Reddit JSON format\")\n",
    "\n",
    "    post_listing = data[0][\"data\"][\"children\"]\n",
    "    if not post_listing:\n",
    "        raise RuntimeError(\"Post listing empty\")\n",
    "    post = post_listing[0][\"data\"]\n",
    "    subreddit = post.get(\"subreddit\") or sr_hint or \"\"\n",
    "    link_id = post.get(\"id\")  # base36\n",
    "    comments_listing = data[1]\n",
    "\n",
    "    # resolve all \"more\"\n",
    "    if link_id:\n",
    "        while True:\n",
    "            more_ids = collect_more_ids(comments_listing)\n",
    "            if not more_ids:\n",
    "                break\n",
    "            for i in range(0, len(more_ids), 100):\n",
    "                chunk = more_ids[i:i+100]\n",
    "                form = {\n",
    "                    \"link_id\": f\"t3_{link_id}\",\n",
    "                    \"api_type\": \"json\",\n",
    "                    \"children\": \",\".join(chunk),\n",
    "                    \"sort\": \"confidence\",\n",
    "                    \"limit_children\": False,\n",
    "                    \"raw_json\": 1,\n",
    "                }\n",
    "                url = f\"{OAUTH_API_BASE}/api/morechildren\"\n",
    "                r2 = request_with_backoff(\"POST\", url, headers=oauth_headers(), data=form, timeout=60)\n",
    "                if r2.status_code == 403 and subreddit and accept_quarantine(subreddit):\n",
    "                    r2 = request_with_backoff(\"POST\", url, headers=oauth_headers(), data=form, timeout=60)\n",
    "                r2.raise_for_status()\n",
    "                payload = r2.json()\n",
    "                parent_idx = index_comments_by_id(comments_listing)\n",
    "                replace_more_with_children(comments_listing, parent_idx, payload)\n",
    "            # clean 'more' nodes consumed this pass\n",
    "            strip_more_nodes(comments_listing)\n",
    "\n",
    "    return post, comments_listing, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "381b7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Classification: external takes precedence over \"media preview\" ----\n",
    "INTERNAL_REDDIT_HOSTS = {\n",
    "    # Reddit-owned domains (internal)\n",
    "    \"reddit.com\", \"www.reddit.com\", \"old.reddit.com\", \"np.reddit.com\", \"oauth.reddit.com\",\n",
    "    \"redd.it\",\n",
    "    # NOTE: we intentionally do NOT put i.redd.it or v.redd.it here, because those\n",
    "    # are \"native media\" and should be treated as media, not \"internal text\".\n",
    "}\n",
    "\n",
    "# Native, Reddit-hosted media (treat as media, not external)\n",
    "NATIVE_MEDIA_HOSTS = {\"i.redd.it\", \"v.redd.it\"}\n",
    "\n",
    "def domain_of(url: Optional[str]) -> str:\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return urlparse(url).netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def classify_post(post_data: dict) -> str:\n",
    "    \"\"\"\n",
    "    Decide among: 'external', 'media', 'text'\n",
    "\n",
    "    Priority (revised):\n",
    "      1) external  -> if it links off-Reddit (e.g., redgifs.com), even if a preview exists\n",
    "      2) media     -> native Reddit media (i.redd.it images, v.redd.it videos), galleries, etc.\n",
    "      3) text      -> self-posts without media\n",
    "    \"\"\"\n",
    "    is_self = bool(post_data.get(\"is_self\"))\n",
    "\n",
    "    # Prefer url_overridden_by_dest (if present), else url\n",
    "    url = post_data.get(\"url_overridden_by_dest\") or post_data.get(\"url\")\n",
    "    d = domain_of(url) or (post_data.get(\"domain\") or \"\").lower()\n",
    "\n",
    "    # 1) External: outbound, non-Reddit, non-native-media hosts\n",
    "    if not is_self and d and (d not in INTERNAL_REDDIT_HOSTS) and (d not in NATIVE_MEDIA_HOSTS):\n",
    "        return \"external\"\n",
    "\n",
    "    # 2) Media: Reddit-native media, previews, galleries, video flags\n",
    "    post_hint   = (post_data.get(\"post_hint\") or \"\").lower()\n",
    "    has_gallery = bool(post_data.get(\"gallery_data\"))\n",
    "    has_preview = bool(post_data.get(\"preview\"))\n",
    "    has_media   = bool(post_data.get(\"media\")) or bool(post_data.get(\"is_video\"))\n",
    "\n",
    "    is_native_media_host = d in NATIVE_MEDIA_HOSTS\n",
    "    is_media_hint = post_hint in {\"image\", \"hosted:video\", \"rich:video\"}\n",
    "    if is_native_media_host or has_media or has_preview or has_gallery or is_media_hint:\n",
    "        return \"media\"\n",
    "\n",
    "    # 3) Text\n",
    "    return \"text\"\n",
    "\n",
    "def extract_comments_full(listing_node):\n",
    "    \"\"\"\n",
    "    Produce a nested list of comments with replies (no depth/limit).\n",
    "    \"\"\"\n",
    "    def convert(node):\n",
    "        if not isinstance(node, dict): return None\n",
    "        kind = node.get(\"kind\"); data = node.get(\"data\", {})\n",
    "        if kind == \"t1\":\n",
    "            item = {\n",
    "                \"id\": data.get(\"id\"),\n",
    "                \"author\": data.get(\"author\"),\n",
    "                \"author_fullname\": data.get(\"author_fullname\"),\n",
    "                \"body\": data.get(\"body\"),\n",
    "                \"body_html\": data.get(\"body_html\"),\n",
    "                \"score\": data.get(\"score\"),\n",
    "                \"created_utc\": data.get(\"created_utc\"),\n",
    "                \"permalink\": \"https://www.reddit.com\" + data.get(\"permalink\", \"\"),\n",
    "                \"is_submitter\": data.get(\"is_submitter\"),\n",
    "                \"parent_id\": data.get(\"parent_id\"),\n",
    "                \"replies\": []\n",
    "            }\n",
    "            replies = data.get(\"replies\")\n",
    "            if isinstance(replies, dict):\n",
    "                children = replies.get(\"data\", {}).get(\"children\", [])\n",
    "                for ch in children:\n",
    "                    child = convert(ch)\n",
    "                    if child:\n",
    "                        item[\"replies\"].append(child)\n",
    "            return item\n",
    "        elif kind == \"Listing\":\n",
    "            out = []\n",
    "            for ch in data.get(\"children\", []):\n",
    "                c = convert(ch)\n",
    "                if c:\n",
    "                    out.append(c)\n",
    "            return out\n",
    "        return None\n",
    "\n",
    "    res = convert(listing_node)\n",
    "    return res if isinstance(res, list) else (res or [])\n",
    "\n",
    "def make_archive_object(post: dict, comments_listing: dict):\n",
    "    \"\"\"\n",
    "    Stable archive object:\n",
    "      - 'raw_post'     : submission object (full)\n",
    "      - 'raw_comments' : full comment Listing (expanded, no 'more')\n",
    "      - 'comments'     : nested list of comments with replies\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"raw_post\": post,\n",
    "        \"raw_comments\": comments_listing,\n",
    "        \"comments\": extract_comments_full(comments_listing),\n",
    "    }\n",
    "\n",
    "def save_archive(doc: dict, base_out: Path, post_id: str, category: str):\n",
    "    target = base_out / category\n",
    "    target.mkdir(parents=True, exist_ok=True)\n",
    "    (target / f\"{post_id}.json\").write_text(\n",
    "        json.dumps(doc, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f89f87a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fcb23054ba84e4a94bb5ae880b7c5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Archiving posts:   0%|          | 0/791 [00:00<?, ?post/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] id=y696cb | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/y696cb/i_love_letting_guys_who_lack_confidence_pound_my\n",
      "[OK] id=yf5fyr | folder=text | link=https://www.reddit.com/r/sexstories/comments/yf5fyr/fun_at_a_swingers_house_party\n",
      "[OK] id=ydqkgu | folder=text | link=https://www.reddit.com/r/sexstories/comments/ydqkgu/my_roommates_and_i_had_sex_with_the_boys_from\n",
      "[OK] id=y9sqye | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/y9sqye/f19_how_i_found_out_my_bf_was_big\n",
      "[OK] id=y8qvtg | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/y8qvtg/my_f_friends_girlfriend_cheated_on_him_with_his\n",
      "[OK] id=1mnxp43 | folder=media | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1mnxp43/i_dont_know_why_you_such_a_big_deal_about_it_its\n",
      "[OK] id=ya5sk7 | folder=external | link=https://www.reddit.com/r/RealGirls/comments/ya5sk7/your_face_should_be_between_my_thighs_right_now\n",
      "[OK] id=ya2yvv | folder=external | link=https://www.reddit.com/r/bdsm/comments/ya2yvv/sex_and_submission_of_penny_pax\n",
      "[OK] id=y8fgv7 | folder=text | link=https://www.reddit.com/r/sexstories/comments/y8fgv7/husband_tied_me_to_the_bed_and_blindfolded_and\n",
      "[OK] id=y4nwl9 | folder=text | link=https://www.reddit.com/r/DirtyConfession/comments/y4nwl9/19f_my_new_kink_goes_against_everything_i_stand\n",
      "[OK] id=y6r08d | folder=text | link=https://www.reddit.com/r/sexstories/comments/y6r08d/f_18_college_turned_me_into_a_total_fucktoy\n",
      "[OK] id=y6lbeq | folder=text | link=https://www.reddit.com/r/sexstories/comments/y6lbeq/19f_got_dared_to_give_up_my_pussy_on_halloween\n",
      "[OK] id=y34u8o | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/y34u8o/my_bf_brutalized_my_pussy_after_a_fight_vanilla\n",
      "[OK] id=y3mx34 | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/y3mx34/18f_finally_got_my_bfs_dad_to_play_with_me\n",
      "[OK] id=y0murt | folder=text | link=https://www.reddit.com/r/sexstories/comments/y0murt/american_airlines_flight_953\n",
      "[OK] id=xx5a1y | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/xx5a1y/part_2_19f_wear_short_skirts_to_concerts_its_wayy\n",
      "[OK] id=xwbeo2 | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/xwbeo2/19f_wear_short_skirts_to_concerts_its_wayy_more\n",
      "[OK] id=xsn4mb | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/xsn4mb/my_bf_made_me_a_cum_addict\n",
      "[OK] id=xpv8uq | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/xpv8uq/f26ound_a_gloryhole_at_a_bar\n",
      "[OK] id=xqn0t1 | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/xqn0t1/this_confession_will_make_you_hard_18f\n",
      "[OK] id=xn7p1m | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/xn7p1m/i_got_both_my_best_friend_and_her_girlfriend\n",
      "[OK] id=xq8o2h | folder=external | link=https://www.reddit.com/r/bdsm/comments/xq8o2h/locked_and_licked\n",
      "[OK] id=xi6kl8 | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/xi6kl8/f19_woke_up_to_my_fwbs_cock_in_my_pussy\n",
      "[OK] id=xecx1x | folder=media | link=https://www.reddit.com/r/bdsm/comments/xecx1x/slapped_and_ready_to_be_used\n",
      "[OK] id=x4zlvh | folder=text | link=https://www.reddit.com/r/sexstories/comments/x4zlvh/a_bet_with_sister_ends_up_with_her_becoming_my\n",
      "[OK] id=x53vr6 | folder=external | link=https://www.reddit.com/r/bdsm/comments/x53vr6/my_new_leather_harness_wearing_nipple_clips\n",
      "[OK] id=x25gf7 | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/x25gf7/went_to_a_party_with_the_word_slut_written_on_my\n",
      "[OK] id=isnvp8 | folder=text | link=https://www.reddit.com/r/videography/comments/isnvp8/recording_skill_and_camera_for_pov_amateur_porn\n",
      "[OK] id=wv7xyg | folder=media | link=https://www.reddit.com/r/bdsm/comments/wv7xyg/masters_view_when_he_comes_home\n",
      "[OK] id=asmtln | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/asmtln/the_ups_and_downs_of_being_a_college_voyeur_f\n",
      "[OK] id=wh5fbn | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/wh5fbn/im_22f_not_wearing_anything_under_my_skirt_right\n",
      "[OK] id=wfj695 | folder=text | link=https://www.reddit.com/r/sexstories/comments/wfj695/free_fuck\n",
      "[OK] id=wflpur | folder=text | link=https://www.reddit.com/r/sexstories/comments/wflpur/first_time_with_a_hotwife_in_thailand_long\n",
      "[OK] id=vzj9yn | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/vzj9yn/my_boyfriend_is_away_on_a_business_trip_and_my_ex\n",
      "[OK] id=wbg7tl | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/wbg7tl/22f_i_honestly_feel_so_proud_of_myself\n",
      "[OK] id=wao1jb | folder=text | link=https://www.reddit.com/r/gonewildstories/comments/wao1jb/my_18_then_pussy_was_rubbed_on_the_dancefloor\n",
      "[OK] id=w5v5cy | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/w5v5cy/23f_i_let_my_husband_35m_fuck_me_in_the_ass_to\n",
      "[OK] id=w9kp3n | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/w9kp3n/after_26_years_of_being_the_sweet_naive_farm_girl\n",
      "[OK] id=w9namq | folder=text | link=https://www.reddit.com/r/sexstories/comments/w9namq/i_fucked_2_of_my_wifes_friends_at_once\n",
      "[OK] id=w9kf76 | folder=external | link=https://www.reddit.com/r/bdsm/comments/w9kf76/helpless_orgasm\n",
      "[OK] id=w79gbn | folder=text | link=https://www.reddit.com/r/sexstories/comments/w79gbn/i_let_my_bf_cum_inside_me_as_frequently_as\n",
      "[OK] id=vidgz2 | folder=text | link=https://www.reddit.com/r/sexstories/comments/vidgz2/i_went_to_a_free_use_party_35f\n",
      "[OK] id=w4ye2e | folder=text | link=https://www.reddit.com/r/sexstories/comments/w4ye2e/i_used_to_force_myself_to_have_sex_with_my\n",
      "[OK] id=w2rh93 | folder=external | link=https://www.reddit.com/r/bdsm/comments/w2rh93/am_i_a_good_fucktoy\n",
      "[OK] id=w0famf | folder=text | link=https://www.reddit.com/r/sexstories/comments/w0famf/what_he_does_to_me\n",
      "[OK] id=vvwnri | folder=external | link=https://www.reddit.com/r/cumsluts/comments/vvwnri/hermione_granger_working_hard_to_drain_him\n",
      "[OK] id=vt72ub | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/vt72ub/i_left_my_door_unlocked_and_i_sat_naked_with_my\n",
      "[OK] id=vsnh9b | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/vsnh9b/i_have_a_freeuse_deal_with_my_husband_but_lately\n",
      "[OK] id=vqvax9 | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/vqvax9/i_19f_told_my_hookup_he_could_do_whatever_he\n",
      "[OK] id=vmo8zr | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/vmo8zr/21f_i_ended_up_deepthroating_my_bfs_best_friend\n",
      "[OK] id=jqx3di | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/jqx3di/f29_when_i_was_penetrated_onstage_in_front_of_an\n",
      "[OK] id=vobi30 | folder=text | link=https://www.reddit.com/r/sexstories/comments/vobi30/the_time_i_m32_fucked_a_girl_f30_that_i_met_via_a\n",
      "[OK] id=fvyorw | folder=text | link=https://www.reddit.com/r/gonewildstories/comments/fvyorw/college_hockey_team_gangbang_20_f\n",
      "[OK] id=v48nha | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/v48nha/i_came_inside_my_friend_and_she_doesnt_know_it\n",
      "[OK] id=b3moxu | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/b3moxu/i_30m_cnc_gang_rape_a_girl_25f_all_weekend_long\n",
      "[OK] id=kpsrov | folder=text | link=https://www.reddit.com/r/gonewildaudio/comments/kpsrov/script_fillf4m_help_my_girlfriend_is_a_hypnotist\n",
      "[OK] id=uj1h0d | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/uj1h0d/f18_i_recently_lost_my_virginity_in_the_kinkiest\n",
      "[OK] id=uxozo3 | folder=media | link=https://www.reddit.com/r/bdsm/comments/uxozo3/am_i_a_good_or_bad_cop\n",
      "[OK] id=s4ofzu | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/s4ofzu/the_most_primal_ive_ever_felt_24f\n",
      "[OK] id=ump4bb | folder=text | link=https://www.reddit.com/r/u_TinyArabBaby/comments/ump4bb/my_story\n",
      "[OK] id=ussz23 | folder=text | link=https://www.reddit.com/r/eroticliterature/comments/ussz23/mf_i_think_i_might_have_been_a_little_too_loud\n",
      "[OK] id=tygj0n | folder=external | link=https://www.reddit.com/r/RealGirls/comments/tygj0n/snacks_belong_in_the_kitchen_so_here_i_am\n",
      "[OK] id=o4vo58 | folder=text | link=https://www.reddit.com/r/sexstories/comments/o4vo58/im_flight_attendant_and_i_know_everything_about\n",
      "[OK] id=uowsux | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/uowsux/finally_my_27f_birthday_gangbang_update\n",
      "[OK] id=uoy0gs | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/uoy0gs/i_f23_made_a_mistake_with_my_student_m18\n",
      "[OK] id=uih95e | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/uih95e/i_24f_let_my_boyfriend_25m_drug_me_before_he_had\n",
      "[OK] id=uie0ok | folder=external | link=https://www.reddit.com/r/RealGirls/comments/uie0ok/metal_bikinis_are_actually_very_uncomfortable\n",
      "[OK] id=udo3ts | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/udo3ts/f45_im_a_married_mom_who_drives_for_uber_and_i\n",
      "[OK] id=1a1ybm | folder=text | link=https://www.reddit.com/r/sex/comments/1a1ybm/what_is_the_hottest_thing_a_man_has_done_to_you\n",
      "[OK] id=i7tz25 | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/i7tz25/the_guy_who_bought_me_lingerie_m27_on_my_work\n",
      "[OK] id=n37bom | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/n37bom/my_longdistance_boyfriend_m27_ordered_a_calvin\n",
      "[OK] id=u7rm72 | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/u7rm72/i_f24_was_face_fucked_by_a_coworker_i_dont_get\n",
      "[OK] id=t6ocqk | folder=external | link=https://www.reddit.com/r/cumfetish/comments/t6ocqk/all_face_in_his_cum_and_my_droll\n",
      "[OK] id=u33ob8 | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/u33ob8/strip_poker_with_my_friends_wife\n",
      "[OK] id=om5x4s | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/om5x4s/oh_my_gosh_f19_the_pay_was_good_but_they_beat_the\n",
      "[OK] id=tt567q | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/tt567q/how_i_f18_went_from_innocent_college_virgin_to\n",
      "[OK] id=s8rzht | folder=external | link=https://www.reddit.com/r/cumfetish/comments/s8rzht/all_over_and_then_some\n",
      "[OK] id=tfucs4 | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/tfucs4/how_i_f18_went_from_innocent_college_virgin_to\n",
      "[OK] id=tex5xg | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/tex5xg/how_i_f18_went_from_innocent_college_virgin_to\n",
      "[OK] id=tc0kxn | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/tc0kxn/how_i_f18_went_from_innocent_college_virgin_to\n",
      "[OK] id=tbxv2a | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/tbxv2a/how_i_f18_went_from_innocent_college_virgin_to\n",
      "[OK] id=tfmh7h | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/tfmh7h/i_gave_a_much_younger_coworker_head_in_the\n",
      "[OK] id=nljzbf | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/nljzbf/f19_i_just_agreed_to_be_free_use_for_a_house_of_6\n",
      "[OK] id=n15k5n | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/n15k5n/my_exwife_blows_me_more_now_that_we_are_divorced\n",
      "[OK] id=t4at3m | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/t4at3m/when_i_was_in_high_school_i_f18_got_gangbanged_by\n",
      "[OK] id=slq7tx | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/slq7tx/i_24f_had_an_unprotected_gangbang_last_month\n",
      "[OK] id=tf5sts | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/tf5sts/i_f25_woke_up_to_him_m25_using_me_as_his_comfort\n",
      "[OK] id=i0kmfd | folder=external | link=https://www.reddit.com/r/RealGirls/comments/i0kmfd/quarantine_has_forced_me_to_get_creative\n",
      "[OK] id=nlaj7k | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/nlaj7k/i_25f_turned_my_friend_24f_into_an_absolute_cock\n",
      "[OK] id=fu8o1h | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/fu8o1h/a_guy_20m_from_work_found_my_19f_nsfw_reddit\n",
      "[OK] id=nj1k7v | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/nj1k7v/33f_even_if_a_guy_im_having_sex_with_is_small_i\n",
      "[OK] id=mpca6j | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/mpca6j/as_a_female_kindergarten_teacher_f_23_i_get\n",
      "[OK] id=t45lv0 | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/t45lv0/f_22_deepthroated_at_a_rave\n",
      "[OK] id=tczvf9 | folder=text | link=https://www.reddit.com/r/sexstories/comments/tczvf9/f_35_i_am_a_policewoman_this_is_the_story_about\n",
      "[OK] id=tck7ff | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/tck7ff/my_life_as_a_free_use_girlfriend_did_we_go_too_far\n",
      "[OK] id=oawmq9 | folder=text | link=https://www.reddit.com/r/gonewildstories/comments/oawmq9/i_curtsy_to_the_men_that_ive_fucked_mf\n",
      "[OK] id=ldhs58 | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/ldhs58/25f_i_gave_my_boyfriend_permission_to_order_me\n",
      "[OK] id=krv16d | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/krv16d/25f_boyfriend_won_a_bet_so_i_have_to_service_him\n",
      "[OK] id=t4dsm5 | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/t4dsm5/f25_my_ex_makes_me_suck_him_whenever_i_apologize\n",
      "[OK] id=t7prc8 | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/t7prc8/i_just_fucked_the_thickest_cock_ive_ever_seen\n",
      "[OK] id=szlm3g | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/szlm3g/i_30f_fucked_a_guy_from_tinder_that_my_fwb_picked\n",
      "[OK] id=svjhir | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/svjhir/28f_in_a_relationship_i_got_finger_fucked_in_a\n",
      "[OK] id=svbc4y | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/svbc4y/28_f_in_a_relationship_part_2_of_how_i_became_a\n",
      "[OK] id=sv5dn8 | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/sv5dn8/28_f_in_a_relationship_i_became_a_collateral_fuck\n",
      "[OK] id=swn777 | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/swn777/i_f23_gave_my_brotherfianc%C3%A9_m21_and_girlfriend\n",
      "[OK] id=swsx9p | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/swsx9p/i_fucked_a_stranger_in_the_bathroom_while_on_a\n",
      "[OK] id=swrxra | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/swrxra/i_38f_love_feeling_cum_ooze_out_of_my_pussy\n",
      "[OK] id=swk8fp | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/swk8fp/a_night_of_firsts_my_valentine_gave_me_multiple\n",
      "[OK] id=sw8vcg | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/sw8vcg/joining_bathrooms_in_mountain_cabin_caught_me_out\n",
      "[OK] id=sw6az0 | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/sw6az0/masturbating_while_listening_to_hubby_fuck\n",
      "[OK] id=lbjagu | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/lbjagu/my_19f_boyfriend_21m_has_turned_me_into_his_cum\n",
      "[OK] id=srk9za | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/srk9za/24f_admitted_i_was_free_use_at_a_party\n",
      "[OK] id=sqtcdw | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/sqtcdw/27f_my_fianc%C3%A9_let_one_of_his_friends_use_and\n",
      "[OK] id=sqsove | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/sqsove/i_26f_blew_my_personal_trainer_28m_this_morning\n",
      "[OK] id=lguhr2 | folder=text | link=https://www.reddit.com/r/EroticHypnosis/comments/lguhr2/boyfriend_has_forced_fetish_fetish\n",
      "[OK] id=sk8q7n | folder=text | link=https://www.reddit.com/r/stupidslutsclub/comments/sk8q7n/my_first_orgy\n",
      "[OK] id=3f3vm0 | folder=text | link=https://www.reddit.com/r/gonewildstories/comments/3f3vm0/turning_my_gf_into_a_slut\n",
      "[OK] id=fo2dbw | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/fo2dbw/how_i_turned_my_girlfriend_into_a_slut\n",
      "[OK] id=spycns | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/spycns/i_didnt_give_my_coworker_a_choice\n",
      "[OK] id=sj9l2s | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/sj9l2s/heres_the_story_of_the_hottest_encounter_i_36m\n",
      "[OK] id=sj7zyi | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/sj7zyi/late_30s_group_nj_the_first_time_my_wifes_friend\n",
      "[OK] id=p6kar1 | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/p6kar1/i_25f_fucked_a_groom_on_a_bachelor_party_pt_2\n",
      "[OK] id=p6dlm7 | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/p6dlm7/i_25f_fucked_a_groom_on_his_bachelor_party\n",
      "[OK] id=l7p04z | folder=text | link=https://www.reddit.com/r/dirtypenpals/comments/l7p04z/f4a_an_ancient_and_mythical_lewd_world_that_we\n",
      "[OK] id=slouk0 | folder=text | link=https://www.reddit.com/r/stupidslutsclub/comments/slouk0/multiple_guys_came_in_me_f_at_our_office_party\n",
      "[OK] id=soxjre | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/soxjre/after_five_years_of_marriage_i_finally_let_my\n",
      "[OK] id=9h3gd1 | folder=text | link=https://www.reddit.com/r/stupidslutsclub/comments/9h3gd1/for_four_years_i_was_a_stupid_slut_in_the_us_army\n",
      "[OK] id=9hm0ma | folder=text | link=https://www.reddit.com/r/stupidslutsclub/comments/9hm0ma/continued_for_four_years_i_was_a_stupid_slut_in\n",
      "[OK] id=sa81xy | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/sa81xy/f20_in_vegas_for_a_bachelor_party_to_provide_the\n",
      "[OK] id=sdg3en | folder=text | link=https://www.reddit.com/r/gonewildstories/comments/sdg3en/f_had_a_wild_time_at_the_bachelor_party_in_vegas\n",
      "[OK] id=sbul85 | folder=text | link=https://www.reddit.com/r/gonewildstories/comments/sbul85/f_had_a_wild_time_at_the_bachelor_party_in_vegas\n",
      "[OK] id=sb2ta1 | folder=text | link=https://www.reddit.com/r/gonewildstories/comments/sb2ta1/f_had_a_wild_time_at_the_bachelor_party_in_vegas\n",
      "[OK] id=se57aa | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/se57aa/f20_had_a_wild_time_at_the_bachelor_party_in\n",
      "[OK] id=jzqcya | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/jzqcya/i_25f_gifted_my_best_friend_22f_a_dildo_in_the\n",
      "[OK] id=r8jxwb | folder=text | link=https://www.reddit.com/r/SluttyConfessions/comments/r8jxwb/i_f22_was_gangbanged_for_my_birthday\n",
      "[OK] id=4ogo37 | folder=text | link=https://www.reddit.com/r/gonewildstories/comments/4ogo37/college_party_first_time_19f_on_my_knees_surprise\n",
      "[OK] id=4fdbr4 | folder=text | link=https://www.reddit.com/r/gonewildstories/comments/4fdbr4/the_most_common_questions_people_ask_me_28f_about\n",
      "[OK] id=3pdu3i | folder=text | link=https://www.reddit.com/r/gonewildstories/comments/3pdu3i/fwb_came_in_my_butt_f_just_hours_before_first\n",
      "[OK] id=hmjq5s | folder=text | link=https://www.reddit.com/r/NSFWIAMA/comments/hmjq5s/ive_participated_in_over_10_paid_gang_bangs_for\n",
      "[OK] id=bwi9m2 | folder=text | link=https://www.reddit.com/r/NSFWIAMA/comments/bwi9m2/my_evolution_from_doing_paid_gang_bangs_to_a\n",
      "[OK] id=7mo598 | folder=text | link=https://www.reddit.com/r/NSFWIAMA/comments/7mo598/i_did_paid_gang_bangs_to_get_out_of_debt_and_pay\n",
      "[OK] id=7mjo4d | folder=text | link=https://www.reddit.com/r/gonewildstories/comments/7mjo4d/i_put_myself_through_med_school_doing_gang_bangs\n",
      "[OK] id=44y6v2 | folder=text | link=https://www.reddit.com/r/gonewildstories/comments/44y6v2/i_28f_returned_to_a_vegas_sex_club_to_have_sex\n",
      "[OK] id=irhty | folder=text | link=https://www.reddit.com/r/sex/comments/irhty/my_experiences_and_tips_for_face_fucking_and_the\n",
      "[OK] id=p0nu3z | folder=text | link=https://www.reddit.com/r/TopsAndBottoms/comments/p0nu3z/preparing_my_hole_for_a_gangbang\n",
      "[OK] id=87w59r | folder=text | link=https://www.reddit.com/r/AskReddit/comments/87w59r/whats_the_best_sexual_experience_youve_ever_had\n",
      "[OK] id=6yvo25 | folder=text | link=https://www.reddit.com/r/stupidslutsclub/comments/6yvo25/whats_a_girl_to_do_a_day_before_her_first_gang\n",
      "[OK] id=rry977 | folder=media | link=https://www.reddit.com/r/MxRMods/comments/rry977/_\n",
      "[OK] id=ahygvt | folder=text | link=https://www.reddit.com/r/gonewildstories/comments/ahygvt/what_i_learned_from_being_gangbanged_group\n",
      "[OK] id=ntpgz9 | folder=media | link=https://www.reddit.com/r/rule34/comments/ntpgz9/hermione_granger_peter_pettigrew_4kolor_harry\n",
      "\n",
      "Saved failed links to: S:\\minds\\Desktop\\Downloader and Reddit System\\DOWNLOAD SYSTEM\\DOWNLOADERS\\NEW SYSTEM\\out\\failed.csv\n",
      "\n",
      "Done. Success: 150, Skipped: 638, Failed: 3, Total: 791. Output root: S:\\minds\\Desktop\\Downloader and Reddit System\\DOWNLOAD SYSTEM\\DOWNLOADERS\\NEW SYSTEM\\out\n"
     ]
    }
   ],
   "source": [
    "# Progress bar + failed-links export + show (id, link, folder) for each success\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs): return x\n",
    "\n",
    "base_out = Path(OUT_DIR)\n",
    "base_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load links first so we can show a progress bar\n",
    "links: list[str] = []\n",
    "with open(CSV_PATH, newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if row and row[0].strip():\n",
    "            links.append(row[0].strip())\n",
    "\n",
    "total = len(links)\n",
    "ok = failed = skipped = 0\n",
    "\n",
    "failed_rows = []  # collect failures to export later\n",
    "\n",
    "def already_archived(pid: str) -> bool:\n",
    "    return any((base_out / sub / f\"{pid}.json\").exists() for sub in (\"media\", \"external\", \"text\"))\n",
    "\n",
    "for link in tqdm(links, desc=\"Archiving posts\", unit=\"post\"):\n",
    "    pre_id, _ = parse_link(link)\n",
    "    if pre_id and already_archived(pre_id):\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        post, comments_listing, raw_array = fetch_full_post_and_comments(link)\n",
    "        pid = post.get(\"id\")\n",
    "        if not pid:\n",
    "            raise RuntimeError(\"Missing post id\")\n",
    "\n",
    "        category = classify_post(post)\n",
    "        archive = make_archive_object(post, comments_listing)\n",
    "        save_archive(archive, base_out, pid, category)\n",
    "        ok += 1\n",
    "        # ðŸ‘‡ show id, link, folder\n",
    "        print(f\"[OK] id={pid} | folder={category} | link={link}\")\n",
    "    except Exception as e:\n",
    "        failed += 1\n",
    "        status = getattr(getattr(e, \"response\", None), \"status_code\", \"\")\n",
    "        failed_rows.append({\n",
    "            \"link\": link,\n",
    "            \"guessed_id\": pre_id or \"\",\n",
    "            \"error\": str(e),\n",
    "            \"status\": status,\n",
    "        })\n",
    "\n",
    "    time.sleep(REQUEST_DELAY_SEC)\n",
    "\n",
    "# Export failed links to CSV in OUT_DIR\n",
    "if failed_rows:\n",
    "    fail_path = base_out / \"failed.csv\"\n",
    "    with fail_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"link\", \"guessed_id\", \"status\", \"error\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(failed_rows)\n",
    "    print(f\"\\nSaved failed links to: {fail_path.resolve()}\")\n",
    "\n",
    "print(f\"\\nDone. Success: {ok}, Skipped: {skipped}, Failed: {failed}, Total: {total}. Output root: {base_out.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a03a470",
   "metadata": {},
   "source": [
    "# EXTERNAL LINK EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08a3df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Extract external links and download Redgifs as <post_id>.mp4 ===\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs): return x\n",
    "\n",
    "BASE_OUT = Path(OUT_DIR)\n",
    "EXTERNAL_DIR = BASE_OUT / \"external\"\n",
    "REDDITS_OK = {\"reddit.com\", \"www.reddit.com\", \"old.reddit.com\", \"np.reddit.com\", \"oauth.reddit.com\", \"redd.it\"}\n",
    "REDDIT_NATIVE_MEDIA = {\"i.redd.it\", \"v.redd.it\"}\n",
    "\n",
    "# ---- 1) Helpers to read archives and extract the outbound link ----\n",
    "def _domain(url: str) -> str:\n",
    "    try:\n",
    "        return urlparse(url).netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_external_url(archive_obj: dict) -> str | None:\n",
    "    \"\"\"\n",
    "    From your saved archive object:\n",
    "      { \"raw_post\": {...}, \"raw_comments\": {...}, \"comments\": [...] }\n",
    "    Pull the outbound link for external posts.\n",
    "    \"\"\"\n",
    "    post = archive_obj.get(\"raw_post\") or {}\n",
    "    # Prefer the 'url_overridden_by_dest' field; fallback to 'url'\n",
    "    url = post.get(\"url_overridden_by_dest\") or post.get(\"url\")\n",
    "    if not url:\n",
    "        return None\n",
    "\n",
    "    d = _domain(url)\n",
    "    # Treat non-Reddit, non-native-media as external\n",
    "    if d and d not in REDDITS_OK and d not in REDDIT_NATIVE_MEDIA:\n",
    "        return url\n",
    "    return None\n",
    "\n",
    "# ---- 2) Redgifs normalization & API download ----\n",
    "# Accept common Redgifs URL shapes:\n",
    "RE_REDGIFS_ID = re.compile(\n",
    "    r\"\"\"(?ix)\n",
    "    (?:^|/)(?:watch|ifr)/([a-z0-9]+)     # redgifs.com/watch/<id> or /ifr/<id>\n",
    "    |                                   # OR\n",
    "    (?:^|/)(?:i)/([a-z0-9]+)            # i.redgifs.com/i/<id>\n",
    "    \"\"\".strip()\n",
    ")\n",
    "\n",
    "def redgifs_id_from_url(url: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extract the media ID from redgifs-style URLs:\n",
    "      - https://redgifs.com/watch/<id>\n",
    "      - https://www.redgifs.com/watch/<id>\n",
    "      - https://v3.redgifs.com/watch/<id>\n",
    "      - https://redgifs.com/ifr/<id>\n",
    "      - https://i.redgifs.com/i/<id>\n",
    "    \"\"\"\n",
    "    m = RE_REDGIFS_ID.search(url)\n",
    "    if not m:\n",
    "        return None\n",
    "    # One of the two groups will be set\n",
    "    gid = m.group(1) or m.group(2)\n",
    "    return gid.lower() if gid else None\n",
    "\n",
    "# Redgifs API: get a temporary token, then resolve mp4 URLs\n",
    "REDGIFS_AUTH_URL = \"https://api.redgifs.com/v2/auth/temporary\"\n",
    "REDGIFS_GIF_URL  = \"https://api.redgifs.com/v2/gifs/{id}\"\n",
    "\n",
    "_SESSION = requests.Session()\n",
    "_RG_TOKEN = None\n",
    "_RG_TOKEN_TS = 0\n",
    "\n",
    "def redgifs_token(force: bool = False) -> str:\n",
    "    global _RG_TOKEN, _RG_TOKEN_TS\n",
    "    now = time.time()\n",
    "    # Reuse token for ~20 minutes unless forced\n",
    "    if not force and _RG_TOKEN and (now - _RG_TOKEN_TS) < 1200:\n",
    "        return _RG_TOKEN\n",
    "    r = _SESSION.get(REDGIFS_AUTH_URL, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    _RG_TOKEN = r.json().get(\"token\")\n",
    "    _RG_TOKEN_TS = now\n",
    "    if not _RG_TOKEN:\n",
    "        raise RuntimeError(\"Failed to obtain Redgifs token.\")\n",
    "    return _RG_TOKEN\n",
    "\n",
    "def redgifs_mp4_url(gid: str) -> str:\n",
    "    tok = redgifs_token()\n",
    "    headers = {\"Authorization\": f\"Bearer {tok}\"}\n",
    "    r = _SESSION.get(REDGIFS_GIF_URL.format(id=gid), headers=headers, timeout=30)\n",
    "    # If token expired, refresh once\n",
    "    if r.status_code in (401, 403):\n",
    "        tok = redgifs_token(force=True)\n",
    "        headers = {\"Authorization\": f\"Bearer {tok}\"}\n",
    "        r = _SESSION.get(REDGIFS_GIF_URL.format(id=gid), headers=headers, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    info = r.json().get(\"gif\") or {}\n",
    "    # Prefer HD if present, else SD, else fallback to urls.origin\n",
    "    urls = info.get(\"urls\") or {}\n",
    "    return urls.get(\"hd\") or urls.get(\"sd\") or urls.get(\"origin\")\n",
    "\n",
    "def download_stream(url: str, dest: Path, *, max_retries: int = 4):\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with _SESSION.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                with open(dest, \"wb\") as f:\n",
    "                    for chunk in r.iter_content(chunk_size=1024 * 256):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            if attempt + 1 >= max_retries:\n",
    "                raise\n",
    "            time.sleep(min(2 ** attempt, 15))\n",
    "\n",
    "# ---- 3) Walk external posts, export external links CSV, download Redgifs ----\n",
    "external_json_files = sorted(EXTERNAL_DIR.glob(\"*.json\"))\n",
    "print(f\"Found {len(external_json_files)} external post JSONs in {EXTERNAL_DIR}\")\n",
    "\n",
    "external_rows = []\n",
    "redgifs_failed = []\n",
    "\n",
    "REDGIFS_OUT = BASE_OUT / \"redgifs\"\n",
    "REDGIFS_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for fp in tqdm(external_json_files, desc=\"Scanning external posts\", unit=\"post\"):\n",
    "    try:\n",
    "        data = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "        post = (data or {}).get(\"raw_post\") or {}\n",
    "        pid  = post.get(\"id\") or fp.stem  # fallback to filename if needed\n",
    "\n",
    "        ext_url = extract_external_url(data)\n",
    "        if not ext_url:\n",
    "            # Still record that this external-typed file has no resolvable URL\n",
    "            external_rows.append({\"id\": pid, \"link\": \"\", \"domain\": \"\"})\n",
    "            continue\n",
    "\n",
    "        dom = _domain(ext_url)\n",
    "        external_rows.append({\"id\": pid, \"link\": ext_url, \"domain\": dom})\n",
    "\n",
    "        # Redgifs download\n",
    "        if \"redgifs.com\" in dom or dom.endswith(\".redgifs.com\"):\n",
    "            gid = redgifs_id_from_url(ext_url)\n",
    "            if not gid:\n",
    "                # Sometimes the external URL is a redirect page; skip but log\n",
    "                redgifs_failed.append({\"id\": pid, \"link\": ext_url, \"reason\": \"no_id_from_url\"})\n",
    "                continue\n",
    "\n",
    "            out_path = REDGIFS_OUT / f\"{pid}.mp4\"\n",
    "            if out_path.exists():\n",
    "                # already downloaded\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                mp4_url = redgifs_mp4_url(gid)\n",
    "                if not mp4_url:\n",
    "                    redgifs_failed.append({\"id\": pid, \"link\": ext_url, \"reason\": \"no_mp4_url\"})\n",
    "                    continue\n",
    "                download_stream(mp4_url, out_path)\n",
    "                # Show success line\n",
    "                print(f\"[REDGIFS] id={pid} -> {out_path.name}\")\n",
    "            except Exception as e:\n",
    "                redgifs_failed.append({\"id\": pid, \"link\": ext_url, \"reason\": str(e)})\n",
    "    except Exception as e:\n",
    "        # If we cannot read this JSON at all, log as a redgifs failure only if it looked like redgifs\n",
    "        redgifs_failed.append({\"id\": fp.stem, \"link\": \"\", \"reason\": f\"read_error: {e}\"})\n",
    "\n",
    "# ---- 4) Write summary CSVs ----\n",
    "ext_csv = BASE_OUT / \"external_links.csv\"\n",
    "with ext_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"id\", \"link\", \"domain\"])\n",
    "    w.writeheader()\n",
    "    w.writerows(external_rows)\n",
    "\n",
    "if redgifs_failed:\n",
    "    fail_csv = BASE_OUT / \"redgifs_failed.csv\"\n",
    "    with fail_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"id\", \"link\", \"reason\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(redgifs_failed)\n",
    "    print(f\"\\nSaved Redgifs download failures to: {fail_csv.resolve()}\")\n",
    "\n",
    "print(f\"\\nSaved external links to: {ext_csv.resolve()}\")\n",
    "print(f\"Redgifs saved (if any) to: {REDGIFS_OUT.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938da43",
   "metadata": {},
   "source": [
    "# MEDIA DOWNLOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb94da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Download embedded Reddit-hosted media for posts in out/media/*.json ===\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import html\n",
    "import mimetypes\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs): return x\n",
    "\n",
    "BASE_OUT = Path(OUT_DIR)\n",
    "MEDIA_JSON_DIR = BASE_OUT / \"media\"\n",
    "MEDIA_OUT_DIR = BASE_OUT / \"media_files\"\n",
    "MEDIA_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\"User-Agent\": \"reddit-media-downloader/1.0\"})\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _clean_url(u: str | None) -> str | None:\n",
    "    if not u:\n",
    "        return None\n",
    "    # Reddit often returns HTML-escaped URLs inside JSON\n",
    "    return html.unescape(u)\n",
    "\n",
    "def _domain(u: str | None) -> str:\n",
    "    if not u:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return urlparse(u).netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _ext_from_url_or_type(url: str | None, content_type: str | None) -> str:\n",
    "    # Prefer extension from URL, else derive from content-type\n",
    "    if url:\n",
    "        path = urlparse(url).path\n",
    "        ext = os.path.splitext(path)[1].lower()\n",
    "        if ext in {\".jpg\", \".jpeg\", \".png\", \".gif\", \".mp4\", \".webm\"}:\n",
    "            return ext\n",
    "    if content_type:\n",
    "        ext = mimetypes.guess_extension(content_type.split(\";\")[0].strip())\n",
    "        if ext:\n",
    "            # normalize jpeg\n",
    "            return \".jpg\" if ext == \".jpe\" else ext\n",
    "    # sensible default fallback\n",
    "    return \".mp4\" if (url and \".mp4\" in url) else \".jpg\"\n",
    "\n",
    "def _stream_download(url: str, dest: Path, *, max_retries: int = 4, chunk=1024 * 256):\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with SESSION.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                ctype = r.headers.get(\"Content-Type\")\n",
    "                # if dest has no extension yet, refine using content-type\n",
    "                if dest.suffix == \"\" and ctype:\n",
    "                    dest = dest.with_suffix(_ext_from_url_or_type(url, ctype))\n",
    "                with open(dest, \"wb\") as f:\n",
    "                    for part in r.iter_content(chunk_size=chunk):\n",
    "                        if part:\n",
    "                            f.write(part)\n",
    "            return dest  # final path (may include refined suffix)\n",
    "        except Exception:\n",
    "            if attempt + 1 >= max_retries:\n",
    "                raise\n",
    "            time.sleep(min(2 ** attempt, 15))\n",
    "\n",
    "def _pick_best_preview(post: dict) -> str | None:\n",
    "    \"\"\"\n",
    "    For image/GIF-like posts where 'preview' exists.\n",
    "    Prefer MP4 variant (smaller, plays everywhere), else best image 'source'.\n",
    "    \"\"\"\n",
    "    prev = post.get(\"preview\") or {}\n",
    "    variants = prev.get(\"variants\") or {}\n",
    "    # mp4 variant for gifs, etc.\n",
    "    mp4v = variants.get(\"mp4\") or variants.get(\"reddit_video_preview\")\n",
    "    if mp4v and mp4v.get(\"source\", {}).get(\"url\"):\n",
    "        return _clean_url(mp4v[\"source\"][\"url\"])\n",
    "    # fallback to the image source\n",
    "    src = (prev.get(\"images\") or [{}])[0].get(\"source\", {})\n",
    "    if src.get(\"url\"):\n",
    "        return _clean_url(src[\"url\"])\n",
    "    return None\n",
    "\n",
    "def _pick_vreddit_urls(post: dict) -> tuple[str | None, str | None]:\n",
    "    \"\"\"\n",
    "    v.redd.it posts: return (preferred_mp4_url, fallback_mp4_url)\n",
    "    Try in order: 'hls_url' (m3u8) -> 'fallback_url' (progressive) -> preview mp4.\n",
    "    We only directly download MP4 (no ffmpeg merge here), so we prefer fallback_url,\n",
    "    and otherwise try preview mp4.\n",
    "    \"\"\"\n",
    "    media = post.get(\"media\") or {}\n",
    "    rv = media.get(\"reddit_video\") or {}\n",
    "    fallback = rv.get(\"fallback_url\")  # often progressive mp4 (may be muted on long vids)\n",
    "    hls = rv.get(\"hls_url\")            # m3u8 playlist (would require ffmpeg)\n",
    "    # If no fallback, sometimes preview.mp4 exists:\n",
    "    prev_mp4 = None\n",
    "    prev = post.get(\"preview\") or {}\n",
    "    pv = prev.get(\"reddit_video_preview\") or {}\n",
    "    if isinstance(pv, dict) and pv.get(\"fallback_url\"):\n",
    "        prev_mp4 = pv[\"fallback_url\"]\n",
    "    return (_clean_url(fallback), _clean_url(prev_mp4 or hls))\n",
    "\n",
    "def _gallery_items(post: dict) -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    For gallery posts: return list of (url, suggested_ext).\n",
    "    Uses media_metadata to select best 's' rendition.\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    meta = post.get(\"media_metadata\") or {}\n",
    "    gdata = post.get(\"gallery_data\") or {}\n",
    "    order = [e.get(\"media_id\") for e in gdata.get(\"items\", []) if e.get(\"media_id\")]\n",
    "    for mid in order:\n",
    "        m = meta.get(mid) or {}\n",
    "        s = m.get(\"s\") or {}\n",
    "        url = _clean_url(s.get(\"mp4\") or s.get(\"gif\") or s.get(\"u\") or s.get(\"url\"))\n",
    "        if not url:\n",
    "            continue\n",
    "        # guess extension: mp4 preferred over gif over image\n",
    "        if \"mp4\" in s:\n",
    "            ext = \".mp4\"\n",
    "        elif \"gif\" in s:\n",
    "            ext = \".mp4\"  # we'll still download the gif URL, but use .mp4 if it's actually mp4\n",
    "        else:\n",
    "            # look at mime if present\n",
    "            m_type = m.get(\"m\")\n",
    "            ext = _ext_from_url_or_type(url, m_type)\n",
    "        items.append((url, ext))\n",
    "    return items\n",
    "\n",
    "# ---------- main walk ----------\n",
    "media_jsons = sorted(MEDIA_JSON_DIR.glob(\"*.json\"))\n",
    "print(f\"Found {len(media_jsons)} media post JSONs in {MEDIA_JSON_DIR}\")\n",
    "\n",
    "fail_rows = []\n",
    "downloaded = 0\n",
    "\n",
    "for fp in tqdm(media_jsons, desc=\"Downloading embedded media\", unit=\"post\"):\n",
    "    try:\n",
    "        data = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "        post = (data or {}).get(\"raw_post\") or {}\n",
    "        pid = post.get(\"id\") or fp.stem\n",
    "\n",
    "        # Prefer Reddit-hosted URL if present\n",
    "        url = _clean_url(post.get(\"url_overridden_by_dest\") or post.get(\"url\"))\n",
    "        dom = _domain(url)\n",
    "\n",
    "        # Case A: gallery\n",
    "        if post.get(\"is_gallery\") or (post.get(\"gallery_data\") and post.get(\"media_metadata\")):\n",
    "            items = _gallery_items(post)\n",
    "            if not items:\n",
    "                fail_rows.append({\"id\": pid, \"reason\": \"gallery_no_items\"})\n",
    "                continue\n",
    "            for idx, (item_url, ext) in enumerate(items, start=1):\n",
    "                outfile = MEDIA_OUT_DIR / f\"{pid}_g{idx:02d}{ext if ext.startswith('.') else ('.' + ext)}\"\n",
    "                if outfile.exists():\n",
    "                    continue\n",
    "                try:\n",
    "                    _stream_download(item_url, outfile)\n",
    "                    downloaded += 1\n",
    "                    print(f\"[GAL] {pid} -> {outfile.name}\")\n",
    "                except Exception as e:\n",
    "                    fail_rows.append({\"id\": pid, \"reason\": f\"gallery_item_fail:{e}\"})\n",
    "\n",
    "            continue  # next post\n",
    "\n",
    "        # Case B: native video (v.redd.it)\n",
    "        if (post.get(\"is_video\") or (post.get(\"media\") or {}).get(\"reddit_video\")) and dom.endswith(\"v.redd.it\"):\n",
    "            main_mp4, alt_mp4 = _pick_vreddit_urls(post)\n",
    "            target = MEDIA_OUT_DIR / f\"{pid}.mp4\"\n",
    "            if target.exists():\n",
    "                continue\n",
    "            src = main_mp4 or alt_mp4\n",
    "            if not src:\n",
    "                # last chance: look into preview variants\n",
    "                src = _pick_best_preview(post)\n",
    "            if not src:\n",
    "                fail_rows.append({\"id\": pid, \"reason\": \"vreddit_no_source\"})\n",
    "                continue\n",
    "            try:\n",
    "                _stream_download(src, target)\n",
    "                downloaded += 1\n",
    "                print(f\"[VID] {pid} -> {target.name}\")\n",
    "            except Exception as e:\n",
    "                fail_rows.append({\"id\": pid, \"reason\": f\"vreddit_dl_fail:{e}\"})\n",
    "            continue\n",
    "\n",
    "        # Case C: image / gif via i.redd.it or preview\n",
    "        if dom.endswith(\"i.redd.it\"):\n",
    "            # Direct i.redd.it link\n",
    "            ext = _ext_from_url_or_type(url, None)\n",
    "            target = MEDIA_OUT_DIR / f\"{pid}{ext}\"\n",
    "            if not target.exists():\n",
    "                try:\n",
    "                    _stream_download(url, target)\n",
    "                    downloaded += 1\n",
    "                    print(f\"[IMG] {pid} -> {target.name}\")\n",
    "                except Exception as e:\n",
    "                    fail_rows.append({\"id\": pid, \"reason\": f\"ireddit_dl_fail:{e}\"})\n",
    "            continue\n",
    "\n",
    "        # Fallback: try preview (covers some GIF-to-MP4 conversions)\n",
    "        prev_url = _pick_best_preview(post)\n",
    "        if prev_url and _domain(prev_url) in {\"i.redd.it\", \"v.redd.it\", \"preview.redd.it\"}:\n",
    "            ext = _ext_from_url_or_type(prev_url, None)\n",
    "            target = MEDIA_OUT_DIR / f\"{pid}{ext}\"\n",
    "            if not target.exists():\n",
    "                try:\n",
    "                    _stream_download(prev_url, target)\n",
    "                    downloaded += 1\n",
    "                    print(f\"[PREV] {pid} -> {target.name}\")\n",
    "                except Exception as e:\n",
    "                    fail_rows.append({\"id\": pid, \"reason\": f\"preview_dl_fail:{e}\"})\n",
    "            continue\n",
    "\n",
    "        # If we reach here, it looks like a Reddit-hosted \"media\" without a reliable direct URL\n",
    "        fail_rows.append({\"id\": pid, \"reason\": \"no_reddit_media_url\"})\n",
    "    except Exception as e:\n",
    "        fail_rows.append({\"id\": fp.stem, \"reason\": f\"read_error:{e}\"})\n",
    "\n",
    "# ---------- write failures ----------\n",
    "if fail_rows:\n",
    "    fail_csv = BASE_OUT / \"media_failed.csv\"\n",
    "    with fail_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"id\", \"reason\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(fail_rows)\n",
    "    print(f\"\\nSaved media failures to: {fail_csv.resolve()}\")\n",
    "\n",
    "print(f\"\\nDone. Downloaded: {downloaded}. Files saved under: {MEDIA_OUT_DIR.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Primary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
