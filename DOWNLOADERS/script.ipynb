{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4869123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reddit script app credentials (temporary hardcode ok for local use) ---\n",
    "REDDIT_CLIENT_ID = \"Ik1IhrLMkUe2Y7_jLqj-Ew\"\n",
    "REDDIT_CLIENT_SECRET = \"1j81ffxuNl-e8EzPV4D3OzCVCH-1lw\"\n",
    "REDDIT_USERNAME = \"Grand_Admiral_Tyken\"\n",
    "REDDIT_PASSWORD = \"X5bugNC9j3Bc^Uf\"\n",
    "\n",
    "\n",
    "# --- IO config ---\n",
    "CSV_PATH = \"links.csv\"   # one Reddit URL per line, no header\n",
    "OUT_DIR  = \"out\"         # base folder for outputs\n",
    "\n",
    "# --- polite request pacing ---\n",
    "REQUEST_DELAY_SEC = 0.5  # delay between requests to Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deda5f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "\n",
    "# Session + UA\n",
    "SESSION = requests.Session()\n",
    "UA = f\"reddit-json-downloader/1.0 (by u/{REDDIT_USERNAME})\"\n",
    "\n",
    "# OAuth endpoints and params\n",
    "OAUTH_TOKEN_URL = \"https://www.reddit.com/api/v1/access_token\"\n",
    "OAUTH_API_BASE  = \"https://oauth.reddit.com\"\n",
    "COMMENTS_QUERY  = \"raw_json=1&limit=500&depth=10&showmore=true\"  # fuller comment payload\n",
    "\n",
    "# URL helpers\n",
    "COMMENTS_ID_RE = re.compile(r\"/comments/([a-z0-9]{5,8})\", re.IGNORECASE)\n",
    "SHORTLINK_RE   = re.compile(r\"redd\\.it/([a-z0-9]{5,8})\", re.IGNORECASE)\n",
    "SUB_RE         = re.compile(r\"/r/([^/]+)/comments/\", re.IGNORECASE)\n",
    "\n",
    "def request_with_backoff(method: str, url: str, *, headers=None, data=None, timeout=60, max_retries=5):\n",
    "    for attempt in range(max_retries):\n",
    "        r = SESSION.request(method, url, headers=headers, data=data, timeout=timeout)\n",
    "        if r.status_code < 400:\n",
    "            return r\n",
    "        if r.status_code in (429,) or 500 <= r.status_code < 600:\n",
    "            time.sleep(min(2 ** attempt, 30))\n",
    "            continue\n",
    "        return r\n",
    "    return r  # last response\n",
    "\n",
    "def get_token() -> str:\n",
    "    auth = requests.auth.HTTPBasicAuth(REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET)\n",
    "    data = {\"grant_type\": \"password\", \"username\": REDDIT_USERNAME, \"password\": REDDIT_PASSWORD}\n",
    "    headers = {\"User-Agent\": UA}\n",
    "    r = requests.post(OAUTH_TOKEN_URL, auth=auth, data=data, headers=headers, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    tok = r.json().get(\"access_token\")\n",
    "    if not tok:\n",
    "        raise RuntimeError(f\"OAuth token missing; resp={r.text}\")\n",
    "    return tok\n",
    "\n",
    "def oauth_headers() -> dict:\n",
    "    tok = getattr(SESSION, \"_oauth_token\", None)\n",
    "    if not tok:\n",
    "        tok = get_token()\n",
    "        SESSION._oauth_token = tok\n",
    "    return {\"Authorization\": f\"bearer {tok}\", \"User-Agent\": UA}\n",
    "\n",
    "def accept_quarantine(subreddit: str) -> bool:\n",
    "    if not subreddit:\n",
    "        return False\n",
    "    url = f\"{OAUTH_API_BASE}/api/accept_quarantine\"\n",
    "    r = request_with_backoff(\"POST\", url, headers=oauth_headers(), max_retries=3)\n",
    "    return r.status_code in (200, 204, 409)  # 409 ~ already accepted\n",
    "\n",
    "def parse_link(link: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    m = COMMENTS_ID_RE.search(link)\n",
    "    post_id = m.group(1) if m else (SHORTLINK_RE.search(link).group(1) if SHORTLINK_RE.search(link) else None)\n",
    "    m_sr = SUB_RE.search(link)\n",
    "    subreddit = m_sr.group(1) if m_sr else None\n",
    "    return post_id, subreddit\n",
    "\n",
    "def normalize_comments_url(link: str, fallback_post_id: Optional[str]) -> str:\n",
    "    p = urlparse(link)\n",
    "    path = p.path or \"\"\n",
    "    host = (p.netloc or \"\").lower()\n",
    "    if \"redd.it\" in host or \"/comments/\" not in path:\n",
    "        if fallback_post_id:\n",
    "            path = f\"/comments/{fallback_post_id}/\"\n",
    "    if not path.endswith(\"/\"):\n",
    "        path += \"/\"\n",
    "    return f\"{OAUTH_API_BASE}{path}.json?{COMMENTS_QUERY}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8457222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_more_ids(listing_node) -> List[str]:\n",
    "    ids = []\n",
    "    def walk(node):\n",
    "        if not isinstance(node, dict): return\n",
    "        kind = node.get(\"kind\"); data = node.get(\"data\", {})\n",
    "        if kind == \"more\":\n",
    "            children = data.get(\"children\") or []\n",
    "            ids.extend([c for c in children if c])\n",
    "        elif kind in (\"Listing\", \"t1\"):\n",
    "            for ch in data.get(\"children\", []): walk(ch)\n",
    "            if kind == \"t1\" and isinstance(data.get(\"replies\"), dict): walk(data[\"replies\"])\n",
    "    walk(listing_node)\n",
    "    return ids\n",
    "\n",
    "def index_comments_by_id(listing_node) -> Dict[str, dict]:\n",
    "    idx = {}\n",
    "    def walk(node):\n",
    "        if not isinstance(node, dict): return\n",
    "        kind = node.get(\"kind\"); data = node.get(\"data\", {})\n",
    "        if kind == \"t1\":\n",
    "            cid = (data.get(\"id\") or \"\").lower()\n",
    "            if cid: idx[cid] = node\n",
    "            if isinstance(data.get(\"replies\"), dict): walk(data[\"replies\"])\n",
    "        elif kind == \"Listing\":\n",
    "            for ch in data.get(\"children\", []): walk(ch)\n",
    "    walk(listing_node)\n",
    "    return idx\n",
    "\n",
    "def replace_more_with_children(root_listing: dict, parent_lookup: Dict[str, dict], chunk_result: dict):\n",
    "    listing = chunk_result.get(\"json\", {}).get(\"data\", {}).get(\"things\", [])\n",
    "    for thing in listing:\n",
    "        if thing.get(\"kind\") != \"t1\": continue\n",
    "        data = thing.get(\"data\", {})\n",
    "        pid = data.get(\"parent_id\", \"\")\n",
    "        if pid.startswith(\"t1_\"):\n",
    "            parent_id = pid[3:].lower()\n",
    "            parent = parent_lookup.get(parent_id)\n",
    "            if parent:\n",
    "                if not isinstance(parent[\"data\"].get(\"replies\"), dict):\n",
    "                    parent[\"data\"][\"replies\"] = {\"kind\": \"Listing\", \"data\": {\"children\": []}}\n",
    "                parent[\"data\"][\"replies\"][\"data\"][\"children\"].append(thing)\n",
    "        elif pid.startswith(\"t3_\"):\n",
    "            root_listing[\"data\"][\"children\"].append(thing)\n",
    "\n",
    "def strip_more_nodes(node):\n",
    "    if not isinstance(node, dict): return\n",
    "    kind = node.get(\"kind\"); data = node.get(\"data\", {})\n",
    "    if kind == \"Listing\":\n",
    "        new_children = [ch for ch in data.get(\"children\", []) if ch.get(\"kind\") != \"more\"]\n",
    "        data[\"children\"] = new_children\n",
    "        for ch in new_children: strip_more_nodes(ch)\n",
    "    if kind == \"t1\" and isinstance(data.get(\"replies\"), dict):\n",
    "        strip_more_nodes(data[\"replies\"])\n",
    "\n",
    "def fetch_full_post_and_comments(link: str) -> Tuple[dict, dict]:\n",
    "    if not link.startswith((\"http://\", \"https://\")):\n",
    "        raise ValueError(f\"Not a URL: {link}\")\n",
    "\n",
    "    post_id, sr_hint = parse_link(link)\n",
    "    comments_url = normalize_comments_url(link, post_id)\n",
    "\n",
    "    r = request_with_backoff(\"GET\", comments_url, headers=oauth_headers(), timeout=60)\n",
    "    if r.status_code == 403:\n",
    "        sr = sr_hint or (re.search(r\"/r/([^/]+)/comments/\", comments_url).group(1) if re.search(r\"/r/([^/]+)/comments/\", comments_url) else \"\")\n",
    "        if sr and accept_quarantine(sr):\n",
    "            r = request_with_backoff(\"GET\", comments_url, headers=oauth_headers(), timeout=60)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    if not (isinstance(data, list) and len(data) >= 2):\n",
    "        raise RuntimeError(\"Unexpected Reddit JSON format\")\n",
    "\n",
    "    post_listing = data[0][\"data\"][\"children\"]\n",
    "    if not post_listing:\n",
    "        raise RuntimeError(\"Post listing empty\")\n",
    "    post = post_listing[0][\"data\"]\n",
    "    subreddit = post.get(\"subreddit\") or sr_hint or \"\"\n",
    "    link_id = post.get(\"id\")\n",
    "    comments_listing = data[1]\n",
    "\n",
    "    if link_id:\n",
    "        while True:\n",
    "            more_ids = collect_more_ids(comments_listing)\n",
    "            if not more_ids: break\n",
    "            for i in range(0, len(more_ids), 100):\n",
    "                chunk = more_ids[i:i+100]\n",
    "                form = {\n",
    "                    \"link_id\": f\"t3_{link_id}\",\n",
    "                    \"api_type\": \"json\",\n",
    "                    \"children\": \",\".join(chunk),\n",
    "                    \"sort\": \"confidence\",\n",
    "                    \"limit_children\": False,\n",
    "                    \"raw_json\": 1,\n",
    "                }\n",
    "                url = f\"{OAUTH_API_BASE}/api/morechildren\"\n",
    "                r2 = request_with_backoff(\"POST\", url, headers=oauth_headers(), data=form, timeout=60)\n",
    "                if r2.status_code == 403 and subreddit and accept_quarantine(subreddit):\n",
    "                    r2 = request_with_backoff(\"POST\", url, headers=oauth_headers(), data=form, timeout=60)\n",
    "                r2.raise_for_status()\n",
    "                payload = r2.json()\n",
    "                parent_idx = index_comments_by_id(comments_listing)\n",
    "                replace_more_with_children(comments_listing, parent_idx, payload)\n",
    "            strip_more_nodes(comments_listing)\n",
    "\n",
    "    return post, comments_listing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "381b7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification + canonical archive builder (matches your 1a1ybm-style top level)\n",
    "\n",
    "INTERNAL_REDDIT_HOSTS = {\n",
    "    \"reddit.com\", \"www.reddit.com\", \"old.reddit.com\", \"np.reddit.com\", \"oauth.reddit.com\",\n",
    "    \"redd.it\",\n",
    "}\n",
    "NATIVE_MEDIA_HOSTS = {\"i.redd.it\", \"v.redd.it\"}\n",
    "\n",
    "def domain_of(url: Optional[str]) -> str:\n",
    "    if not url: return \"\"\n",
    "    try:\n",
    "        return urlparse(url).netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def is_gallery(post: dict) -> bool:\n",
    "    return bool(post.get(\"is_gallery\") or (post.get(\"gallery_data\") and post.get(\"media_metadata\")))\n",
    "\n",
    "def summarize_media(post: dict) -> dict:\n",
    "    is_self = bool(post.get(\"is_self\"))\n",
    "    url = post.get(\"url_overridden_by_dest\") or post.get(\"url\")\n",
    "    dom = domain_of(url)\n",
    "    if is_self:\n",
    "        return {\"kind\": \"self\", \"files\": []}\n",
    "    if is_gallery(post):\n",
    "        return {\"kind\": \"gallery\", \"files\": []}\n",
    "    if dom in NATIVE_MEDIA_HOSTS or post.get(\"is_video\") or (post.get(\"media\") or {}).get(\"reddit_video\"):\n",
    "        return {\"kind\": \"video\", \"files\": []}\n",
    "    if dom and dom not in INTERNAL_REDDIT_HOSTS:\n",
    "        return {\"kind\": \"link\", \"files\": []}\n",
    "    if dom == \"i.redd.it\" or (post.get(\"preview\") and (post.get(\"post_hint\") or \"\").startswith(\"image\")):\n",
    "        return {\"kind\": \"image\", \"files\": []}\n",
    "    return {\"kind\": \"self\" if is_self else \"link\", \"files\": []}\n",
    "\n",
    "def external_link_or_none(post: dict) -> Optional[str]:\n",
    "    url = post.get(\"url_overridden_by_dest\") or post.get(\"url\")\n",
    "    dom = domain_of(url)\n",
    "    if url and dom and (dom not in INTERNAL_REDDIT_HOSTS) and (dom not in NATIVE_MEDIA_HOSTS):\n",
    "        return url\n",
    "    return None\n",
    "\n",
    "def classify_post(post_data: dict) -> str:\n",
    "    \"\"\"\n",
    "    Decide among: 'external', 'media', 'text'.\n",
    "    Priority:\n",
    "      1) external  -> off-Reddit (e.g., redgifs.com)\n",
    "      2) media     -> native Reddit media (i.redd.it, v.redd.it), previews, galleries\n",
    "      3) text      -> self-posts without media\n",
    "    \"\"\"\n",
    "    is_self = bool(post_data.get(\"is_self\"))\n",
    "    url = post_data.get(\"url_overridden_by_dest\") or post_data.get(\"url\")\n",
    "    d = domain_of(url) or (post_data.get(\"domain\") or \"\").lower()\n",
    "\n",
    "    if not is_self and d and (d not in INTERNAL_REDDIT_HOSTS) and (d not in NATIVE_MEDIA_HOSTS):\n",
    "        return \"external\"\n",
    "\n",
    "    post_hint   = (post_data.get(\"post_hint\") or \"\").lower()\n",
    "    has_gallery = bool(post_data.get(\"gallery_data\"))\n",
    "    has_preview = bool(post_data.get(\"preview\"))\n",
    "    has_media   = bool(post_data.get(\"media\")) or bool(post_data.get(\"is_video\"))\n",
    "    is_native_media_host = d in NATIVE_MEDIA_HOSTS\n",
    "    is_media_hint = post_hint in {\"image\", \"hosted:video\", \"rich:video\"}\n",
    "\n",
    "    if is_native_media_host or has_media or has_preview or has_gallery or is_media_hint:\n",
    "        return \"media\"\n",
    "    return \"text\"\n",
    "\n",
    "def canonical_archive(post: dict, comments_listing: dict) -> dict:\n",
    "    permalink = post.get(\"permalink\") or \"\"\n",
    "    if permalink and not permalink.startswith(\"http\"):\n",
    "        permalink = f\"https://www.reddit.com{permalink}\"\n",
    "    return {\n",
    "        \"archived_at\": time.strftime(\"%Y-%m-%dT%H:%M:%S.000000+00:00\", time.gmtime()),\n",
    "        \"reddit_fullname\": f\"t3_{post.get('id') or ''}\",\n",
    "        \"reddit_id\": post.get(\"id\"),\n",
    "        \"permalink\": permalink,\n",
    "        \"title\": post.get(\"title\"),\n",
    "        \"selftext\": post.get(\"selftext\"),\n",
    "        \"author\": post.get(\"author\"),\n",
    "        \"author_fullname\": post.get(\"author_fullname\"),\n",
    "        \"subreddit\": post.get(\"subreddit\"),\n",
    "        \"subreddit_id\": post.get(\"subreddit_id\"),\n",
    "        \"created_utc\": post.get(\"created_utc\"),\n",
    "        \"is_self\": post.get(\"is_self\"),\n",
    "        \"url\": post.get(\"url\"),\n",
    "        \"domain\": post.get(\"domain\"),\n",
    "        \"post_hint\": post.get(\"post_hint\"),\n",
    "        \"is_gallery\": is_gallery(post),\n",
    "        \"over_18\": post.get(\"over_18\"),\n",
    "        \"spoiler\": post.get(\"spoiler\"),\n",
    "        \"link_flair_text\": post.get(\"link_flair_text\"),\n",
    "        \"is_original_content\": post.get(\"is_original_content\"),\n",
    "        \"stickied\": post.get(\"stickied\"),\n",
    "        \"locked\": post.get(\"locked\"),\n",
    "        \"edited\": post.get(\"edited\"),\n",
    "        \"num_comments\": post.get(\"num_comments\"),\n",
    "        \"score\": post.get(\"score\"),\n",
    "        \"upvote_ratio\": post.get(\"upvote_ratio\"),\n",
    "        \"media\": summarize_media(post),\n",
    "        \"external_link\": external_link_or_none(post),\n",
    "        \"raw_post\": post,\n",
    "        \"raw_comments\": comments_listing,\n",
    "    }\n",
    "\n",
    "def save_archive(doc: dict, base_out: Path, post_id: str, category: str):\n",
    "    target = base_out / category\n",
    "    target.mkdir(parents=True, exist_ok=True)\n",
    "    (target / f\"{post_id}.json\").write_text(json.dumps(doc, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def already_archived(base_out: Path, pid: str) -> bool:\n",
    "    return any((base_out / sub / f\"{pid}.json\").exists() for sub in (\"media\", \"external\", \"text\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f89f87a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b572719ba1ee4e3aadd2a5b326e4fec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Archiving posts:   0%|          | 0/231 [00:00<?, ?post/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] id=kk219h | folder=external | link=https://www.reddit.com/r/HENTAI_GIF/comments/kk219h/this_is_a_nice_gift_by_kamuo/\n",
      "[OK] id=1jjo5bz | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1jjo5bz/410_japanese_milf_learns_why_you_dont_put_a/\n",
      "[OK] id=1h616ou | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1h616ou/the_greatest_amateur_anal_cumslut/\n",
      "[OK] id=1ekrwun | folder=external | link=https://www.reddit.com/r/rape_hentai/comments/1ekrwun/the_aftermath_of_getting_infected_by_the/\n",
      "[OK] id=1jxakwg | folder=external | link=https://www.reddit.com/r/cumshots/comments/1jxakwg/i_dont_think_ive_ever_seen_a_more_perfect_pair_of/\n",
      "[OK] id=1lyv4t3 | folder=external | link=https://www.reddit.com/r/GOONED/comments/1lyv4t3/this_is_the_type_of_princess_treatment_i_want/\n",
      "[OK] id=1lvg3mg | folder=external | link=https://www.reddit.com/r/rapefantasies/comments/1lvg3mg/would_men_even_be_into_raping_me_because_id_enjoy/\n",
      "[OK] id=1llpi8o | folder=external | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1llpi8o/please_please_pleasestuff_and_breed_my_needy_goth/\n",
      "[OK] id=ttra64 | folder=external | link=https://www.reddit.com/r/NSFW_GIF/comments/ttra64/quivering/\n",
      "[OK] id=1g43bef | folder=external | link=https://www.reddit.com/r/rape_hentai/comments/1g43bef/am_i_a_bad_daddy_for_using_my_daughter_as_my/\n",
      "[OK] id=1ia9t5a | folder=external | link=https://www.reddit.com/r/rapefantasies/comments/1ia9t5a/ive_always_secretly_wanted_to_have_my_ass/\n",
      "[OK] id=1n4xmb2 | folder=external | link=https://www.reddit.com/r/misogynyKINKmemes/comments/1n4xmb2/are_you_brave_enough_to_be_next_in_line_slut/\n",
      "[OK] id=1jvqz1f | folder=external | link=https://www.reddit.com/r/Miso_Paradise/comments/1jvqz1f/youre_nothing_more_then_a_set_of_holes_for_my/\n",
      "[OK] id=14hnsiu | folder=external | link=https://www.reddit.com/r/porn/comments/14hnsiu/ryourpornaddiction_pull_her_over/\n",
      "[OK] id=1j35gby | folder=external | link=https://www.reddit.com/r/rape_hentai/comments/1j35gby/you_asked_a_demon_for_a_new_toy_little_did_you/\n",
      "[OK] id=1fkx8ff | folder=external | link=https://www.reddit.com/r/bdsm/comments/1fkx8ff/one_way_mirror_box/\n",
      "[OK] id=1lr652v | folder=external | link=https://www.reddit.com/r/MisogynyCaps/comments/1lr652v/genz_have_been_born_and_bred_to_be_rapetoys/\n",
      "[OK] id=1mc951v | folder=external | link=https://www.reddit.com/r/bdsm/comments/1mc951v/dream_come_true/\n",
      "[OK] id=xdyc0g | folder=external | link=https://www.reddit.com/r/CumDumpsters/comments/xdyc0g/one_of_my_favorite_cum_and_go_vids/\n",
      "[OK] id=xs4a2u | folder=external | link=https://www.reddit.com/r/cumsluts/comments/xs4a2u/perfect_aim/\n",
      "[OK] id=1fozwa1 | folder=external | link=https://www.reddit.com/r/Bukkake/comments/1fozwa1/18_years_teen_making_her_father_proud_by_becoming/\n",
      "[OK] id=1lgqfdn | folder=external | link=https://www.reddit.com/r/Sissies/comments/1lgqfdn/gorgeous_bi_girl_tries_anal_sex_for_the_first/\n",
      "[OK] id=1dagoke | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1dagoke/cumslut_getting_her_lunch/\n",
      "[OK] id=1lhyfda | folder=external | link=https://www.reddit.com/r/BBCparadise/comments/1lhyfda/her_face_says_it_all/\n",
      "[OK] id=1kzvaqh | folder=external | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1kzvaqh/accidentally_leaving_the_public_shower_unlocked/\n",
      "[OK] id=176jobm | folder=external | link=https://www.reddit.com/r/HENTAI_GIF/comments/176jobm/im_gonna_shoot_it_inside/\n",
      "[OK] id=1l35mkn | folder=external | link=https://www.reddit.com/r/rape_hentai/comments/1l35mkn/a_powerful_dangerous_outbreak/\n",
      "[OK] id=1k4dqsr | folder=external | link=https://www.reddit.com/r/bdsm/comments/1k4dqsr/sorry_not_too_much_bondage_but_probably_my/\n",
      "[OK] id=1hvuypg | folder=external | link=https://www.reddit.com/r/Swingersgw/comments/1hvuypg/i_love_sharing_my_bf_with_my_bestie/\n",
      "[OK] id=1j0ousb | folder=external | link=https://www.reddit.com/r/rape_hentai/comments/1j0ousb/im_not_a_girl_anymore_im_now_a_broken_toy_for/\n",
      "[OK] id=uw2e36 | folder=external | link=https://www.reddit.com/r/Cuckold/comments/uw2e36/i_let_a_group_of_college_guys_gangbang_me_and/\n",
      "[OK] id=1j0yn5n | folder=external | link=https://www.reddit.com/r/bdsm/comments/1j0yn5n/i_think_this_woman_is_incredible_absolute/\n",
      "[OK] id=1gptc7t | folder=external | link=https://www.reddit.com/r/GOONED/comments/1gptc7t/when_everybody_at_the_party_knows_that_your_girl/\n",
      "[OK] id=1ckrbt9 | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1ckrbt9/he_missed_a_spot/\n",
      "[OK] id=1j7wugc | folder=external | link=https://www.reddit.com/r/pornID/comments/1j7wugc/anyone_have_the_originalfull_vid/\n",
      "[OK] id=1j0jsq8 | folder=external | link=https://www.reddit.com/r/bdsm/comments/1j0jsq8/in_a_vac_bed_and_forced_to_cum/\n",
      "[OK] id=1b7ph7f | folder=external | link=https://www.reddit.com/r/u_ellesaurus23/comments/1b7ph7f/my_first_ever_joi_i_think_did_so_well/\n",
      "[OK] id=1lzy7kt | folder=external | link=https://www.reddit.com/r/u_SleepyxxxSarah/comments/1lzy7kt/hi_reddit/\n",
      "[OK] id=xcgz7y | folder=external | link=https://www.reddit.com/r/rape_hentai/comments/xcgz7y/i_knew_i_shouldnt_have_gone_to_that_party/\n",
      "[OK] id=zweksh | folder=external | link=https://www.reddit.com/r/SuctionBlowjobs/comments/zweksh/she_makes_him_squirm_as_he_cums_on_her_mouth/\n",
      "[OK] id=1mhfe0x | folder=external | link=https://www.reddit.com/r/rape_hentai/comments/1mhfe0x/hey_dont_complain_bitch_your_team_lost_its_not/\n",
      "[OK] id=1jjrk0j | folder=external | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1jjrk0j/wwait_you_only_paid_for_one_session_you_cant_just/\n",
      "[OK] id=1lqevos | folder=external | link=https://www.reddit.com/r/cumshots/comments/1lqevos/like_a_baby_bird_mouth_wide_open_getting_her_food/\n",
      "[OK] id=1lmlhj4 | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1lmlhj4/theyve_figured_out_safer_sex_she_definitely_not/\n",
      "[OK] id=1fxlziv | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1fxlziv/how_i_want_to_remember_college/\n",
      "[OK] id=1mc0iau | folder=external | link=https://www.reddit.com/r/CumSwallowing/comments/1mc0iau/shes_an_angel_delivering_pure_joy_to_these_fine/\n",
      "[OK] id=1j51vs2 | folder=external | link=https://www.reddit.com/r/rape_hentai/comments/1j51vs2/you_really_believed_me_when_i_told_you_id_fix/\n",
      "[OK] id=w80nyn | folder=external | link=https://www.reddit.com/r/cumshots/comments/w80nyn/mouthful_of_thick_cum_from_my_bathroom_glory_hole/\n",
      "[OK] id=1cfi6e1 | folder=external | link=https://www.reddit.com/r/throatpussy/comments/1cfi6e1/tight_throat_long_dick_no_holding_back/\n",
      "[OK] id=1ikyl72 | folder=external | link=https://www.reddit.com/r/u_ellesaurus23/comments/1ikyl72/first_time_anal_in_a_year_do_u_think_i_could_ever/\n",
      "[OK] id=1majxok | folder=external | link=https://www.reddit.com/r/RoughPorn/comments/1majxok/sluts_are_meant_to_be_controlled_used_and_abused/\n",
      "[OK] id=1n5rfjv | folder=external | link=https://www.reddit.com/r/rape_hentai/comments/1n5rfjv/i_wanna_do_this_to_a_girl_so_bad/\n",
      "[OK] id=1gj94gm | folder=external | link=https://www.reddit.com/r/rape_hentai/comments/1gj94gm/you_will_pay_for_this/\n",
      "[OK] id=1hmmdcd | folder=external | link=https://www.reddit.com/r/Hotwife/comments/1hmmdcd/taking_turns_on_his_girlfriend/\n",
      "[OK] id=1hm0rtx | folder=external | link=https://www.reddit.com/r/rape_hentai/comments/1hm0rtx/you_thought_you_were_a_highly_skilled_adventurer/\n",
      "[OK] id=1m8bjym | folder=external | link=https://www.reddit.com/r/u_ellesaurus23/comments/1m8bjym/this_is_so_porn_brained_i_need_to_make_more/\n",
      "[OK] id=1fbz4gt | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1fbz4gt/goth_girls_love_cum/\n",
      "[OK] id=qrgggv | folder=external | link=https://www.reddit.com/r/Cuckold/comments/qrgggv/lets_watch_my_man_fuck_your_gf_like_he_is_fucking/\n",
      "[OK] id=1ityeo7 | folder=external | link=https://www.reddit.com/r/rape_hentai/comments/1ityeo7/what_she_means_by_treating_her_like_a_princess/\n",
      "[OK] id=1m3cqjw | folder=external | link=https://www.reddit.com/r/rapefantasies/comments/1m3cqjw/maybe_you_should_tell_me_what_your_favorite_rape/\n",
      "[OK] id=1hhxzsf | folder=external | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1hhxzsf/i_would_kill_to_worship_a_man_like_this_omggg/\n",
      "[OK] id=1m6bund | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1m6bund/a_good_cumslut_continues_to_suck/\n",
      "[OK] id=1l7phn7 | folder=external | link=https://www.reddit.com/r/Threesome/comments/1l7phn7/5_fun_facts_about_this_video_1_it_happens_in_my/\n",
      "[OK] id=y6oem2 | folder=external | link=https://www.reddit.com/r/JapanesePorn2/comments/y6oem2/aika_jerks_out_your_load_and_impresses_her_friends/\n",
      "[OK] id=1gjxkcn | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1gjxkcn/its_not_cheating_because_technically_he_didnt_put/\n",
      "[OK] id=1mamlhb | folder=external | link=https://www.reddit.com/r/pornID/comments/1mamlhb/name/\n",
      "[OK] id=1jexvkh | folder=external | link=https://www.reddit.com/r/HealSluts/comments/1jexvkh/when_they_reward_you_for_pocketing_them3/\n",
      "[OK] id=1kedn0a | folder=external | link=https://www.reddit.com/r/rapefantasies/comments/1kedn0a/this_is_how_i_would_sound_and_look_when_you_rape/\n",
      "[OK] id=1lj675o | folder=external | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1lj675o/having_sex_while_cosplaying_sounds_like_so_much/\n",
      "[OK] id=1hi1dlt | folder=external | link=https://www.reddit.com/r/AhegaoGirls/comments/1hi1dlt/i_dont_know_who_this_is_but_god_damn/\n",
      "[OK] id=1met8te | folder=external | link=https://www.reddit.com/r/cumshots/comments/1met8te/he_had_so_much_precum/\n",
      "[OK] id=1hkss3r | folder=external | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1hkss3r/hehe_this_set_of_lingerie_really_turns_you_into_a/\n",
      "[OK] id=1k0qdnl | folder=external | link=https://www.reddit.com/r/cuckquean/comments/1k0qdnl/this_is_the_fourth_friend_that_has_asked_to_fuck/\n",
      "[OK] id=1lz6fmk | folder=external | link=https://www.reddit.com/r/creampiegifs/comments/1lz6fmk/he_fucks_her_til_shes_filled_up/\n",
      "[OK] id=1fisdrb | folder=external | link=https://www.reddit.com/r/bdsm/comments/1fisdrb/she_looked_thirsty/\n",
      "[OK] id=1lduimh | folder=external | link=https://www.reddit.com/r/FaceFuck/comments/1lduimh/this_is_what_i_mean_when_i_ask_you_to_fuck_my_face/\n",
      "[OK] id=1812srx | folder=external | link=https://www.reddit.com/r/BallsDeepThroat/comments/1812srx/i_bet_this_is_what_johnny_meant_when_he_said_i/\n",
      "[OK] id=1ite20t | folder=external | link=https://www.reddit.com/r/netorare/comments/1ite20t/he_is_forced_to_watch_his_wife_being_fucked_by/\n",
      "[OK] id=1lkoi10 | folder=external | link=https://www.reddit.com/r/DoubleFacial/comments/1lkoi10/pulling_ropes_from_left_and_right/\n",
      "[OK] id=1lbtqj9 | folder=external | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1lbtqj9/its_just_natural_for_me_to_lose_all_my_iq_the/\n",
      "[OK] id=1mwgthc | folder=external | link=https://www.reddit.com/r/GirlsFinishingTheJob/comments/1mwgthc/me_and_who/\n",
      "[OK] id=1l49jlv | folder=external | link=https://www.reddit.com/r/u_ellesaurus23/comments/1l49jlv/filled_up_so_fcking_good/\n",
      "[OK] id=1mtslv9 | folder=external | link=https://www.reddit.com/r/GOONED/comments/1mtslv9/i_need_a_good_bud_who_lets_me_use_them_whenever/\n",
      "[OK] id=1n1wdf1 | folder=external | link=https://www.reddit.com/r/Miso_Paradise/comments/1n1wdf1/im_tired_of_ur_pussy_switch_with_ur_friend/\n",
      "[OK] id=1luigb8 | folder=external | link=https://www.reddit.com/r/GOONED/comments/1luigb8/does_this_goonette_make_you_want_to_start_gooning/\n",
      "[OK] id=10edn7m | folder=external | link=https://www.reddit.com/r/Roughsex/comments/10edn7m/bound_together/\n",
      "[OK] id=1m88wjj | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1m88wjj/weak_pullout_game/\n",
      "[OK] id=grrh0u | folder=external | link=https://www.reddit.com/r/hentai/comments/grrh0u/time_stopped_brush_derpixon/\n",
      "[OK] id=1gn25xz | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1gn25xz/the_secret_to_her_fit_body_comes_from_a_high_cum/\n",
      "[OK] id=1jp85xo | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1jp85xo/freckled_blowjob/\n",
      "[OK] id=1kgil5u | folder=external | link=https://www.reddit.com/r/AdultTheaterGals/comments/1kgil5u/everyone_wanted_to_touch/\n",
      "[OK] id=1n7rmug | folder=external | link=https://www.reddit.com/r/GirlsFinishingTheJob/comments/1n7rmug/a_great_finish/\n",
      "[OK] id=168ymw5 | folder=external | link=https://www.reddit.com/r/rule34/comments/168ymw5/shadowheart_thematchandkerosene_baldurs_gate_3/\n",
      "[OK] id=1l5mpda | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1l5mpda/shes_getting_nutritional_injections_its_halal/\n",
      "[OK] id=17qz3af | folder=external | link=https://www.reddit.com/r/cumsluts/comments/17qz3af/if_my_sisters_boyfriend_doesnt_cum_in_my_pussy_it/\n",
      "[OK] id=1g2pezj | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1g2pezj/nice_ending/\n",
      "[OK] id=1ixlp5p | folder=external | link=https://www.reddit.com/r/CumSwallowing/comments/1ixlp5p/school_girl_learns_a_proper_lesson/\n",
      "[OK] id=1leumj0 | folder=external | link=https://www.reddit.com/r/short_porn/comments/1leumj0/no_way_he_came_3_times/\n",
      "[OK] id=1bluhfi | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1bluhfi/she_got_what_she_asked_for/\n",
      "[OK] id=1lcdbrm | folder=external | link=https://www.reddit.com/r/u_ellesaurus23/comments/1lcdbrm/u_know_youre_ovulating_when_u_start_fingering/\n",
      "[OK] id=1kcye0i | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1kcye0i/all_her_friends_know_the_cum_goes_in_her_mouth/\n",
      "[OK] id=1iccou6 | folder=external | link=https://www.reddit.com/r/bdsm/comments/1iccou6/infernal_restraints_molly/\n",
      "[OK] id=1hvx0fr | folder=external | link=https://www.reddit.com/r/SissyInspiration/comments/1hvx0fr/this_is_the_treatment_i_give_anyone_who_comments/\n",
      "[OK] id=1m2flyx | folder=external | link=https://www.reddit.com/r/hentaibondage/comments/1m2flyx/chained_up/\n",
      "[OK] id=1m58kok | folder=external | link=https://www.reddit.com/r/collegesluts/comments/1m58kok/hes_not_a_real_recruiter_but_i_let_him_train_me/\n",
      "[OK] id=1j0umys | folder=external | link=https://www.reddit.com/r/Threesome/comments/1j0umys/what_is_the_correct_order_first_your_girlfriend/\n",
      "[OK] id=1fon3jz | folder=external | link=https://www.reddit.com/r/cuckoldcaptions/comments/1fon3jz/just_making_sure_you_will_feel_this_dick_forever/\n",
      "[OK] id=1jx7d2s | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1jx7d2s/i_have_a_hard_time_believing_this_was_one_mans_cum/\n",
      "[OK] id=1mp3r6o | folder=external | link=https://www.reddit.com/r/iwanttobeher/comments/1mp3r6o/houston_we_have_liftoff/\n",
      "[OK] id=1hap1sy | folder=external | link=https://www.reddit.com/r/GangbangCreampieXXX/comments/1hap1sy/lesbian_eats_pussy_creampies/\n",
      "[OK] id=1jntwmo | folder=external | link=https://www.reddit.com/r/BlowjobGirls/comments/1jntwmo/my_wife_sucks_me_off_in_front_of_her_busty_friend/\n",
      "[OK] id=1lgggv1 | folder=external | link=https://www.reddit.com/r/AhegaoGirls/comments/1lgggv1/my_cutest_ahegao_so_far/\n",
      "[OK] id=1ah1ath | folder=external | link=https://www.reddit.com/r/GOONED/comments/1ah1ath/sorry_bud_i_couldnt_hold_it_in_any_longer/\n",
      "[OK] id=1ik27eg | folder=external | link=https://www.reddit.com/r/GOONED/comments/1ik27eg/i_want_to_tease_and_milk_you_just_like_this_baby/\n",
      "[OK] id=14tzmfr | folder=external | link=https://www.reddit.com/r/DeutscheNSFW/comments/14tzmfr/morgens_halb_10_in_deutschland/\n",
      "[OK] id=t913s5 | folder=external | link=https://www.reddit.com/r/Ahegao_IRL/comments/t913s5/ever_been_in_bed_with_a_cumdesperate_nymphomaniac/\n",
      "[OK] id=1jzzrt7 | folder=external | link=https://www.reddit.com/r/GOONED/comments/1jzzrt7/what_an_amazing_talent_to_have/\n",
      "[OK] id=zqynyb | folder=external | link=https://www.reddit.com/r/AnalAtLast/comments/zqynyb/playing_spin_the_butthole/\n",
      "[OK] id=1m2ry6l | folder=external | link=https://www.reddit.com/r/Facials/comments/1m2ry6l/pov_the_guy_she_tells_you_not_to_worry_about/\n",
      "[OK] id=xvbzze | folder=external | link=https://www.reddit.com/r/CumExtractor/comments/xvbzze/emily_willis_x_succession/\n",
      "[OK] id=1idafc9 | folder=external | link=https://www.reddit.com/r/AsianCumsluts/comments/1idafc9/she_begged_for_it/\n",
      "[OK] id=nyx531 | folder=external | link=https://www.reddit.com/r/MelodyMarks/comments/nyx531/melody_marks_preparing_for_a_selfie/\n",
      "[OK] id=1bwjire | folder=external | link=https://www.reddit.com/r/BlackedMILFs/comments/1bwjire/pure_satisfaction/\n",
      "[OK] id=1heprl9 | folder=external | link=https://www.reddit.com/r/GangbangChicks/comments/1heprl9/fun_continuesi_love_it_when_they_take_their_turn/\n",
      "[OK] id=w9w2be | folder=external | link=https://www.reddit.com/r/GirlsFinishingTheJob/comments/w9w2be/two_girls_finishing_the_job/\n",
      "[OK] id=1m0x4m4 | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1m0x4m4/sprayed_down_and_straight_to_round_2/\n",
      "[OK] id=1js9ygu | folder=external | link=https://www.reddit.com/r/Threesome/comments/1js9ygu/i_cant_decide_if_i_should_sit_on_her_face_on_lay/\n",
      "[OK] id=1mnko5w | folder=external | link=https://www.reddit.com/r/Not_Incest/comments/1mnko5w/its_not_incest_if_both_of_us_suck_daddy_off_right/\n",
      "[OK] id=1hlzo8l | folder=external | link=https://www.reddit.com/r/MollyLittleporn/comments/1hlzo8l/the_christmas_gift_i_want/\n",
      "[OK] id=tz20i3 | folder=external | link=https://www.reddit.com/r/deepthroat/comments/tz20i3/relentless/\n",
      "[OK] id=186hblj | folder=external | link=https://www.reddit.com/r/rapefantasies/comments/186hblj/im_daddys_raped_fuck_puppet_audio_comments_if/\n",
      "[OK] id=1g3dlo3 | folder=external | link=https://www.reddit.com/r/rape_hentai/comments/1g3dlo3/everyone_thought_she_was_the_perfect_innocent/\n",
      "[OK] id=1lnmseb | folder=external | link=https://www.reddit.com/r/AhegaoGirls/comments/1lnmseb/i_get_horny_when_i_make_this_silly_face/\n",
      "[OK] id=1hmear6 | folder=external | link=https://www.reddit.com/r/rape_hentai/comments/1hmear6/testing_a_demon_toy/\n",
      "[OK] id=11t2eil | folder=external | link=https://www.reddit.com/r/cumsluts/comments/11t2eil/oops_accidental_cum/\n",
      "[OK] id=zvqtq5 | folder=external | link=https://www.reddit.com/r/u_GangbNgCum/comments/zvqtq5/its_me_im_the_whore/\n",
      "[OK] id=1kdq9bv | folder=external | link=https://www.reddit.com/r/Blowjobs/comments/1kdq9bv/18_years_old_amateur_blowjob_cumshot_feet_homemade/\n",
      "[OK] id=1mgb4ly | folder=external | link=https://www.reddit.com/r/collegesluts/comments/1mgb4ly/part_1_step_bro_said_it_was_dangerous_then_showed/\n",
      "[OK] id=vg66o1 | folder=external | link=https://www.reddit.com/r/rule34/comments/vg66o1/lisa_x_aether_part_2_てお_genshin_impact/\n",
      "[OK] id=1g3oeg4 | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1g3oeg4/cute_brunette_take_a_nice_load_on_her_tight/\n",
      "[OK] id=1ffgx1y | folder=external | link=https://www.reddit.com/r/u_bugortheletter/comments/1ffgx1y/sex_kitten_going_out_for_free_tonight_to_all_my/\n",
      "[OK] id=10oad2o | folder=external | link=https://www.reddit.com/r/deepthroat/comments/10oad2o/throat_training_endurance/\n",
      "[OK] id=169r7qy | folder=external | link=https://www.reddit.com/r/bodyshots/comments/169r7qy/what_a_covering/\n",
      "[OK] id=1hnadp4 | folder=external | link=https://www.reddit.com/r/sources4porn/comments/1hnadp4/what_is_her_name/\n",
      "[OK] id=1902mgj | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1902mgj/the_pop_doesnt_mean_i_stop/\n",
      "[OK] id=1llax0n | folder=external | link=https://www.reddit.com/r/AhegaoGirls/comments/1llax0n/which_of_my_holes_youd_fuck_if_i_do_ahegao_face/\n",
      "[OK] id=1kkwtvj | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1kkwtvj/drives_a_hard_bargain/\n",
      "[OK] id=1byxder | folder=external | link=https://www.reddit.com/r/notgayatall/comments/1byxder/how_my_bud_left_me_after_our_last_totally/\n",
      "[OK] id=1078oq6 | folder=external | link=https://www.reddit.com/r/SlutMouth/comments/1078oq6/she_was_not_expecting_this_much_cum_in_throat/\n",
      "[OK] id=1lmag9v | folder=external | link=https://www.reddit.com/r/GOONED/comments/1lmag9v/would_you_shoot_it_deeper_if_i_begged_for_a_baby/\n",
      "[OK] id=y22vj7 | folder=external | link=https://www.reddit.com/r/hentaigamer/comments/y22vj7/hot_hentai_creampie_erotic_sex_uncensored/\n",
      "[OK] id=11t7616 | folder=external | link=https://www.reddit.com/r/DiligentBlowjob/comments/11t7616/the_holy_threesome/\n",
      "[OK] id=1kj6iha | folder=external | link=https://www.reddit.com/r/IWantToBeHerHentai2/comments/1kj6iha/just_one_more_time_and_then_we_go_back_to_the/\n",
      "[OK] id=1is2v6h | folder=external | link=https://www.reddit.com/r/TraumatizedSluts/comments/1is2v6h/dad_i_love_being_your_little_fuckslut_i_hope_you/\n",
      "[OK] id=1bjzawv | folder=external | link=https://www.reddit.com/r/notgayatall/comments/1bjzawv/its_not_gay_broi_heard_cum_is_a_good_moisturizer/\n",
      "[OK] id=1673s4k | folder=external | link=https://www.reddit.com/r/anal/comments/1673s4k/ramming_my_tight_asshole/\n",
      "[OK] id=1imo9c5 | folder=external | link=https://www.reddit.com/r/u_layndare/comments/1imo9c5/come_with_us_on_amor_air_as_we_take_off_and_join/\n",
      "[OK] id=1lvfoyz | folder=external | link=https://www.reddit.com/r/AhegaoHardcore/comments/1lvfoyz/_/\n",
      "[OK] id=1knea6a | folder=external | link=https://www.reddit.com/r/ffmFantasy/comments/1knea6a/if_you_can_last_10_min_with_us_swapping_mouths_on/\n",
      "[OK] id=1dx6y9h | folder=external | link=https://www.reddit.com/r/cumfetish/comments/1dx6y9h/whats_the_hottest_thing_a_girl_has_done_while_you/\n",
      "[OK] id=1hdp7ff | folder=external | link=https://www.reddit.com/r/GOONED/comments/1hdp7ff/is_there_anything_hotter_than_your_girl_training/\n",
      "[OK] id=1b9267x | folder=external | link=https://www.reddit.com/r/AmateurPorn/comments/1b9267x/draining_cocks_in_my_face_as_a_lifestyle/\n",
      "[OK] id=1fz14nz | folder=external | link=https://www.reddit.com/r/BlakeBlossom/comments/1fz14nz/blake_blossom_double_blowjob/\n",
      "[OK] id=1fp6g8v | folder=external | link=https://www.reddit.com/r/karleegrey/comments/1fp6g8v/when_she_needs_it_she_needs_it/\n",
      "[OK] id=yq5k0y | folder=external | link=https://www.reddit.com/r/rapefantasies/comments/yq5k0y/id_be_a_naive_little_girl_to_think_i_could_travel/\n",
      "[OK] id=1lhkp3i | folder=external | link=https://www.reddit.com/r/AhegaoGirls/comments/1lhkp3i/how_about_you_cover_my_face_and_boobies_with_your/\n",
      "[OK] id=14zy5zm | folder=external | link=https://www.reddit.com/r/Fastcummers/comments/14zy5zm/cum_fast_5_seconds_inside_her_sweet_pussy_and/\n",
      "[OK] id=1lro0nz | folder=external | link=https://www.reddit.com/r/AhegaoGirls/comments/1lro0nz/does_my_ahegao_make_your_dick_hard_or_i_am_not/\n",
      "[OK] id=1lpipw5 | folder=external | link=https://www.reddit.com/r/AhegaoGirls/comments/1lpipw5/would_you_fuck_my_pussy_as_hard_as_you_can_if_id/\n",
      "[OK] id=12lssm5 | folder=external | link=https://www.reddit.com/r/PussyParadise_/comments/12lssm5/hottest_fuck_ever/\n",
      "[OK] id=1i0ttm5 | folder=external | link=https://www.reddit.com/r/GOONED/comments/1i0ttm5/fuck_this_just_triggered_me_so_hard_im_already/\n",
      "[OK] id=1durtbi | folder=external | link=https://www.reddit.com/r/chokeherslapher/comments/1durtbi/best_way_to_clean_cum_off_of_my_face/\n",
      "[OK] id=1gj0rnl | folder=external | link=https://www.reddit.com/r/SorryCantTalk/comments/1gj0rnl/throat_fuck_queen/\n",
      "[OK] id=1cinlnc | folder=external | link=https://www.reddit.com/r/FFM/comments/1cinlnc/expectantly_waiting/\n",
      "[OK] id=1mnwlyo | folder=external | link=https://www.reddit.com/r/iwanttobeher/comments/1mnwlyo/my_best_friend_asked_me_for_help_to_fulfill_her/\n",
      "[OK] id=1eoe1aw | folder=external | link=https://www.reddit.com/r/blowbang/comments/1eoe1aw/i_get_a_few_big_loads_in_my_mouth/\n",
      "[OK] id=1ffbl1s | folder=external | link=https://www.reddit.com/r/u_bugortheletter/comments/1ffbl1s/cum_but_dont_stop/\n",
      "[OK] id=1m6kp8j | folder=external | link=https://www.reddit.com/r/FFM/comments/1m6kp8j/how_often_does_your_gf_shove_your_cock_in_her/\n",
      "[OK] id=1jtsmm4 | folder=external | link=https://www.reddit.com/r/SheLikesItRough/comments/1jtsmm4/it_was_her_idea_to_make_the_house_a_free_use_zone/\n",
      "[OK] id=kpz0n5 | folder=external | link=https://www.reddit.com/r/squirting/comments/kpz0n5/tiny_japenese_slut_fucks_herself_upside_down_and/\n",
      "[OK] id=1guudlv | folder=external | link=https://www.reddit.com/r/Short_Hentai/comments/1guudlv/break_me_make_me_explode_with_cum_please/\n",
      "[OK] id=ubodgi | folder=external | link=https://www.reddit.com/r/EliteBlowjob/comments/ubodgi/blowjob_god/\n",
      "[OK] id=1cto3bu | folder=external | link=https://www.reddit.com/r/u_jokeaboutdaddyissues/comments/1cto3bu/feminist_as_fuck/\n",
      "[OK] id=1mqufec | folder=external | link=https://www.reddit.com/r/BalkanNSFWxxx/comments/1mqufec/_/\n",
      "[OK] id=leuuoa | folder=external | link=https://www.reddit.com/r/latinchickswhitedicks/comments/leuuoa/petite_amateur_getting_it_rough/\n",
      "[OK] id=1jahtmn | folder=external | link=https://www.reddit.com/r/MelodyMarks/comments/1jahtmn/tipped_for_success/\n",
      "[OK] id=1hcct6o | folder=external | link=https://www.reddit.com/r/dildothreesomes/comments/1hcct6o/bound_and_used/\n",
      "[OK] id=uoxb04 | folder=external | link=https://www.reddit.com/r/BDMaster5588/comments/uoxb04/girl_with_glasses_splitroasted/\n",
      "[OK] id=1h7ctty | folder=external | link=https://www.reddit.com/r/CumHentai/comments/1h7ctty/giving_her_ass_and_pussy_a_good_stretch/\n",
      "[OK] id=1bd91t2 | folder=external | link=https://www.reddit.com/r/cumsluts/comments/1bd91t2/cumslut/\n",
      "[OK] id=y1lpgu | folder=external | link=https://www.reddit.com/r/bangmybully/comments/y1lpgu/your_girlfriend_went_to_beg_your_bully_to_be/\n",
      "[OK] id=1eqp8et | folder=external | link=https://www.reddit.com/r/Handjob/comments/1eqp8et/when_he_offered_up_his_wife_to_give_his_best_bud/\n",
      "[OK] id=1k6le9e | folder=external | link=https://www.reddit.com/r/bodyswap/comments/1k6le9e/hey_youre_here_check_me_out_matt_said_standing_up/\n",
      "[OK] id=1d1f06x | folder=external | link=https://www.reddit.com/r/PublicSexHub/comments/1d1f06x/sharon_lee_big_tit_asian_chick_fucked_in_public/\n",
      "[OK] id=u7ot13 | folder=external | link=https://www.reddit.com/r/BigCumMenu/comments/u7ot13/huge_facial_for_riley/\n",
      "[OK] id=1lwivov | folder=external | link=https://www.reddit.com/r/OneMinutePorn/comments/1lwivov/short_hair_beauty/\n",
      "[OK] id=1n44zck | folder=external | link=https://www.reddit.com/r/TurkishBullMentality/comments/1n44zck/sevgilin_benden_ufak_bi_yüz_maskesi_isteyince/\n",
      "[OK] id=vvmcqf | folder=external | link=https://www.reddit.com/r/naughtychicks/comments/vvmcqf/verification_video/\n",
      "[OK] id=1hk0z7f | folder=external | link=https://www.reddit.com/r/SkinnyBabes/comments/1hk0z7f/who_would_you_fuck_after_class/\n",
      "[OK] id=1h0vb2e | folder=external | link=https://www.reddit.com/r/coxukkerzZ/comments/1h0vb2e/annablossom/\n",
      "[OK] id=1mcgiqk | folder=external | link=https://www.reddit.com/r/ClassAss/comments/1mcgiqk/couple_having_sex_in_moving_train/\n",
      "[OK] id=umhkuc | folder=external | link=https://www.reddit.com/r/BoobsAndTities/comments/umhkuc/mandy_dee/\n",
      "[OK] id=qo6xjf | folder=external | link=https://www.reddit.com/r/u_vpvids/comments/qo6xjf/the_magic_pink_toy/\n",
      "[OK] id=121e3a6 | folder=external | link=https://www.reddit.com/r/ASSttraction/comments/121e3a6/demonic_dicks/\n",
      "[OK] id=gv69pf | folder=external | link=https://www.reddit.com/r/AllPornGifs/comments/gv69pf/arianna_marie_throwback_greatest_of_all_time/\n",
      "[OK] id=14deyfj | folder=external | link=https://www.reddit.com/r/u_WilliWhite82/comments/14deyfj/caseys_gangrape/\n",
      "[OK] id=1m4aq0u | folder=external | link=https://www.reddit.com/r/839714867856843975893/comments/1m4aq0u/snacks_belong_in_the_kitchen_so_here_i_am/\n",
      "[OK] id=zhiryz | folder=external | link=https://www.reddit.com/r/u_dreadmurk/comments/zhiryz/your_face_should_be_between_my_thighs_right_now/\n",
      "[OK] id=1avjta9 | folder=external | link=https://www.reddit.com/r/WhatMakesKevRev/comments/1avjta9/cumshot_facial_cumslut_porn_gif_by_erica7447/\n",
      "[OK] id=1mrkn46 | folder=external | link=https://www.reddit.com/r/hotparade/comments/1mrkn46/do_you_think_my_dad_is_proud/\n",
      "[OK] id=1epp6xm | folder=external | link=https://www.reddit.com/r/u_Guilty-Jello-9885/comments/1epp6xm/eva_green_political_power_to_eva_mendes/\n",
      "[OK] id=1ak3ca6 | folder=external | link=https://www.reddit.com/r/u_Nightpuzzle/comments/1ak3ca6/bbc/\n",
      "[OK] id=wc6j4q | folder=external | link=https://www.reddit.com/r/plusultrafavs/comments/wc6j4q/the_guide_your_girlfriend_uses_on_your/\n",
      "[OK] id=tnlom2 | folder=external | link=https://www.reddit.com/r/u_cebers/comments/tnlom2/18_years_old_babe_big_dick_big_tits_blowjob_cum/\n",
      "[OK] id=1eunoq1 | folder=external | link=https://www.reddit.com/r/u_tbag15455/comments/1eunoq1/youre_at_work_and_im_breeding_your_wife_in_your/\n",
      "[OK] id=1lhcwz2 | folder=external | link=https://www.reddit.com/r/u_bikeweeny4280/comments/1lhcwz2/eva_elfie_handjob/\n",
      "[OK] id=183maa3 | folder=external | link=https://www.reddit.com/r/u_billybadass999/comments/183maa3/you_were_supposed_to_pick_up_your_girlfriend_from/\n",
      "[OK] id=zoniqx | folder=external | link=https://www.reddit.com/r/taboocaptions/comments/zoniqx/thats_how_you_help_your_cuck_kid/\n",
      "\n",
      "Done. Success: 218, Skipped: 13, Failed: 0, Total: 231. Output root: S:\\minds\\Desktop\\Downloader and Reddit System\\Saved-Reddit\\DOWNLOADERS\\out\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs): return x\n",
    "\n",
    "base_out = Path(OUT_DIR)\n",
    "base_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load links first (for progress bar)\n",
    "links: List[str] = []\n",
    "with open(CSV_PATH, newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if row and row[0].strip():\n",
    "            links.append(row[0].strip())\n",
    "\n",
    "ok = failed = skipped = 0\n",
    "failed_rows: List[Dict[str, str]] = []\n",
    "\n",
    "for link in tqdm(links, desc=\"Archiving posts\", unit=\"post\"):\n",
    "    pre_id, _ = parse_link(link)\n",
    "    if pre_id and already_archived(base_out, pre_id):\n",
    "        skipped += 1\n",
    "        continue\n",
    "    try:\n",
    "        post, comments_listing = fetch_full_post_and_comments(link)\n",
    "        pid = post.get(\"id\")\n",
    "        if not pid:\n",
    "            raise RuntimeError(\"Missing post id\")\n",
    "        category = classify_post(post)\n",
    "        archive = canonical_archive(post, comments_listing)\n",
    "        save_archive(archive, base_out, pid, category)\n",
    "        ok += 1\n",
    "        print(f\"[OK] id={pid} | folder={category} | link={link}\")\n",
    "    except Exception as e:\n",
    "        failed += 1\n",
    "        status = getattr(getattr(e, \"response\", None), \"status_code\", \"\")\n",
    "        failed_rows.append({\"link\": link, \"guessed_id\": pre_id or \"\", \"status\": str(status), \"error\": str(e)})\n",
    "    time.sleep(REQUEST_DELAY_SEC)\n",
    "\n",
    "# Export failed links\n",
    "if failed_rows:\n",
    "    fail_path = base_out / \"failed.csv\"\n",
    "    with fail_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"link\", \"guessed_id\", \"status\", \"error\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(failed_rows)\n",
    "    print(f\"\\nSaved failed links to: {fail_path.resolve()}\")\n",
    "\n",
    "print(f\"\\nDone. Success: {ok}, Skipped: {skipped}, Failed: {failed}, Total: {len(links)}. Output root: {base_out.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a03a470",
   "metadata": {},
   "source": [
    "# EXTERNAL LINK EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08a3df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Extract external links and download Redgifs as <post_id>.mp4 ===\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs): return x\n",
    "\n",
    "BASE_OUT = Path(OUT_DIR)\n",
    "EXTERNAL_DIR = BASE_OUT / \"external\"\n",
    "REDDITS_OK = {\"reddit.com\", \"www.reddit.com\", \"old.reddit.com\", \"np.reddit.com\", \"oauth.reddit.com\", \"redd.it\"}\n",
    "REDDIT_NATIVE_MEDIA = {\"i.redd.it\", \"v.redd.it\"}\n",
    "\n",
    "# ---- 1) Helpers to read archives and extract the outbound link ----\n",
    "def _domain(url: str) -> str:\n",
    "    try:\n",
    "        return urlparse(url).netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_external_url(archive_obj: dict) -> str | None:\n",
    "    \"\"\"\n",
    "    From your saved archive object:\n",
    "      { \"raw_post\": {...}, \"raw_comments\": {...}, \"comments\": [...] }\n",
    "    Pull the outbound link for external posts.\n",
    "    \"\"\"\n",
    "    post = archive_obj.get(\"raw_post\") or {}\n",
    "    # Prefer the 'url_overridden_by_dest' field; fallback to 'url'\n",
    "    url = post.get(\"url_overridden_by_dest\") or post.get(\"url\")\n",
    "    if not url:\n",
    "        return None\n",
    "\n",
    "    d = _domain(url)\n",
    "    # Treat non-Reddit, non-native-media as external\n",
    "    if d and d not in REDDITS_OK and d not in REDDIT_NATIVE_MEDIA:\n",
    "        return url\n",
    "    return None\n",
    "\n",
    "# ---- 2) Redgifs normalization & API download ----\n",
    "# Accept common Redgifs URL shapes:\n",
    "RE_REDGIFS_ID = re.compile(\n",
    "    r\"\"\"(?ix)\n",
    "    (?:^|/)(?:watch|ifr)/([a-z0-9]+)     # redgifs.com/watch/<id> or /ifr/<id>\n",
    "    |                                   # OR\n",
    "    (?:^|/)(?:i)/([a-z0-9]+)            # i.redgifs.com/i/<id>\n",
    "    \"\"\".strip()\n",
    ")\n",
    "\n",
    "def redgifs_id_from_url(url: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extract the media ID from redgifs-style URLs:\n",
    "      - https://redgifs.com/watch/<id>\n",
    "      - https://www.redgifs.com/watch/<id>\n",
    "      - https://v3.redgifs.com/watch/<id>\n",
    "      - https://redgifs.com/ifr/<id>\n",
    "      - https://i.redgifs.com/i/<id>\n",
    "    \"\"\"\n",
    "    m = RE_REDGIFS_ID.search(url)\n",
    "    if not m:\n",
    "        return None\n",
    "    # One of the two groups will be set\n",
    "    gid = m.group(1) or m.group(2)\n",
    "    return gid.lower() if gid else None\n",
    "\n",
    "# Redgifs API: get a temporary token, then resolve mp4 URLs\n",
    "REDGIFS_AUTH_URL = \"https://api.redgifs.com/v2/auth/temporary\"\n",
    "REDGIFS_GIF_URL  = \"https://api.redgifs.com/v2/gifs/{id}\"\n",
    "\n",
    "_SESSION = requests.Session()\n",
    "_RG_TOKEN = None\n",
    "_RG_TOKEN_TS = 0\n",
    "\n",
    "def redgifs_token(force: bool = False) -> str:\n",
    "    global _RG_TOKEN, _RG_TOKEN_TS\n",
    "    now = time.time()\n",
    "    # Reuse token for ~20 minutes unless forced\n",
    "    if not force and _RG_TOKEN and (now - _RG_TOKEN_TS) < 1200:\n",
    "        return _RG_TOKEN\n",
    "    r = _SESSION.get(REDGIFS_AUTH_URL, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    _RG_TOKEN = r.json().get(\"token\")\n",
    "    _RG_TOKEN_TS = now\n",
    "    if not _RG_TOKEN:\n",
    "        raise RuntimeError(\"Failed to obtain Redgifs token.\")\n",
    "    return _RG_TOKEN\n",
    "\n",
    "def redgifs_mp4_url(gid: str) -> str:\n",
    "    tok = redgifs_token()\n",
    "    headers = {\"Authorization\": f\"Bearer {tok}\"}\n",
    "    r = _SESSION.get(REDGIFS_GIF_URL.format(id=gid), headers=headers, timeout=30)\n",
    "    # If token expired, refresh once\n",
    "    if r.status_code in (401, 403):\n",
    "        tok = redgifs_token(force=True)\n",
    "        headers = {\"Authorization\": f\"Bearer {tok}\"}\n",
    "        r = _SESSION.get(REDGIFS_GIF_URL.format(id=gid), headers=headers, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    info = r.json().get(\"gif\") or {}\n",
    "    # Prefer HD if present, else SD, else fallback to urls.origin\n",
    "    urls = info.get(\"urls\") or {}\n",
    "    return urls.get(\"hd\") or urls.get(\"sd\") or urls.get(\"origin\")\n",
    "\n",
    "def download_stream(url: str, dest: Path, *, max_retries: int = 4):\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with _SESSION.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                with open(dest, \"wb\") as f:\n",
    "                    for chunk in r.iter_content(chunk_size=1024 * 256):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            if attempt + 1 >= max_retries:\n",
    "                raise\n",
    "            time.sleep(min(2 ** attempt, 15))\n",
    "\n",
    "# ---- 3) Walk external posts, export external links CSV, download Redgifs ----\n",
    "external_json_files = sorted(EXTERNAL_DIR.glob(\"*.json\"))\n",
    "print(f\"Found {len(external_json_files)} external post JSONs in {EXTERNAL_DIR}\")\n",
    "\n",
    "external_rows = []\n",
    "redgifs_failed = []\n",
    "\n",
    "REDGIFS_OUT = BASE_OUT / \"redgifs\"\n",
    "REDGIFS_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for fp in tqdm(external_json_files, desc=\"Scanning external posts\", unit=\"post\"):\n",
    "    try:\n",
    "        data = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "        post = (data or {}).get(\"raw_post\") or {}\n",
    "        pid  = post.get(\"id\") or fp.stem  # fallback to filename if needed\n",
    "\n",
    "        ext_url = extract_external_url(data)\n",
    "        if not ext_url:\n",
    "            # Still record that this external-typed file has no resolvable URL\n",
    "            external_rows.append({\"id\": pid, \"link\": \"\", \"domain\": \"\"})\n",
    "            continue\n",
    "\n",
    "        dom = _domain(ext_url)\n",
    "        external_rows.append({\"id\": pid, \"link\": ext_url, \"domain\": dom})\n",
    "\n",
    "        # Redgifs download\n",
    "        if \"redgifs.com\" in dom or dom.endswith(\".redgifs.com\"):\n",
    "            gid = redgifs_id_from_url(ext_url)\n",
    "            if not gid:\n",
    "                # Sometimes the external URL is a redirect page; skip but log\n",
    "                redgifs_failed.append({\"id\": pid, \"link\": ext_url, \"reason\": \"no_id_from_url\"})\n",
    "                continue\n",
    "\n",
    "            out_path = REDGIFS_OUT / f\"{pid}.mp4\"\n",
    "            if out_path.exists():\n",
    "                # already downloaded\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                mp4_url = redgifs_mp4_url(gid)\n",
    "                if not mp4_url:\n",
    "                    redgifs_failed.append({\"id\": pid, \"link\": ext_url, \"reason\": \"no_mp4_url\"})\n",
    "                    continue\n",
    "                download_stream(mp4_url, out_path)\n",
    "                # Show success line\n",
    "                print(f\"[REDGIFS] id={pid} -> {out_path.name}\")\n",
    "            except Exception as e:\n",
    "                redgifs_failed.append({\"id\": pid, \"link\": ext_url, \"reason\": str(e)})\n",
    "    except Exception as e:\n",
    "        # If we cannot read this JSON at all, log as a redgifs failure only if it looked like redgifs\n",
    "        redgifs_failed.append({\"id\": fp.stem, \"link\": \"\", \"reason\": f\"read_error: {e}\"})\n",
    "\n",
    "# ---- 4) Write summary CSVs ----\n",
    "ext_csv = BASE_OUT / \"external_links.csv\"\n",
    "with ext_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"id\", \"link\", \"domain\"])\n",
    "    w.writeheader()\n",
    "    w.writerows(external_rows)\n",
    "\n",
    "if redgifs_failed:\n",
    "    fail_csv = BASE_OUT / \"redgifs_failed.csv\"\n",
    "    with fail_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"id\", \"link\", \"reason\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(redgifs_failed)\n",
    "    print(f\"\\nSaved Redgifs download failures to: {fail_csv.resolve()}\")\n",
    "\n",
    "print(f\"\\nSaved external links to: {ext_csv.resolve()}\")\n",
    "print(f\"Redgifs saved (if any) to: {REDGIFS_OUT.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938da43",
   "metadata": {},
   "source": [
    "# MEDIA DOWNLOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb94da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Download embedded Reddit-hosted media for posts in out/media/*.json ===\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import html\n",
    "import mimetypes\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs): return x\n",
    "\n",
    "BASE_OUT = Path(OUT_DIR)\n",
    "MEDIA_JSON_DIR = BASE_OUT / \"media\"\n",
    "MEDIA_OUT_DIR = BASE_OUT / \"media_files\"\n",
    "MEDIA_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\"User-Agent\": \"reddit-media-downloader/1.0\"})\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _clean_url(u: str | None) -> str | None:\n",
    "    if not u:\n",
    "        return None\n",
    "    # Reddit often returns HTML-escaped URLs inside JSON\n",
    "    return html.unescape(u)\n",
    "\n",
    "def _domain(u: str | None) -> str:\n",
    "    if not u:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return urlparse(u).netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _ext_from_url_or_type(url: str | None, content_type: str | None) -> str:\n",
    "    # Prefer extension from URL, else derive from content-type\n",
    "    if url:\n",
    "        path = urlparse(url).path\n",
    "        ext = os.path.splitext(path)[1].lower()\n",
    "        if ext in {\".jpg\", \".jpeg\", \".png\", \".gif\", \".mp4\", \".webm\"}:\n",
    "            return ext\n",
    "    if content_type:\n",
    "        ext = mimetypes.guess_extension(content_type.split(\";\")[0].strip())\n",
    "        if ext:\n",
    "            # normalize jpeg\n",
    "            return \".jpg\" if ext == \".jpe\" else ext\n",
    "    # sensible default fallback\n",
    "    return \".mp4\" if (url and \".mp4\" in url) else \".jpg\"\n",
    "\n",
    "def _stream_download(url: str, dest: Path, *, max_retries: int = 4, chunk=1024 * 256):\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with SESSION.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                ctype = r.headers.get(\"Content-Type\")\n",
    "                # if dest has no extension yet, refine using content-type\n",
    "                if dest.suffix == \"\" and ctype:\n",
    "                    dest = dest.with_suffix(_ext_from_url_or_type(url, ctype))\n",
    "                with open(dest, \"wb\") as f:\n",
    "                    for part in r.iter_content(chunk_size=chunk):\n",
    "                        if part:\n",
    "                            f.write(part)\n",
    "            return dest  # final path (may include refined suffix)\n",
    "        except Exception:\n",
    "            if attempt + 1 >= max_retries:\n",
    "                raise\n",
    "            time.sleep(min(2 ** attempt, 15))\n",
    "\n",
    "def _pick_best_preview(post: dict) -> str | None:\n",
    "    \"\"\"\n",
    "    For image/GIF-like posts where 'preview' exists.\n",
    "    Prefer MP4 variant (smaller, plays everywhere), else best image 'source'.\n",
    "    \"\"\"\n",
    "    prev = post.get(\"preview\") or {}\n",
    "    variants = prev.get(\"variants\") or {}\n",
    "    # mp4 variant for gifs, etc.\n",
    "    mp4v = variants.get(\"mp4\") or variants.get(\"reddit_video_preview\")\n",
    "    if mp4v and mp4v.get(\"source\", {}).get(\"url\"):\n",
    "        return _clean_url(mp4v[\"source\"][\"url\"])\n",
    "    # fallback to the image source\n",
    "    src = (prev.get(\"images\") or [{}])[0].get(\"source\", {})\n",
    "    if src.get(\"url\"):\n",
    "        return _clean_url(src[\"url\"])\n",
    "    return None\n",
    "\n",
    "def _pick_vreddit_urls(post: dict) -> tuple[str | None, str | None]:\n",
    "    \"\"\"\n",
    "    v.redd.it posts: return (preferred_mp4_url, fallback_mp4_url)\n",
    "    Try in order: 'hls_url' (m3u8) -> 'fallback_url' (progressive) -> preview mp4.\n",
    "    We only directly download MP4 (no ffmpeg merge here), so we prefer fallback_url,\n",
    "    and otherwise try preview mp4.\n",
    "    \"\"\"\n",
    "    media = post.get(\"media\") or {}\n",
    "    rv = media.get(\"reddit_video\") or {}\n",
    "    fallback = rv.get(\"fallback_url\")  # often progressive mp4 (may be muted on long vids)\n",
    "    hls = rv.get(\"hls_url\")            # m3u8 playlist (would require ffmpeg)\n",
    "    # If no fallback, sometimes preview.mp4 exists:\n",
    "    prev_mp4 = None\n",
    "    prev = post.get(\"preview\") or {}\n",
    "    pv = prev.get(\"reddit_video_preview\") or {}\n",
    "    if isinstance(pv, dict) and pv.get(\"fallback_url\"):\n",
    "        prev_mp4 = pv[\"fallback_url\"]\n",
    "    return (_clean_url(fallback), _clean_url(prev_mp4 or hls))\n",
    "\n",
    "def _gallery_items(post: dict) -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    For gallery posts: return list of (url, suggested_ext).\n",
    "    Uses media_metadata to select best 's' rendition.\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    meta = post.get(\"media_metadata\") or {}\n",
    "    gdata = post.get(\"gallery_data\") or {}\n",
    "    order = [e.get(\"media_id\") for e in gdata.get(\"items\", []) if e.get(\"media_id\")]\n",
    "    for mid in order:\n",
    "        m = meta.get(mid) or {}\n",
    "        s = m.get(\"s\") or {}\n",
    "        url = _clean_url(s.get(\"mp4\") or s.get(\"gif\") or s.get(\"u\") or s.get(\"url\"))\n",
    "        if not url:\n",
    "            continue\n",
    "        # guess extension: mp4 preferred over gif over image\n",
    "        if \"mp4\" in s:\n",
    "            ext = \".mp4\"\n",
    "        elif \"gif\" in s:\n",
    "            ext = \".mp4\"  # we'll still download the gif URL, but use .mp4 if it's actually mp4\n",
    "        else:\n",
    "            # look at mime if present\n",
    "            m_type = m.get(\"m\")\n",
    "            ext = _ext_from_url_or_type(url, m_type)\n",
    "        items.append((url, ext))\n",
    "    return items\n",
    "\n",
    "# ---------- main walk ----------\n",
    "media_jsons = sorted(MEDIA_JSON_DIR.glob(\"*.json\"))\n",
    "print(f\"Found {len(media_jsons)} media post JSONs in {MEDIA_JSON_DIR}\")\n",
    "\n",
    "fail_rows = []\n",
    "downloaded = 0\n",
    "\n",
    "for fp in tqdm(media_jsons, desc=\"Downloading embedded media\", unit=\"post\"):\n",
    "    try:\n",
    "        data = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "        post = (data or {}).get(\"raw_post\") or {}\n",
    "        pid = post.get(\"id\") or fp.stem\n",
    "\n",
    "        # Prefer Reddit-hosted URL if present\n",
    "        url = _clean_url(post.get(\"url_overridden_by_dest\") or post.get(\"url\"))\n",
    "        dom = _domain(url)\n",
    "\n",
    "        # Case A: gallery\n",
    "        if post.get(\"is_gallery\") or (post.get(\"gallery_data\") and post.get(\"media_metadata\")):\n",
    "            items = _gallery_items(post)\n",
    "            if not items:\n",
    "                fail_rows.append({\"id\": pid, \"reason\": \"gallery_no_items\"})\n",
    "                continue\n",
    "            for idx, (item_url, ext) in enumerate(items, start=1):\n",
    "                outfile = MEDIA_OUT_DIR / f\"{pid}_g{idx:02d}{ext if ext.startswith('.') else ('.' + ext)}\"\n",
    "                if outfile.exists():\n",
    "                    continue\n",
    "                try:\n",
    "                    _stream_download(item_url, outfile)\n",
    "                    downloaded += 1\n",
    "                    print(f\"[GAL] {pid} -> {outfile.name}\")\n",
    "                except Exception as e:\n",
    "                    fail_rows.append({\"id\": pid, \"reason\": f\"gallery_item_fail:{e}\"})\n",
    "\n",
    "            continue  # next post\n",
    "\n",
    "        # Case B: native video (v.redd.it)\n",
    "        if (post.get(\"is_video\") or (post.get(\"media\") or {}).get(\"reddit_video\")) and dom.endswith(\"v.redd.it\"):\n",
    "            main_mp4, alt_mp4 = _pick_vreddit_urls(post)\n",
    "            target = MEDIA_OUT_DIR / f\"{pid}.mp4\"\n",
    "            if target.exists():\n",
    "                continue\n",
    "            src = main_mp4 or alt_mp4\n",
    "            if not src:\n",
    "                # last chance: look into preview variants\n",
    "                src = _pick_best_preview(post)\n",
    "            if not src:\n",
    "                fail_rows.append({\"id\": pid, \"reason\": \"vreddit_no_source\"})\n",
    "                continue\n",
    "            try:\n",
    "                _stream_download(src, target)\n",
    "                downloaded += 1\n",
    "                print(f\"[VID] {pid} -> {target.name}\")\n",
    "            except Exception as e:\n",
    "                fail_rows.append({\"id\": pid, \"reason\": f\"vreddit_dl_fail:{e}\"})\n",
    "            continue\n",
    "\n",
    "        # Case C: image / gif via i.redd.it or preview\n",
    "        if dom.endswith(\"i.redd.it\"):\n",
    "            # Direct i.redd.it link\n",
    "            ext = _ext_from_url_or_type(url, None)\n",
    "            target = MEDIA_OUT_DIR / f\"{pid}{ext}\"\n",
    "            if not target.exists():\n",
    "                try:\n",
    "                    _stream_download(url, target)\n",
    "                    downloaded += 1\n",
    "                    print(f\"[IMG] {pid} -> {target.name}\")\n",
    "                except Exception as e:\n",
    "                    fail_rows.append({\"id\": pid, \"reason\": f\"ireddit_dl_fail:{e}\"})\n",
    "            continue\n",
    "\n",
    "        # Fallback: try preview (covers some GIF-to-MP4 conversions)\n",
    "        prev_url = _pick_best_preview(post)\n",
    "        if prev_url and _domain(prev_url) in {\"i.redd.it\", \"v.redd.it\", \"preview.redd.it\"}:\n",
    "            ext = _ext_from_url_or_type(prev_url, None)\n",
    "            target = MEDIA_OUT_DIR / f\"{pid}{ext}\"\n",
    "            if not target.exists():\n",
    "                try:\n",
    "                    _stream_download(prev_url, target)\n",
    "                    downloaded += 1\n",
    "                    print(f\"[PREV] {pid} -> {target.name}\")\n",
    "                except Exception as e:\n",
    "                    fail_rows.append({\"id\": pid, \"reason\": f\"preview_dl_fail:{e}\"})\n",
    "            continue\n",
    "\n",
    "        # If we reach here, it looks like a Reddit-hosted \"media\" without a reliable direct URL\n",
    "        fail_rows.append({\"id\": pid, \"reason\": \"no_reddit_media_url\"})\n",
    "    except Exception as e:\n",
    "        fail_rows.append({\"id\": fp.stem, \"reason\": f\"read_error:{e}\"})\n",
    "\n",
    "# ---------- write failures ----------\n",
    "if fail_rows:\n",
    "    fail_csv = BASE_OUT / \"media_failed.csv\"\n",
    "    with fail_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"id\", \"reason\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(fail_rows)\n",
    "    print(f\"\\nSaved media failures to: {fail_csv.resolve()}\")\n",
    "\n",
    "print(f\"\\nDone. Downloaded: {downloaded}. Files saved under: {MEDIA_OUT_DIR.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Primary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
