{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4869123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reddit script app credentials (temporary hardcode ok for local use) ---\n",
    "REDDIT_CLIENT_ID = \"Ik1IhrLMkUe2Y7_jLqj-Ew\"\n",
    "REDDIT_CLIENT_SECRET = \"1j81ffxuNl-e8EzPV4D3OzCVCH-1lw\"\n",
    "REDDIT_USERNAME = \"Grand_Admiral_Tyken\"\n",
    "REDDIT_PASSWORD = \"X5bugNC9j3Bc^Uf\"\n",
    "REDDIT_USER_AGENT = \"SavedRedditJSON/1.0 by u/\" + REDDIT_USERNAME\n",
    "\n",
    "for k, v in {\n",
    "    \"REDDIT_CLIENT_ID\": REDDIT_CLIENT_ID,\n",
    "    \"REDDIT_CLIENT_SECRET\": \"[set]\" if REDDIT_CLIENT_SECRET else \"\",\n",
    "    \"REDDIT_USERNAME\": REDDIT_USERNAME,\n",
    "    \"REDDIT_PASSWORD\": \"[set]\" if REDDIT_PASSWORD else \"\",\n",
    "}.items():\n",
    "    if not v:\n",
    "        print(f\"WARNING: missing {k}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "227caf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Notebook Config ---\n",
    "DATA_ROOT = \"out\"          # output root used by your site\n",
    "CSV_PATH  = \"../links.csv\"           # one URL per line (optional)\n",
    "SKIP_EXISTING   = True            # skip if a JSON for that id already exists in any bucket\n",
    "COMMENTS_DEPTH  = 1000               # nested reply depth cap\n",
    "COMMENTS_LIMIT  = 100000             # max comments per thread (total)\n",
    "REQ_MAX_RETRIES = 10               # HTTP backoff retries\n",
    "DELAY_BETWEEN   = 0.05             # polite delay (seconds) per URL\n",
    "BATCH_PAUSE     = 100               # extra pause between batches (0 = none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31ada7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, csv, json, time, random, base64, requests\n",
    "from datetime import datetime, timezone\n",
    "from urllib.parse import urlparse, urlunparse, urlencode, parse_qsl\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    _ = load_dotenv(override=False)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# progress bar\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except Exception:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "RUN_TS = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "REPORTS_DIR = \"reports\"\n",
    "os.makedirs(REPORTS_DIR, exist_ok=True)\n",
    "REPORTS_CSV = os.path.join(REPORTS_DIR, f\"run-{RUN_TS}.csv\")\n",
    "\n",
    "# session with UA\n",
    "UA = REDDIT_USER_AGENT or \"Reddit-Archiver/JSON\"\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\"User-Agent\": UA})\n",
    "\n",
    "# make sure buckets exist\n",
    "for sub in (\"media\", \"external\", \"text\"):\n",
    "    os.makedirs(os.path.join(DATA_ROOT, sub), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c089a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditOAuth:\n",
    "    TOKEN_URL = \"https://www.reddit.com/api/v1/access_token\"\n",
    "    def __init__(self, client_id, client_secret, username, password, user_agent):\n",
    "        self.client_id = client_id\n",
    "        self.client_secret = client_secret\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.user_agent = user_agent\n",
    "        self._token = None\n",
    "        self._exp = 0\n",
    "\n",
    "    def fetch(self):\n",
    "        auth = requests.auth.HTTPBasicAuth(self.client_id, self.client_secret)\n",
    "        data = {\"grant_type\":\"password\", \"username\": self.username, \"password\": self.password}\n",
    "        headers = {\"User-Agent\": self.user_agent}\n",
    "        r = requests.post(self.TOKEN_URL, auth=auth, data=data, headers=headers, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        js = r.json()\n",
    "        self._token = js[\"access_token\"]\n",
    "        self._exp = time.time() + int(js.get(\"expires_in\", 3600)) * 0.9  # refresh early\n",
    "\n",
    "    def token(self):\n",
    "        if not self._token or time.time() >= self._exp:\n",
    "            self.fetch()\n",
    "        return self._token\n",
    "\n",
    "    def headers(self):\n",
    "        return {\"Authorization\": f\"bearer {self.token()}\", \"User-Agent\": self.user_agent}\n",
    "\n",
    "# uses REDDIT_* from your .env / first cell\n",
    "oauth = RedditOAuth(REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USERNAME, REDDIT_PASSWORD, REDDIT_USER_AGENT)\n",
    "\n",
    "def _to_oauth_url(url: str) -> str:\n",
    "    # convert public reddit URLs to oauth host; drop .json\n",
    "    u = url.rstrip()\n",
    "    if u.endswith(\".json\"):\n",
    "        u = u[:-5]\n",
    "    p = urlparse(u)\n",
    "    if p.netloc.lower() in {\"www.reddit.com\",\"reddit.com\",\"old.reddit.com\",\"np.reddit.com\"}:\n",
    "        p = p._replace(netloc=\"oauth.reddit.com\")\n",
    "        u = urlunparse(p)\n",
    "    return u\n",
    "\n",
    "def oauth_request(method: str, url: str, **kw):\n",
    "    headers = kw.pop(\"headers\", {}) or {}\n",
    "    headers.update(oauth.headers())\n",
    "    url2 = _to_oauth_url(url)\n",
    "    while True:\n",
    "        r = SESSION.request(method, url2, headers=headers, **kw)\n",
    "        if r.status_code == 401:  # expired token\n",
    "            oauth._token = None\n",
    "            headers.update(oauth.headers())\n",
    "            r = SESSION.request(method, url2, headers=headers, **kw)\n",
    "        if r.status_code == 429:  # rate limited\n",
    "            delay = r.headers.get(\"retry-after\")\n",
    "            try:\n",
    "                delay = float(delay) if delay is not None else 2.0\n",
    "            except Exception:\n",
    "                delay = 2.0\n",
    "            time.sleep(max(2.0, delay))\n",
    "            continue\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68b18893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auth user: Grand_Admiral_Tyken\n"
     ]
    }
   ],
   "source": [
    "me = oauth_request(\"GET\", \"https://oauth.reddit.com/api/v1/me\").json()\n",
    "print(\"Auth user:\", me.get(\"name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deda5f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def now_iso():\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def request_with_backoff(method: str, url: str, *, max_retries=5, timeout=30, stream=False, headers=None, params=None):\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        try:\n",
    "            resp = oauth_request(method, url, timeout=timeout, stream=stream, headers=headers, params=params)\n",
    "        except requests.RequestException as e:\n",
    "            if attempt >= max_retries:\n",
    "                raise\n",
    "            sleep = min(60, 2 ** attempt) + random.uniform(0, 0.5)\n",
    "            print(f\"Network error {e}; retrying in {sleep:.1f}s …\")\n",
    "            time.sleep(sleep); attempt += 1\n",
    "            continue\n",
    "\n",
    "        # Respect rate limits / transient errors\n",
    "        if resp.status_code == 429 or 500 <= resp.status_code < 600:\n",
    "            if attempt >= max_retries:\n",
    "                resp.raise_for_status()\n",
    "                return resp\n",
    "            retry_after = resp.headers.get(\"retry-after\")\n",
    "            try:\n",
    "                sleep = float(retry_after) if retry_after is not None else min(60, 2 ** attempt)\n",
    "            except Exception:\n",
    "                sleep = min(60, 2 ** attempt)\n",
    "            sleep += random.uniform(0, 0.5)\n",
    "            print(f\"HTTP {resp.status_code}; retrying in {sleep:.1f}s …\")\n",
    "            time.sleep(sleep); attempt += 1\n",
    "            continue\n",
    "\n",
    "        return resp\n",
    "    \n",
    "def find_existing_path_and_bucket(root, rid):\n",
    "    for b in (\"media\",\"external\",\"text\"):\n",
    "        p = os.path.join(root, b, f\"{rid}.json\")\n",
    "        if os.path.exists(p):\n",
    "            return p, b\n",
    "    return None, None\n",
    "\n",
    "def out_path_any_bucket(root, rid):\n",
    "    \"\"\"Return the first existing path for rid across buckets, else None.\"\"\"\n",
    "    for b in (\"media\", \"external\", \"text\"):\n",
    "        p = os.path.join(root, b, f\"{rid}.json\")\n",
    "        if os.path.exists(p): return p\n",
    "    return None\n",
    "\n",
    "def out_path_for(root, bucket, rid):\n",
    "    return os.path.join(root, bucket, f\"{rid}.json\")\n",
    "\n",
    "def init_report():\n",
    "    if not os.path.exists(REPORTS_CSV):\n",
    "        with open(REPORTS_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            csv.writer(f).writerow([\"ts\",\"id\",\"status\",\"url\",\"bucket\",\"out_path\",\"reason\",\"http_status\"])\n",
    "\n",
    "def log_report(*, ts, rid, status, url, bucket=None, out_path=None, reason=None, http_status=None):\n",
    "    with open(REPORTS_CSV, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        csv.writer(f).writerow([ts, rid, status, url, bucket or \"\", out_path or \"\", reason or \"\", http_status or \"\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8457222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_post_and_comments(url: str, *, max_retries=REQ_MAX_RETRIES):\n",
    "    if not url.startswith((\"http://\", \"https://\")):\n",
    "        raise ValueError(f\"Not a URL: {url}\")\n",
    "    u = url\n",
    "    if not u.endswith(\"/\"):\n",
    "        u += \"/\"\n",
    "    params = {\"raw_json\": 1, \"limit\": COMMENTS_LIMIT, \"depth\": COMMENTS_DEPTH}\n",
    "    r = request_with_backoff(\"GET\", u, max_retries=max_retries, timeout=30, params=params)\n",
    "    data = r.json()\n",
    "    if not (isinstance(data, list) and len(data) >= 2):\n",
    "        raise RuntimeError(\"Unexpected Reddit JSON format\")\n",
    "    post_listing = data[0][\"data\"][\"children\"]\n",
    "    if not post_listing:\n",
    "        raise RuntimeError(\"Post listing empty\")\n",
    "    post = post_listing[0][\"data\"]\n",
    "    comments_listing = data[1]\n",
    "    return post, comments_listing, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "381b7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_comments(listing_node, *, max_depth=COMMENTS_DEPTH, max_count=COMMENTS_LIMIT):\n",
    "    \"\"\"\n",
    "    Normalize Reddit 'Listing' trees into a clean array of nested comment dicts:\n",
    "    { id, author, body, body_html, score, created_utc, permalink, is_submitter, parent_id, replies: [] }\n",
    "    - Replies are ALWAYS an array ('' -> [])\n",
    "    - Only kind 't1' (comments) are collected\n",
    "    - Depth and total count are bounded for safety\n",
    "    \"\"\"\n",
    "    collected = []\n",
    "    def walk(node, depth, remaining):\n",
    "        if remaining[0] <= 0 or depth > max_depth: return\n",
    "        if not isinstance(node, dict): return\n",
    "        kind = node.get(\"kind\"); data = node.get(\"data\", {})\n",
    "\n",
    "        if kind == \"t1\":\n",
    "            remaining[0] -= 1\n",
    "            item = {\n",
    "                \"id\": data.get(\"id\"),\n",
    "                \"author\": data.get(\"author\"),\n",
    "                \"author_fullname\": data.get(\"author_fullname\"),\n",
    "                \"body\": data.get(\"body\"),\n",
    "                \"body_html\": data.get(\"body_html\"),\n",
    "                \"score\": data.get(\"score\"),\n",
    "                \"created_utc\": data.get(\"created_utc\"),\n",
    "                \"permalink\": \"https://www.reddit.com\" + (data.get(\"permalink\") or \"\"),\n",
    "                \"is_submitter\": data.get(\"is_submitter\"),\n",
    "                \"parent_id\": data.get(\"parent_id\"),\n",
    "                \"replies\": []\n",
    "            }\n",
    "            replies = data.get(\"replies\")\n",
    "            if replies and isinstance(replies, dict):  # Listing case\n",
    "                children = replies.get(\"data\", {}).get(\"children\", [])\n",
    "                for ch in children:\n",
    "                    if remaining[0] <= 0: break\n",
    "                    child_obj = walk(ch, depth + 1, remaining)\n",
    "                    if child_obj: item[\"replies\"].append(child_obj)\n",
    "            # If replies == \"\" → keep replies as []\n",
    "            return item\n",
    "\n",
    "        if kind == \"Listing\":\n",
    "            for ch in node.get(\"data\", {}).get(\"children\", []):\n",
    "                if remaining[0] <= 0: break\n",
    "                obj = walk(ch, depth, remaining)\n",
    "                if obj: collected.append(obj)\n",
    "        return None\n",
    "\n",
    "    remaining = [max_count]\n",
    "    walk(listing_node, 1, remaining)\n",
    "    return collected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f89f87a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_media_kind(post: dict) -> str:\n",
    "    url = (post.get(\"url_overridden_by_dest\") or post.get(\"url\") or \"\").lower()\n",
    "    domain = (post.get(\"domain\") or \"\").lower()\n",
    "    post_hint = (post.get(\"post_hint\") or \"\").lower()\n",
    "    if post.get(\"is_gallery\", False): return \"gallery\"\n",
    "    elif \"v.redd.it\" in url or \\\n",
    "       (post.get(\"secure_media\") and post[\"secure_media\"].get(\"reddit_video\")) or \\\n",
    "       (post.get(\"media\") and post[\"media\"].get(\"reddit_video\")) or \\\n",
    "       bool(post.get(\"crosspost_parent_list\")):\n",
    "        return \"video\"\n",
    "    elif post_hint == \"image\" or domain in (\"i.redd.it\", \"i.reddituploads.com\"): return \"image\"\n",
    "    elif post.get(\"is_self\", False): return \"self\"\n",
    "    return \"external\"\n",
    "\n",
    "def select_bucket(media_kind: str) -> str:\n",
    "    if media_kind in (\"image\", \"gallery\", \"video\"):\n",
    "        return \"media\"\n",
    "    elif media_kind == \"external\":\n",
    "        return \"external\"\n",
    "    return \"text\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84f85ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_archive_object(post: dict, comments_listing: dict, *,\n",
    "                        include_comments=True, comments_depth=COMMENTS_DEPTH, comments_limit=COMMENTS_LIMIT):\n",
    "    obj = {\n",
    "        \"archived_at\": now_iso(),\n",
    "        \"reddit_fullname\": post.get(\"name\"),\n",
    "        \"reddit_id\": post.get(\"id\"),\n",
    "        \"permalink\": \"https://www.reddit.com\" + (post.get(\"permalink\") or \"\"),\n",
    "        \"title\": post.get(\"title\", \"\"),\n",
    "        \"selftext\": post.get(\"selftext\", \"\"),\n",
    "        \"author\": post.get(\"author\"),\n",
    "        \"author_fullname\": post.get(\"author_fullname\"),\n",
    "        \"subreddit\": post.get(\"subreddit\"),\n",
    "        \"subreddit_id\": post.get(\"subreddit_id\"),\n",
    "        \"created_utc\": post.get(\"created_utc\"),\n",
    "        \"is_self\": post.get(\"is_self\", False),\n",
    "        \"url\": post.get(\"url_overridden_by_dest\") or post.get(\"url\"),\n",
    "        \"domain\": post.get(\"domain\"),\n",
    "        \"post_hint\": post.get(\"post_hint\"),\n",
    "        \"is_gallery\": post.get(\"is_gallery\", False),\n",
    "        \"over_18\": post.get(\"over_18\", False),\n",
    "        \"spoiler\": post.get(\"spoiler\", False),\n",
    "        \"link_flair_text\": post.get(\"link_flair_text\"),\n",
    "        \"is_original_content\": post.get(\"is_original_content\", False),\n",
    "        \"stickied\": post.get(\"stickied\", False),\n",
    "        \"locked\": post.get(\"locked\", False),\n",
    "        \"edited\": post.get(\"edited\"),\n",
    "        \"num_comments\": post.get(\"num_comments\"),\n",
    "        \"score\": post.get(\"score\"),\n",
    "        \"upvote_ratio\": post.get(\"upvote_ratio\"),\n",
    "        \"media_kind\": classify_media_kind(post),\n",
    "        \"media\": None,              # (extend later if you also download assets)\n",
    "        \"external_link\": None,      # (fill if media_kind == external)\n",
    "        \"raw_post\": post,\n",
    "        \"raw_comments\": None\n",
    "    }\n",
    "    if include_comments:\n",
    "        obj[\"comments\"] = extract_comments(comments_listing,\n",
    "                                           max_depth=comments_depth,\n",
    "                                           max_count=comments_limit)\n",
    "        obj[\"raw_comments\"] = comments_listing\n",
    "\n",
    "    if obj[\"media_kind\"] == \"external\":\n",
    "        obj[\"external_link\"] = obj[\"url\"]\n",
    "\n",
    "    return obj\n",
    "\n",
    "def write_archive_json(archive_obj, root=DATA_ROOT):\n",
    "    rid = archive_obj.get(\"reddit_id\") or \"post\"\n",
    "    bucket = select_bucket(archive_obj.get(\"media_kind\"))\n",
    "    out_dir = os.path.join(root, bucket)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    path = os.path.join(out_dir, f\"{rid}.json\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(archive_obj, f, ensure_ascii=False, indent=2)\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24ced5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one(url: str, *, skip_existing=SKIP_EXISTING):\n",
    "    ts = now_iso()\n",
    "    try:\n",
    "        # Fetch first to learn the reddit id (cheap single request)\n",
    "        post, comments_listing, _ = fetch_post_and_comments(url, max_retries=REQ_MAX_RETRIES)\n",
    "        rid = post.get(\"id\") or \"post\"\n",
    "\n",
    "        # Skip if JSON already exists in any bucket\n",
    "        if skip_existing:\n",
    "            existing_path, existing_bucket = find_existing_path_and_bucket(DATA_ROOT, rid)\n",
    "            if existing_path:\n",
    "                return {\n",
    "                    \"ts\": ts,\n",
    "                    \"status\": \"skipped\",\n",
    "                    \"id\": rid,\n",
    "                    \"url\": url,\n",
    "                    \"bucket\": existing_bucket,\n",
    "                    \"path\": existing_path,\n",
    "                    \"reason\": \"exists\",\n",
    "                    \"http_status\": None,\n",
    "                }\n",
    "\n",
    "        # Build archive, classify bucket, write file\n",
    "        obj = make_archive_object(post, comments_listing)\n",
    "        bucket = select_bucket(obj.get(\"media_kind\"))\n",
    "        path = write_archive_json(obj)  # writes to DATA_ROOT/<bucket>/<id>.json\n",
    "\n",
    "        return {\n",
    "            \"ts\": ts,\n",
    "            \"status\": \"success\",\n",
    "            \"id\": rid,\n",
    "            \"url\": url,\n",
    "            \"bucket\": bucket,\n",
    "            \"path\": path,\n",
    "            \"reason\": None,\n",
    "            \"http_status\": None,\n",
    "        }\n",
    "\n",
    "    except requests.HTTPError as he:\n",
    "        code = getattr(he.response, \"status_code\", None)\n",
    "        # try to recover id from URL on failure\n",
    "        rid = locals().get(\"rid\", None)\n",
    "        return {\n",
    "            \"ts\": ts,\n",
    "            \"status\": \"failed\",\n",
    "            \"id\": rid or \"\",\n",
    "            \"url\": url,\n",
    "            \"bucket\": None,\n",
    "            \"path\": None,\n",
    "            \"reason\": str(he),\n",
    "            \"http_status\": code,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        rid = locals().get(\"rid\", None)\n",
    "        return {\n",
    "            \"ts\": ts,\n",
    "            \"status\": \"failed\",\n",
    "            \"id\": rid or \"\",\n",
    "            \"url\": url,\n",
    "            \"bucket\": None,\n",
    "            \"path\": None,\n",
    "            \"reason\": str(e),\n",
    "            \"http_status\": None,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d80008de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79da66bd6f2946f1af822f83910231b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Archiving posts:   0%|          | 0/1 [00:00<?, ?post/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. {'success': 1, 'skipped': 0, 'failed': 0} Report: reports\\run-20251016-145538.csv\n"
     ]
    }
   ],
   "source": [
    "def read_links(csv_path):\n",
    "    out = []\n",
    "    with open(csv_path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        for row in csv.reader(f):\n",
    "            if not row: continue\n",
    "            u = (row[0] or \"\").strip()\n",
    "            if not u or u.startswith(\"#\"): continue\n",
    "            out.append(u)\n",
    "    return out\n",
    "\n",
    "links = read_links(CSV_PATH)\n",
    "\n",
    "init_report()  # <-- add this\n",
    "\n",
    "results = {\"success\": 0, \"skipped\": 0, \"failed\": 0}\n",
    "with tqdm(total=len(links), desc=\"Archiving posts\", unit=\"post\") as pbar:\n",
    "    for link in links:\n",
    "        info = process_one(link)\n",
    "        results[info[\"status\"]] += 1\n",
    "        log_report(\n",
    "            ts=info[\"ts\"], rid=info[\"id\"], status=info[\"status\"], url=info[\"url\"],\n",
    "            bucket=info.get(\"bucket\"), out_path=info.get(\"path\"),\n",
    "            reason=info.get(\"reason\"), http_status=info.get(\"http_status\")\n",
    "        )\n",
    "        pbar.set_postfix(results=results)\n",
    "        pbar.update(1)\n",
    "        if DELAY_BETWEEN: time.sleep(DELAY_BETWEEN)\n",
    "\n",
    "print(\"Done.\", results, \"Report:\", REPORTS_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a03a470",
   "metadata": {},
   "source": [
    "# EXTERNAL LINK EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c08a3df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 external post JSONs in out\\external\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3050f5e1f2ee40c7a1e35985db60b7e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scanning external posts: 0post [00:00, ?post/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved external links to: S:\\minds\\Desktop\\Downloader and Reddit System\\Saved-Reddit\\SCRIPTS\\out\\external_links.csv\n",
      "Redgifs saved (if any) to: S:\\minds\\Desktop\\Downloader and Reddit System\\Saved-Reddit\\SCRIPTS\\out\\redgifs\n"
     ]
    }
   ],
   "source": [
    "# === Extract external links and download Redgifs as <post_id>.mp4 ===\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs): return x\n",
    "\n",
    "BASE_OUT = Path(DATA_ROOT)\n",
    "EXTERNAL_DIR = BASE_OUT / \"external\"\n",
    "REDDITS_OK = {\"reddit.com\", \"www.reddit.com\", \"old.reddit.com\", \"np.reddit.com\", \"oauth.reddit.com\", \"redd.it\"}\n",
    "REDDIT_NATIVE_MEDIA = {\"i.redd.it\", \"v.redd.it\"}\n",
    "\n",
    "# ---- 1) Helpers to read archives and extract the outbound link ----\n",
    "def _domain(url: str) -> str:\n",
    "    try:\n",
    "        return urlparse(url).netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_external_url(archive_obj: dict) -> str | None:\n",
    "    \"\"\"\n",
    "    From your saved archive object:\n",
    "      { \"raw_post\": {...}, \"raw_comments\": {...}, \"comments\": [...] }\n",
    "    Pull the outbound link for external posts.\n",
    "    \"\"\"\n",
    "    post = archive_obj.get(\"raw_post\") or {}\n",
    "    # Prefer the 'url_overridden_by_dest' field; fallback to 'url'\n",
    "    url = post.get(\"url_overridden_by_dest\") or post.get(\"url\")\n",
    "    if not url:\n",
    "        return None\n",
    "\n",
    "    d = _domain(url)\n",
    "    # Treat non-Reddit, non-native-media as external\n",
    "    if d and d not in REDDITS_OK and d not in REDDIT_NATIVE_MEDIA:\n",
    "        return url\n",
    "    return None\n",
    "\n",
    "# ---- 2) Redgifs normalization & API download ----\n",
    "# Accept common Redgifs URL shapes:\n",
    "RE_REDGIFS_ID = re.compile(\n",
    "    r\"\"\"(?ix)\n",
    "    (?:^|/)(?:watch|ifr)/([a-z0-9]+)     # redgifs.com/watch/<id> or /ifr/<id>\n",
    "    |                                   # OR\n",
    "    (?:^|/)(?:i)/([a-z0-9]+)            # i.redgifs.com/i/<id>\n",
    "    \"\"\".strip()\n",
    ")\n",
    "\n",
    "def redgifs_id_from_url(url: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extract the media ID from redgifs-style URLs:\n",
    "      - https://redgifs.com/watch/<id>\n",
    "      - https://www.redgifs.com/watch/<id>\n",
    "      - https://v3.redgifs.com/watch/<id>\n",
    "      - https://redgifs.com/ifr/<id>\n",
    "      - https://i.redgifs.com/i/<id>\n",
    "    \"\"\"\n",
    "    m = RE_REDGIFS_ID.search(url)\n",
    "    if not m:\n",
    "        return None\n",
    "    # One of the two groups will be set\n",
    "    gid = m.group(1) or m.group(2)\n",
    "    return gid.lower() if gid else None\n",
    "\n",
    "# Redgifs API: get a temporary token, then resolve mp4 URLs\n",
    "REDGIFS_AUTH_URL = \"https://api.redgifs.com/v2/auth/temporary\"\n",
    "REDGIFS_GIF_URL  = \"https://api.redgifs.com/v2/gifs/{id}\"\n",
    "\n",
    "_SESSION = requests.Session()\n",
    "_RG_TOKEN = None\n",
    "_RG_TOKEN_TS = 0\n",
    "\n",
    "def redgifs_token(force: bool = False) -> str:\n",
    "    global _RG_TOKEN, _RG_TOKEN_TS\n",
    "    now = time.time()\n",
    "    # Reuse token for ~20 minutes unless forced\n",
    "    if not force and _RG_TOKEN and (now - _RG_TOKEN_TS) < 1200:\n",
    "        return _RG_TOKEN\n",
    "    r = _SESSION.get(REDGIFS_AUTH_URL, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    _RG_TOKEN = r.json().get(\"token\")\n",
    "    _RG_TOKEN_TS = now\n",
    "    if not _RG_TOKEN:\n",
    "        raise RuntimeError(\"Failed to obtain Redgifs token.\")\n",
    "    return _RG_TOKEN\n",
    "\n",
    "def redgifs_mp4_url(gid: str) -> str:\n",
    "    tok = redgifs_token()\n",
    "    headers = {\"Authorization\": f\"Bearer {tok}\"}\n",
    "    r = _SESSION.get(REDGIFS_GIF_URL.format(id=gid), headers=headers, timeout=30)\n",
    "    # If token expired, refresh once\n",
    "    if r.status_code in (401, 403):\n",
    "        tok = redgifs_token(force=True)\n",
    "        headers = {\"Authorization\": f\"Bearer {tok}\"}\n",
    "        r = _SESSION.get(REDGIFS_GIF_URL.format(id=gid), headers=headers, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    info = r.json().get(\"gif\") or {}\n",
    "    # Prefer HD if present, else SD, else fallback to urls.origin\n",
    "    urls = info.get(\"urls\") or {}\n",
    "    return urls.get(\"hd\") or urls.get(\"sd\") or urls.get(\"origin\")\n",
    "\n",
    "def download_stream(url: str, dest: Path, *, max_retries: int = 4):\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with _SESSION.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                with open(dest, \"wb\") as f:\n",
    "                    for chunk in r.iter_content(chunk_size=1024 * 256):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            if attempt + 1 >= max_retries:\n",
    "                raise\n",
    "            time.sleep(min(2 ** attempt, 15))\n",
    "\n",
    "# ---- 3) Walk external posts, export external links CSV, download Redgifs ----\n",
    "external_json_files = sorted(EXTERNAL_DIR.glob(\"*.json\"))\n",
    "print(f\"Found {len(external_json_files)} external post JSONs in {EXTERNAL_DIR}\")\n",
    "\n",
    "external_rows = []\n",
    "redgifs_failed = []\n",
    "\n",
    "REDGIFS_OUT = BASE_OUT / \"redgifs\"\n",
    "REDGIFS_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for fp in tqdm(external_json_files, desc=\"Scanning external posts\", unit=\"post\"):\n",
    "    try:\n",
    "        data = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "        post = (data or {}).get(\"raw_post\") or {}\n",
    "        pid  = post.get(\"id\") or fp.stem  # fallback to filename if needed\n",
    "\n",
    "        ext_url = extract_external_url(data)\n",
    "        if not ext_url:\n",
    "            # Still record that this external-typed file has no resolvable URL\n",
    "            external_rows.append({\"id\": pid, \"link\": \"\", \"domain\": \"\"})\n",
    "            continue\n",
    "\n",
    "        dom = _domain(ext_url)\n",
    "        external_rows.append({\"id\": pid, \"link\": ext_url, \"domain\": dom})\n",
    "\n",
    "        # Redgifs download\n",
    "        if \"redgifs.com\" in dom or dom.endswith(\".redgifs.com\"):\n",
    "            gid = redgifs_id_from_url(ext_url)\n",
    "            if not gid:\n",
    "                # Sometimes the external URL is a redirect page; skip but log\n",
    "                redgifs_failed.append({\"id\": pid, \"link\": ext_url, \"reason\": \"no_id_from_url\"})\n",
    "                continue\n",
    "\n",
    "            out_path = REDGIFS_OUT / f\"{pid}.mp4\"\n",
    "            if out_path.exists():\n",
    "                # already downloaded\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                mp4_url = redgifs_mp4_url(gid)\n",
    "                if not mp4_url:\n",
    "                    redgifs_failed.append({\"id\": pid, \"link\": ext_url, \"reason\": \"no_mp4_url\"})\n",
    "                    continue\n",
    "                download_stream(mp4_url, out_path)\n",
    "                # Show success line\n",
    "                print(f\"[REDGIFS] id={pid} -> {out_path.name}\")\n",
    "            except Exception as e:\n",
    "                redgifs_failed.append({\"id\": pid, \"link\": ext_url, \"reason\": str(e)})\n",
    "    except Exception as e:\n",
    "        # If we cannot read this JSON at all, log as a redgifs failure only if it looked like redgifs\n",
    "        redgifs_failed.append({\"id\": fp.stem, \"link\": \"\", \"reason\": f\"read_error: {e}\"})\n",
    "\n",
    "# ---- 4) Write summary CSVs ----\n",
    "ext_csv = BASE_OUT / \"external_links.csv\"\n",
    "with ext_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"id\", \"link\", \"domain\"])\n",
    "    w.writeheader()\n",
    "    w.writerows(external_rows)\n",
    "\n",
    "if redgifs_failed:\n",
    "    fail_csv = BASE_OUT / \"redgifs_failed.csv\"\n",
    "    with fail_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"id\", \"link\", \"reason\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(redgifs_failed)\n",
    "    print(f\"\\nSaved Redgifs download failures to: {fail_csv.resolve()}\")\n",
    "\n",
    "print(f\"\\nSaved external links to: {ext_csv.resolve()}\")\n",
    "print(f\"Redgifs saved (if any) to: {REDGIFS_OUT.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938da43",
   "metadata": {},
   "source": [
    "# MEDIA DOWNLOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb94da8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 media post JSONs in out\\media\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:159: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:159: SyntaxWarning: invalid escape sequence '\\|'\n",
      "C:\\Users\\minds\\AppData\\Local\\Temp\\ipykernel_19792\\491745515.py:159: SyntaxWarning: invalid escape sequence '\\|'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d472f45d83494605b0e8d502bbbdcc93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading embedded media:   0%|          | 0/1 [00:00<?, ?post/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GAL] 1o7yp0r -> 1o7yp0r/01.jpg\n",
      "[GAL] 1o7yp0r -> 1o7yp0r/02.jpg\n",
      "[GAL] 1o7yp0r -> 1o7yp0r/03.jpg\n",
      "[GAL] 1o7yp0r -> 1o7yp0r/04.jpg\n",
      "[GAL] 1o7yp0r -> 1o7yp0r/05.jpg\n",
      "[GAL] 1o7yp0r -> 1o7yp0r/06.jpg\n",
      "[GAL] 1o7yp0r -> 1o7yp0r/07.jpg\n",
      "[GAL] 1o7yp0r -> 1o7yp0r/08.jpg\n",
      "[GAL] 1o7yp0r -> 1o7yp0r/09.jpg\n",
      "[GAL] 1o7yp0r -> 1o7yp0r/10.jpg\n",
      "\n",
      "Done. Downloaded: 10. Files saved under: S:\\minds\\Desktop\\Downloader and Reddit System\\Saved-Reddit\\SCRIPTS\\out\\media_files\n"
     ]
    }
   ],
   "source": [
    "# === Download embedded Reddit-hosted media for posts in out/media/*.json ===\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import html\n",
    "import mimetypes\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs): return x\n",
    "\n",
    "# ---- configurable root (defaults to ./out) ----\n",
    "DATA_ROOT = os.environ.get(\"DATA_ROOT\", \"out\")\n",
    "\n",
    "BASE_OUT = Path(DATA_ROOT)\n",
    "MEDIA_JSON_DIR = BASE_OUT / \"media\"\n",
    "MEDIA_OUT_DIR = BASE_OUT / \"media_files\"\n",
    "MEDIA_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\"User-Agent\": \"reddit-media-downloader/1.1 (preserve-originals)\"})\n",
    "\n",
    "# ---------- helpers ----------\n",
    "WIN_ILLEGAL = set('<>:\"/\\\\|?*')\n",
    "\n",
    "def _clean_url(u: str | None) -> str | None:\n",
    "    if not u:\n",
    "        return None\n",
    "    # Reddit often returns HTML-escaped URLs inside JSON\n",
    "    return html.unescape(u)\n",
    "\n",
    "def _domain(u: str | None) -> str:\n",
    "    if not u:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return urlparse(u).netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _ext_from_url_or_type(url: str | None, content_type: str | None) -> str:\n",
    "    # Prefer extension from URL, else derive from content-type\n",
    "    if url:\n",
    "        path = urlparse(url).path\n",
    "        ext = os.path.splitext(path)[1].lower()\n",
    "        if ext in {\n",
    "            \".jpg\", \".jpeg\", \".png\", \".webp\", \".avif\",\n",
    "            \".gif\", \".mp4\", \".webm\", \".mov\"\n",
    "        }:\n",
    "            return ext\n",
    "    if content_type:\n",
    "        # normalize content-type → extension (mimetypes includes many)\n",
    "        ct = content_type.split(\";\")[0].strip().lower()\n",
    "        ext = mimetypes.guess_extension(ct) or \"\"\n",
    "        if ext:\n",
    "            # common normalizations\n",
    "            if ext == \".jpe\":\n",
    "                return \".jpg\"\n",
    "            if ext == \".apng\":\n",
    "                return \".png\"\n",
    "            return ext.lower()\n",
    "        # a few manual fallbacks\n",
    "        if ct == \"image/jpg\":\n",
    "            return \".jpg\"\n",
    "        if ct == \"image/jpeg\":\n",
    "            return \".jpg\"\n",
    "        if ct == \"image/webp\":\n",
    "            return \".webp\"\n",
    "        if ct == \"image/avif\":\n",
    "            return \".avif\"\n",
    "        if ct == \"image/gif\":\n",
    "            return \".gif\"\n",
    "        if ct == \"video/mp4\":\n",
    "            return \".mp4\"\n",
    "        if ct == \"video/webm\":\n",
    "            return \".webm\"\n",
    "    # last-resort: try to infer from URL substrings\n",
    "    if url:\n",
    "        low = url.lower()\n",
    "        for marker, e in [\n",
    "            (\".jpg\", \".jpg\"), (\".jpeg\", \".jpg\"), (\".png\", \".png\"),\n",
    "            (\".webp\", \".webp\"), (\".avif\", \".avif\"),\n",
    "            (\".gif\", \".gif\"), (\".mp4\", \".mp4\"), (\".webm\", \".webm\")\n",
    "        ]:\n",
    "            if marker in low:\n",
    "                return e\n",
    "    # default image if unknown\n",
    "    return \".jpg\"\n",
    "\n",
    "def _stream_download(url: str, dest: Path, *, max_retries: int = 4, chunk=1024 * 256) -> Path:\n",
    "    \"\"\"\n",
    "    Stream download to `dest`. If `dest` has no suffix, we'll refine it from Content-Type.\n",
    "    Returns the final path (might differ if we refined suffix).\n",
    "    \"\"\"\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    final_dest = dest\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with SESSION.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                ctype = r.headers.get(\"Content-Type\")\n",
    "                # If dest has no extension yet, refine using content-type\n",
    "                if final_dest.suffix == \"\" and ctype:\n",
    "                    final_dest = final_dest.with_suffix(_ext_from_url_or_type(url, ctype))\n",
    "                with open(final_dest, \"wb\") as f:\n",
    "                    for part in r.iter_content(chunk_size=chunk):\n",
    "                        if part:\n",
    "                            f.write(part)\n",
    "            return final_dest\n",
    "        except Exception as e:\n",
    "            if attempt + 1 >= max_retries:\n",
    "                raise\n",
    "            time.sleep(min(2 ** attempt, 15))\n",
    "    return final_dest\n",
    "\n",
    "def _pick_best_preview_original_first(post: dict) -> str | None:\n",
    "    \"\"\"\n",
    "    For image/GIF-like posts where 'preview' exists.\n",
    "    Prefer GIF (original) over MP4 transcodes; else best image 'source'.\n",
    "    \"\"\"\n",
    "    prev = post.get(\"preview\") or {}\n",
    "    variants = prev.get(\"variants\") or {}\n",
    "    # prefer gif over mp4 to keep original\n",
    "    gifv = variants.get(\"gif\")\n",
    "    if gifv and gifv.get(\"source\", {}).get(\"url\"):\n",
    "        return _clean_url(gifv[\"source\"][\"url\"])\n",
    "    # image source\n",
    "    src = (prev.get(\"images\") or [{}])[0].get(\"source\", {})\n",
    "    if src.get(\"url\"):\n",
    "        return _clean_url(src[\"url\"])\n",
    "    # finally, allow mp4 if nothing else available\n",
    "    mp4v = variants.get(\"mp4\") or variants.get(\"reddit_video_preview\")\n",
    "    if mp4v and mp4v.get(\"source\", {}).get(\"url\"):\n",
    "        return _clean_url(mp4v[\"source\"][\"url\"])\n",
    "    return None\n",
    "\n",
    "def _pick_vreddit_urls(post: dict) -> tuple[str | None, str | None]:\n",
    "    \"\"\"\n",
    "    v.redd.it posts: return (preferred_mp4_url, fallback_mp4_url)\n",
    "    We only download MP4 for native reddit video; there's no \"original\" other than mp4.\n",
    "    \"\"\"\n",
    "    media = post.get(\"media\") or {}\n",
    "    rv = media.get(\"reddit_video\") or {}\n",
    "    fallback = rv.get(\"fallback_url\")  # progressive mp4\n",
    "    hls = rv.get(\"hls_url\")            # m3u8 (requires ffmpeg; we don't use it here)\n",
    "    # Sometimes preview mp4 exists:\n",
    "    prev = post.get(\"preview\") or {}\n",
    "    pv = prev.get(\"reddit_video_preview\") or {}\n",
    "    prev_mp4 = pv.get(\"fallback_url\") if isinstance(pv, dict) else None\n",
    "    return (_clean_url(fallback), _clean_url(prev_mp4 or hls))\n",
    "\n",
    "def _safe_dirname_from_title(title: str | None, pid: str) -> str:\n",
    "    \"\"\"\n",
    "    Build a Windows-safe folder name from the post title.\n",
    "    - Strip illegal characters <>:\"/\\|?*\n",
    "    - Collapse whitespace\n",
    "    - Trim trailing dots/spaces\n",
    "    - Limit length\n",
    "    Fallback to post id if empty.\n",
    "    \"\"\"\n",
    "    t = html.unescape((title or \"\").strip())\n",
    "    # collapse whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    # remove illegal chars\n",
    "    t = \"\".join(ch for ch in t if ch not in WIN_ILLEGAL and ord(ch) >= 32)\n",
    "    # trim length generously (Windows path limits are tighter, but this is fine)\n",
    "    t = t[:120].strip(\" .\")\n",
    "    if not t:\n",
    "        t = pid\n",
    "    return t\n",
    "\n",
    "def _gallery_items_with_originals(post: dict) -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    For gallery posts: return list of (url, suggested_ext) preserving original format.\n",
    "    Uses media_metadata to select the best 's' rendition. Prefers GIF over MP4 for animated items.\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    meta = post.get(\"media_metadata\") or {}\n",
    "    gdata = post.get(\"gallery_data\") or {}\n",
    "    order = [e.get(\"media_id\") for e in gdata.get(\"items\", []) if e.get(\"media_id\")]\n",
    "    for mid in order:\n",
    "        m = meta.get(mid) or {}\n",
    "        s = m.get(\"s\") or {}\n",
    "        # Prefer original GIF when present, else still image, else mp4 fallback\n",
    "        url = _clean_url(s.get(\"gif\") or s.get(\"u\") or s.get(\"url\") or s.get(\"mp4\"))\n",
    "        if not url:\n",
    "            continue\n",
    "        m_type = m.get(\"m\")  # e.g., \"image/jpeg\", \"image/png\", \"image/gif\"\n",
    "        ext = _ext_from_url_or_type(url, m_type)\n",
    "        items.append((url, ext))\n",
    "    return items\n",
    "\n",
    "def _num_pad_width(n: int) -> int:\n",
    "    \"\"\"Choose padding width for numbering (01, 02 …).\"\"\"\n",
    "    return max(2, len(str(n)))\n",
    "\n",
    "# ---------- main walk ----------\n",
    "media_jsons = sorted(MEDIA_JSON_DIR.glob(\"*.json\"))\n",
    "print(f\"Found {len(media_jsons)} media post JSONs in {MEDIA_JSON_DIR}\")\n",
    "\n",
    "fail_rows = []\n",
    "downloaded = 0\n",
    "\n",
    "for fp in tqdm(media_jsons, desc=\"Downloading embedded media\", unit=\"post\"):\n",
    "    try:\n",
    "        data = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "        post = (data or {}).get(\"raw_post\") or {}\n",
    "        pid = post.get(\"id\") or fp.stem\n",
    "\n",
    "        # Prefer Reddit-hosted URL if present\n",
    "        url = _clean_url(post.get(\"url_overridden_by_dest\") or post.get(\"url\"))\n",
    "        dom = _domain(url)\n",
    "\n",
    "        # Case A: gallery  (→ save into folder named after the post title; files 01.ext, 02.ext…)\n",
    "        if post.get(\"is_gallery\") or (post.get(\"gallery_data\") and post.get(\"media_metadata\")):\n",
    "            items = _gallery_items_with_originals(post)\n",
    "            if not items:\n",
    "                fail_rows.append({\"id\": pid, \"reason\": \"gallery_no_items\"})\n",
    "                continue\n",
    "\n",
    "            # Build Windows-safe folder name from title; fallback to pid if needed\n",
    "            folder = str(pid).strip()\n",
    "            gal_dir = MEDIA_OUT_DIR / folder\n",
    "            gal_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            pad = _num_pad_width(len(items))\n",
    "            for idx, (item_url, ext) in enumerate(items, start=1):\n",
    "                # Always ensure ext begins with dot\n",
    "                if not ext.startswith(\".\"):\n",
    "                    ext = \".\" + ext\n",
    "                # 01.ext, 02.ext …\n",
    "                outfile = gal_dir / f\"{str(idx).zfill(pad)}{ext}\"\n",
    "                if outfile.exists():\n",
    "                    continue\n",
    "                try:\n",
    "                    _stream_download(item_url, outfile)\n",
    "                    downloaded += 1\n",
    "                    print(f\"[GAL] {pid} -> {folder}/{outfile.name}\")\n",
    "                except Exception as e:\n",
    "                    fail_rows.append({\"id\": pid, \"reason\": f\"gallery_item_fail:{e}\"})\n",
    "            continue  # next post\n",
    "\n",
    "        # Case B: native video (v.redd.it) → original is MP4\n",
    "        if (post.get(\"is_video\") or (post.get(\"media\") or {}).get(\"reddit_video\")) and dom.endswith(\"v.redd.it\"):\n",
    "            main_mp4, alt_mp4 = _pick_vreddit_urls(post)\n",
    "            target = MEDIA_OUT_DIR / f\"{pid}.mp4\"\n",
    "            if target.exists():\n",
    "                continue\n",
    "            src = main_mp4 or alt_mp4\n",
    "            if not src:\n",
    "                # last chance: look into preview variants (might be mp4)\n",
    "                src = _pick_best_preview_original_first(post)\n",
    "            if not src:\n",
    "                fail_rows.append({\"id\": pid, \"reason\": \"vreddit_no_source\"})\n",
    "                continue\n",
    "            try:\n",
    "                _stream_download(src, target)\n",
    "                downloaded += 1\n",
    "                print(f\"[VID] {pid} -> {target.name}\")\n",
    "            except Exception as e:\n",
    "                fail_rows.append({\"id\": pid, \"reason\": f\"vreddit_dl_fail:{e}\"})\n",
    "            continue\n",
    "\n",
    "        # Case C: direct image/gif via i.redd.it\n",
    "        if dom.endswith(\"i.redd.it\"):\n",
    "            ext = _ext_from_url_or_type(url, None)\n",
    "            target = MEDIA_OUT_DIR / f\"{pid}{ext}\"\n",
    "            if not target.exists():\n",
    "                try:\n",
    "                    _stream_download(url, target)\n",
    "                    downloaded += 1\n",
    "                    print(f\"[IMG] {pid} -> {target.name}\")\n",
    "                except Exception as e:\n",
    "                    fail_rows.append({\"id\": pid, \"reason\": f\"ireddit_dl_fail:{e}\"})\n",
    "            continue\n",
    "\n",
    "        # Fallback: try preview, preferring original formats (gif/image) before mp4\n",
    "        prev_url = _pick_best_preview_original_first(post)\n",
    "        if prev_url and _domain(prev_url) in {\"i.redd.it\", \"v.redd.it\", \"preview.redd.it\"}:\n",
    "            ext = _ext_from_url_or_type(prev_url, None)\n",
    "            target = MEDIA_OUT_DIR / f\"{pid}{ext}\"\n",
    "            if not target.exists():\n",
    "                try:\n",
    "                    _stream_download(prev_url, target)\n",
    "                    downloaded += 1\n",
    "                    print(f\"[PREV] {pid} -> {target.name}\")\n",
    "                except Exception as e:\n",
    "                    fail_rows.append({\"id\": pid, \"reason\": f\"preview_dl_fail:{e}\"})\n",
    "            continue\n",
    "\n",
    "        # If we reach here, it looks like a Reddit-hosted \"media\" without a reliable direct URL\n",
    "        fail_rows.append({\"id\": pid, \"reason\": \"no_reddit_media_url\"})\n",
    "    except Exception as e:\n",
    "        fail_rows.append({\"id\": fp.stem, \"reason\": f\"read_error:{e}\"})\n",
    "\n",
    "# ---------- write failures ----------\n",
    "if fail_rows:\n",
    "    fail_csv = BASE_OUT / \"media_failed.csv\"\n",
    "    with fail_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"id\", \"reason\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(fail_rows)\n",
    "    print(f\"\\nSaved media failures to: {fail_csv.resolve()}\")\n",
    "\n",
    "print(f\"\\nDone. Downloaded: {downloaded}. Files saved under: {MEDIA_OUT_DIR.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Primary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
